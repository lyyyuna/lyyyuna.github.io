<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>lyyyuna 的小花园</title>
  
  <subtitle>动静中之动</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.lyyyuna.com/"/>
  <updated>2017-11-29T00:19:00.165Z</updated>
  <id>http://www.lyyyuna.com/</id>
  
  <author>
    <name>lyyyuna</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python 中的 descriptor</title>
    <link href="http://www.lyyyuna.com/2017/09/14/python-descriptor/"/>
    <id>http://www.lyyyuna.com/2017/09/14/python-descriptor/</id>
    <published>2017-09-14T08:20:02.000Z</published>
    <updated>2017-11-29T00:19:00.165Z</updated>
    
    <content type="html"><![CDATA[<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>通常，一个 descriptor 是具有“绑定行为”的对象属性。所绑定行为可通过 descriptor 协议被自定义的 <code>__get__()</code>, <code>__set__()</code> 和 <code>__delete__()</code> 方法重写。如果一个对象的上述三个方法任意一个被重写，则就可被称为 descriptor。</p><p>属性的默认操作是从对象字典中获取、设置和删除一个属性。例如，a.x 有一个查找链，先 <code>a.__dict__[&#39;x&#39;]</code>，若没有则 <code>type(a).__dict__[&#39;x&#39;]</code>，若没有增往上查找父类直到元类。如果查找链中，对象被定义了 descriptor 方法，Python 就会覆盖默认行为。</p><p>Descriptor 是一个强大的工具，虽然开发者不常接触到它，但它其实就是类、属性、函数、方法、静态方法、类方法以及 <code>super()</code> 背后的运行机制。</p><h2 id="Descriptor-协议"><a href="#Descriptor-协议" class="headerlink" title="Descriptor 协议"></a>Descriptor 协议</h2><p>三个方法原型如下所示：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">descr.__get__(self, obj, type=None) --&gt; value</span><br><span class="line"></span><br><span class="line">descr.__set__(self, obj, value) --&gt; None</span><br><span class="line"></span><br><span class="line">descr.__delete__(self, obj) --&gt; None</span><br></pre></td></tr></table></figure><p>数据 descriptor 是同时具有 <code>__get__()</code> 和 <code>__set__()</code> 方法的对象，若只有 <code>__get__()</code> 方法，则为非数据 descriptor。如果实例字典中有和数据 descriptor 同名的入口，则数据 descriptor 优先级更高。相反，非数据 descriptor 优先级低。</p><p>让 <code>__set__()</code> 方法抛出异常，就能创建一个只读数据 descriptor。</p><h2 id="调用-descriptor"><a href="#调用-descriptor" class="headerlink" title="调用 descriptor"></a>调用 descriptor</h2><p>descriptor 可以直接通过方法名调用。例如，<code>d.__get__(obj)</code>。</p><p>而通过访问对象属性，自动调用 descriptor 才是更通用的做法。例如，如果 <code>d</code> 定义了方法 <code>__get__()</code>，则 <code>obj.d</code> 会调用 <code>d.__get__(obj)</code>。</p><p>对于对象，<code>b.x</code> 会被转换成 <code>type(b).__dict__[&#39;x&#39;].__get__(b, type(b))</code>。而对于类（是的，类也可以调用），<code>B.x</code> 会被转换成 <code>B.__dict__[&#39;x&#39;].__get__(None, B)</code>。</p><h2 id="Descriptor-例子"><a href="#Descriptor-例子" class="headerlink" title="Descriptor 例子"></a>Descriptor 例子</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RevealAccess</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""A data descriptor that sets and returns values</span></span><br><span class="line"><span class="string">       normally and prints a message logging their access.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, initval=None, name=<span class="string">'var'</span>)</span>:</span></span><br><span class="line">        self.val = initval</span><br><span class="line">        self.name = name</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__get__</span><span class="params">(self, obj, objtype)</span>:</span></span><br><span class="line">        print(<span class="string">'Retrieving'</span>, self.name)</span><br><span class="line">        <span class="keyword">return</span> self.val</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__set__</span><span class="params">(self, obj, val)</span>:</span></span><br><span class="line">        print(<span class="string">'Updating'</span>, self.name)</span><br><span class="line">        self.val = val</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span><span class="params">(object)</span>:</span></span><br><span class="line"><span class="meta">... </span>    x = RevealAccess(<span class="number">10</span>, <span class="string">'var "x"'</span>)</span><br><span class="line"><span class="meta">... </span>    y = <span class="number">5</span></span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = MyClass()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.x</span><br><span class="line">Retrieving var <span class="string">"x"</span></span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.x = <span class="number">20</span></span><br><span class="line">Updating var <span class="string">"x"</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.x</span><br><span class="line">Retrieving var <span class="string">"x"</span></span><br><span class="line"><span class="number">20</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.y</span><br><span class="line"><span class="number">5</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;定义&quot;&gt;&lt;a href=&quot;#定义&quot; class=&quot;headerlink&quot; title=&quot;定义&quot;&gt;&lt;/a&gt;定义&lt;/h2&gt;&lt;p&gt;通常，一个 descriptor 是具有“绑定行为”的对象属性。所绑定行为可通过 descriptor 协议被自定义的 &lt;code&gt;__ge
      
    
    </summary>
    
      <category term="语言" scheme="http://www.lyyyuna.com/categories/%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="Python" scheme="http://www.lyyyuna.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Writing Your Own Windows Debugger - Debug Event</title>
    <link href="http://www.lyyyuna.com/2017/05/01/write-a-windows-debugger-02-debug-event/"/>
    <id>http://www.lyyyuna.com/2017/05/01/write-a-windows-debugger-02-debug-event/</id>
    <published>2017-05-01T13:36:25.000Z</published>
    <updated>2017-11-29T00:19:00.169Z</updated>
    
    <content type="html"><![CDATA[<p>We have introduced the debug loop last time, in this post, I will talk about various debug events.</p><h3 id="RIP-EVENT"><a href="#RIP-EVENT" class="headerlink" title="RIP_EVENT"></a>RIP_EVENT</h3><p>I find very few documents about this event, only mentioned with words like <em>system error</em> or <em>internal error</em>. So I decide to print a error message and skip it. As my project is not fully tested, I have never<br>encountered such a situation.</p><h3 id="OUTPUT-DEBUG-STRING-EVENT"><a href="#OUTPUT-DEBUG-STRING-EVENT" class="headerlink" title="OUTPUT_DEBUG_STRING_EVENT"></a>OUTPUT_DEBUG_STRING_EVENT</h3><p>When the debuggee calls the <em>OutpuDebugString</em> function, it will raise this debug event. The following structure describes the detail of this event:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> _<span class="title">OUTPUT_DEBUG_STRING_INFO</span> &#123;</span></span><br><span class="line">  LPSTR lpDebugStringData;</span><br><span class="line">  WORD  fUnicode;</span><br><span class="line">  WORD  nDebugStringLength;</span><br><span class="line">&#125; OUTPUT_DEBUG_STRING_INFO, *LPOUTPUT_DEBUG_STRING_INFO;</span><br></pre></td></tr></table></figure><ul><li>lpDebugStringData, The debugging string in the calling process’s address space.</li><li>fUnicode, The format of the debugging string. If this member is zero, the debugging string is ANSI; if it is nonzero, the string is Unicode.</li><li>nDebugStringLength, The size of the debugging string, in characters. The length includes the string’s terminating null character.</li></ul><p>With ReadProcessMemory function, the debugger can obtain the value of the string:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">OnOutputDebugString</span><span class="params">(<span class="keyword">const</span> OUTPUT_DEBUG_STRING_INFO* pInfo)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    BYTE* pBuffer = (BYTE*)<span class="built_in">malloc</span>(pInfo-&gt;nDebugStringLength);</span><br><span class="line"></span><br><span class="line">    SIZE_T bytesRead;</span><br><span class="line"></span><br><span class="line">    ReadProcessMemory(</span><br><span class="line">        g_hProcess,</span><br><span class="line">        pInfo-&gt;lpDebugStringData,</span><br><span class="line">        pBuffer, </span><br><span class="line">        pInfo-&gt;nDebugStringLength,</span><br><span class="line">        &amp;bytesRead);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> requireLen = MultiByteToWideChar(</span><br><span class="line">        CP_ACP,</span><br><span class="line">        MB_PRECOMPOSED,</span><br><span class="line">        (LPCSTR)pBuffer,</span><br><span class="line">        pInfo-&gt;nDebugStringLength,</span><br><span class="line">        <span class="literal">NULL</span>,</span><br><span class="line">        <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    TCHAR* pWideStr = (TCHAR*)<span class="built_in">malloc</span>(requireLen * <span class="keyword">sizeof</span>(TCHAR));</span><br><span class="line"></span><br><span class="line">    MultiByteToWideChar(</span><br><span class="line">        CP_ACP,</span><br><span class="line">        MB_PRECOMPOSED,</span><br><span class="line">        (LPCSTR)pBuffer,</span><br><span class="line">        pInfo-&gt;nDebugStringLength,</span><br><span class="line">        pWideStr,</span><br><span class="line">        requireLen);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">std</span>::wcout &lt;&lt; TEXT(<span class="string">"Debuggee debug string: "</span>) &lt;&lt; pWideStr &lt;&lt;  <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">free</span>(pWideStr);</span><br><span class="line">    <span class="built_in">free</span>(pBuffer);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="LOAD-DLL-DEBUG-EVENT"><a href="#LOAD-DLL-DEBUG-EVENT" class="headerlink" title="LOAD_DLL_DEBUG_EVENT"></a>LOAD_DLL_DEBUG_EVENT</h3><p>After the debuggee loads a dll, this debug event will be triggered. The following structure describes the detail of this event:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> _<span class="title">LOAD_DLL_DEBUG_INFO</span> &#123;</span></span><br><span class="line">  HANDLE hFile;</span><br><span class="line">  LPVOID lpBaseOfDll;</span><br><span class="line">  DWORD  dwDebugInfoFileOffset;</span><br><span class="line">  DWORD  nDebugInfoSize;</span><br><span class="line">  LPVOID lpImageName;</span><br><span class="line">  WORD   fUnicode;</span><br><span class="line">&#125; LOAD_DLL_DEBUG_INFO, *LPLOAD_DLL_DEBUG_INFO;</span><br></pre></td></tr></table></figure><p>You may want to use the member <em>lpImageName</em> to retrieve the dll file name, however, it doesn’t work. According the explaination on MSDN, this member is pointer to the file name of the associated <em>hFile</em>, it  may, in turn, either be NULL or point to the actual filename. Even it is not NULL, ReadProcessMemory may also return a NULL. As a result, this membor is not reliable.</p><p>It seems that there is no direct Windows API to get the filename from the file handle. Someone has tried <a href="http://blog.csdn.net/bodybo/archive/2006/08/28/1131346.aspx" target="_blank" rel="noopener">this way</a>.</p><h3 id="UNLOAD-DLL-DEBUG-EVENT"><a href="#UNLOAD-DLL-DEBUG-EVENT" class="headerlink" title="UNLOAD_DLL_DEBUG_EVENT"></a>UNLOAD_DLL_DEBUG_EVENT</h3><p>When a dll module is unloaded, this event will be triggered, nothing needs handled, just skip it.</p><h3 id="CREATE-PROCESS-DEBUG-EVENT"><a href="#CREATE-PROCESS-DEBUG-EVENT" class="headerlink" title="CREATE_PROCESS_DEBUG_EVENT"></a>CREATE_PROCESS_DEBUG_EVENT</h3><p>After the process is created, this is the first debug event. The following structure describes the detail of this event:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> _<span class="title">CREATE_PROCESS_DEBUG_INFO</span> &#123;</span></span><br><span class="line">  HANDLE                 hFile;</span><br><span class="line">  HANDLE                 hProcess;</span><br><span class="line">  HANDLE                 hThread;</span><br><span class="line">  LPVOID                 lpBaseOfImage;</span><br><span class="line">  DWORD                  dwDebugInfoFileOffset;</span><br><span class="line">  DWORD                  nDebugInfoSize;</span><br><span class="line">  LPVOID                 lpThreadLocalBase;</span><br><span class="line">  LPTHREAD_START_ROUTINE lpStartAddress;</span><br><span class="line">  LPVOID                 lpImageName;</span><br><span class="line">  WORD                   fUnicode;</span><br><span class="line">&#125; CREATE_PROCESS_DEBUG_INFO, *LPCREATE_PROCESS_DEBUG_INFO;</span><br></pre></td></tr></table></figure><p>We can use this structure to get the symbols of the debuggee program.</p><h3 id="EXIT-PROCESS-DEBUG-EVENT"><a href="#EXIT-PROCESS-DEBUG-EVENT" class="headerlink" title="EXIT_PROCESS_DEBUG_EVENT"></a>EXIT_PROCESS_DEBUG_EVENT</h3><p>When debuggee process exits, this event will be triggered. The following structure describe the detail of the event:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> _<span class="title">EXIT_PROCESS_DEBUG_INFO</span> &#123;</span></span><br><span class="line">  DWORD dwExitCode;</span><br><span class="line">&#125; EXIT_PROCESS_DEBUG_INFO, *LPEXIT_PROCESS_DEBUG_INFO;</span><br></pre></td></tr></table></figure><p>What we can do is to print the exit code.</p><h3 id="CREATE-THREAD-DEBUG-EVENT"><a href="#CREATE-THREAD-DEBUG-EVENT" class="headerlink" title="CREATE_THREAD_DEBUG_EVENT"></a>CREATE_THREAD_DEBUG_EVENT</h3><p>It is similar to the process create debug event.</p><h3 id="EXIT-THREAD-DEBUG-EVENT"><a href="#EXIT-THREAD-DEBUG-EVENT" class="headerlink" title="EXIT_THREAD_DEBUG_EVENT"></a>EXIT_THREAD_DEBUG_EVENT</h3><p>It is similar to the process exit debug event.</p><h3 id="EXCEPTION-DEBUG-EVENT"><a href="#EXCEPTION-DEBUG-EVENT" class="headerlink" title="EXCEPTION_DEBUG_EVENT"></a>EXCEPTION_DEBUG_EVENT</h3><p>It is the most important event of our debugger, I will cover it in the next post.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;We have introduced the debug loop last time, in this post, I will talk about various debug events.&lt;/p&gt;
&lt;h3 id=&quot;RIP-EVENT&quot;&gt;&lt;a href=&quot;#RIP-E
      
    
    </summary>
    
      <category term="系统" scheme="http://www.lyyyuna.com/categories/%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="Windows Debugger" scheme="http://www.lyyyuna.com/tags/Windows-Debugger/"/>
    
  </entry>
  
  <entry>
    <title>Writing Your Own Windows Debugger - Overview</title>
    <link href="http://www.lyyyuna.com/2017/04/27/write-a-windows-debugger-01-overview/"/>
    <id>http://www.lyyyuna.com/2017/04/27/write-a-windows-debugger-01-overview/</id>
    <published>2017-04-27T09:36:25.000Z</published>
    <updated>2017-11-29T00:19:00.169Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Debuggers are the apple of the hacker’s eye. We benefit a lot from the debugger, but few of us know the principle of it.</p><p>In the book <em>Gray Hat Python</em> , the author has constructed a simple debugger. However, it is too simple, it is only a machine language level debugger, and can only set basic breakpoints and show CPU register information. We also want to know how to </p><ul><li>Show source code </li><li>Set breakpoint based on lines, not memory address</li><li>Set Step In, Step Out, Step Over</li><li>Show stack trace</li><li>Show global and local variables</li></ul><p>In this Chinese blog <a href="http://www.cnblogs.com/zplutor/archive/2011/03/04/1971279.html" target="_blank" rel="noopener">Zplutor’s</a>, I find a excellent series which has covered most above topics. I decide to write a English blog about it, and I will turn his code into a C++ version.</p><p>Before getting started, let’s make some limitations:</p><ul><li>It is only a user mode debugger.</li><li>It is only a Windows debugger. Although the principle is quite same, but Windows has offered lots of convenient APIs. The implementation will be different on Linux.</li><li>It is only a terminal-based debugger.</li><li>Different from <em>Gray Hat Python</em> , the debugger will be implemented by C++.</li><li>The debuggee program is single thread.</li></ul><p>The modified debugger can be found <a href="https://github.com/lyyyuna/anotherDebugger" target="_blank" rel="noopener">here</a>. It is only tested under Windows 10 + Visual Studio 2013.</p><h2 id="To-Start-the-Debuggee-Program"><a href="#To-Start-the-Debuggee-Program" class="headerlink" title="To Start the Debuggee Program"></a>To Start the Debuggee Program</h2><p>The so-called user mode debugger is to debug the program in user mode. Windows has provided a series of open API for debugging, and they can be devided into three categories:</p><ul><li>API for starting the debuggee program</li><li>API for handling debug event during debug loop</li><li>API for inspecing and modifying debuggee program</li></ul><p>The first thing to do before debugging a program is to start it. On Windows, we use <em>CreateProcess</em> to start to program:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">STARTUPINFO startupinfo = &#123; <span class="number">0</span> &#125;;</span><br><span class="line">startupinfo.cb = <span class="keyword">sizeof</span>(startupinfo);</span><br><span class="line">PROCESS_INFORMATION processinfo = &#123; <span class="number">0</span> &#125;;</span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> creationflags = DEBUG_ONLY_THIS_PROCESS | CREATE_NEW_CONSOLE;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (CreateProcess(</span><br><span class="line">    <span class="string">"L:\\git_up\\anotherDebugger\\anotherDebugger\\Debug\\test.exe"</span>,</span><br><span class="line">    <span class="comment">//path,</span></span><br><span class="line">    <span class="literal">NULL</span>,</span><br><span class="line">    <span class="literal">NULL</span>,</span><br><span class="line">    <span class="literal">NULL</span>,</span><br><span class="line">    FALSE,</span><br><span class="line">    creationflags,</span><br><span class="line">    <span class="literal">NULL</span>,</span><br><span class="line">    <span class="literal">NULL</span>,</span><br><span class="line">    &amp;startupinfo,</span><br><span class="line">    &amp;processinfo) == FALSE)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"CreateProcess failed: "</span> &lt;&lt; GetLastError() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>DEBUG_ONLY_THIS_PROCESS means the subprocess of the debuggee will not be debugged. If you need subprocess, use DEBUG_PROCESS.</li><li>CREATE_NEW_CONSOLE means the debuggee’s and debugger’s output will be separated in two consoles.</li><li>If the debugger process exits, the debuggee will also exit.</li></ul><h2 id="Debugger-loop"><a href="#Debugger-loop" class="headerlink" title="Debugger loop"></a>Debugger loop</h2><p>The debugger loop is a bit like Windows GUI message loop, some operations and exceptions will stop the debuggee and send event to the debugger. We always use </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DEBUG_EVENT debugEvent;</span><br><span class="line">WaitForDebugEvent(&amp;debugEvent, INFINITE)</span><br></pre></td></tr></table></figure><p>to capture the debug event.</p><p>There are 9 debug event in total:</p><ul><li>CREATE_PROCESS_DEBUG_EVENT. Reports a create-process debugging event. </li><li>CREATE_THREAD_DEBUG_EVENT. Reports a create-thread debugging event.</li><li>EXCEPTION_DEBUG_EVENT. Reports an exception debugging event.</li><li>EXIT_PROCESS_DEBUG_EVENT. Reports an exit-process debugging event.</li><li>EXIT_THREAD_DEBUG_EVENT. Reports an exit-thread debugging event.</li><li>LOAD_DLL_DEBUG_EVENT. Reports a load-dynamic-link-library (DLL) debugging event.</li><li>OUTPUT_DEBUG_STRING_EVENT. Reports an output-debugging-string debugging event.</li><li>RIP_EVENT. Reports a RIP-debugging event (system debugging error).</li><li>UNLOAD_DLL_DEBUG_EVENT. Reports an unload-DLL debugging event. </li></ul><p>If the debug event has been handled correctly, then</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ContinueDebugEvent(debuggeeprocessID, debuggeethreadID, DBG_CONTINUE);</span><br></pre></td></tr></table></figure><p>to continue the debuggee process. Let’s combine the above to construct the debug loop:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (WaitForDebugEvent(&amp;debugEvent, INFINITE) == TRUE)</span><br><span class="line">&#123;</span><br><span class="line">    debuggeeprocessID = debugEvent.dwProcessId;</span><br><span class="line">    debuggeethreadID = debugEvent.dwThreadId;</span><br><span class="line">    <span class="keyword">if</span> (dispatchDebugEvent(debugEvent) == TRUE)</span><br><span class="line">    &#123;</span><br><span class="line">        ContinueDebugEvent(debuggeeprocessID, debuggeethreadID, FLAG.continueStatus);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">dispatchDebugEvent</span><span class="params">(<span class="keyword">const</span> DEBUG_EVENT &amp; debugEvent)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">switch</span> (debugEvent.dwDebugEventCode)</span><br><span class="line">    &#123;</span><br><span class="line">    <span class="keyword">case</span> CREATE_PROCESS_DEBUG_EVENT:</span><br><span class="line">        <span class="comment">// TBD</span></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> CREATE_THREAD_DEBUG_EVENT:</span><br><span class="line">        <span class="comment">// TBD</span></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> EXCEPTION_DEBUG_EVENT:</span><br><span class="line">        <span class="comment">// TBD</span></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> EXIT_PROCESS_DEBUG_EVENT:</span><br><span class="line">        <span class="comment">// TBD</span></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> EXIT_THREAD_DEBUG_EVENT:</span><br><span class="line">        <span class="comment">// TBD</span></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> LOAD_DLL_DEBUG_EVENT:</span><br><span class="line">        <span class="comment">// TBD</span></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> UNLOAD_DLL_DEBUG_EVENT:</span><br><span class="line">        <span class="comment">// TBD</span></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> OUTPUT_DEBUG_STRING_EVENT:</span><br><span class="line">        <span class="comment">// TBD</span></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> RIP_EVENT:</span><br><span class="line">        <span class="comment">// TBD</span></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"Unknown debug event."</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>In the next part of the series, I intend to give a brief introduction about the 9 debug events.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;Debuggers are the apple of the
      
    
    </summary>
    
      <category term="系统" scheme="http://www.lyyyuna.com/categories/%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="Windows Debugger" scheme="http://www.lyyyuna.com/tags/Windows-Debugger/"/>
    
  </entry>
  
  <entry>
    <title>分布式 B 站用户信息爬虫</title>
    <link href="http://www.lyyyuna.com/2017/04/24/bilibili-users-spider/"/>
    <id>http://www.lyyyuna.com/2017/04/24/bilibili-users-spider/</id>
    <published>2017-04-24T13:20:36.000Z</published>
    <updated>2017-11-29T00:19:00.162Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>上周末写了一个 B 站用户信息爬虫，怎么爬网页实在是没什么好讲的，所以这篇描述一下简单的分布式爬虫。</p><p>知乎和 B 站用户爬虫极其类似。它们都有两种爬取策略：</p><ul><li>从一个用户开始，爬取关注者和被关注者，将新找到的用户存储下来，然后再从其关注网络中遍历爬取新的用户。这种爬取策略类似广度优先搜素，根据著名的<a href="http://baike.baidu.com/link?url=w0Tr_YMnE4BHSLk8MN9QBaAlbAUS18BJrlq85ZuhNDYHcN4pQKXg9KIxJ6fMIcW-rr7pQbT3Ya02hlHfiFZVijScjomLbTfhvwwavVAN3XD4GQCjRACiVhza_tndVf0KUjhj1iYrBgvZ6mTe8UCGw_" target="_blank" rel="noopener">六度人脉理论</a>，我们可以将大部分用户都找出来。不过该方法会增加程序的复杂度，你得维护好已搜索用户和未搜索用户的列表，快速去重等。</li><li>大部分网站在建立用户时会使用自增字段，或者有一个字段其值会在特定的数字范围内。对于 B 站，每个人的主页就是很好的例子，<a href="http://space.bilibili.com/8222478/#!/" target="_blank" rel="noopener">http://space.bilibili.com/8222478/#!/</a>。而对于知乎，这个值藏的比较隐秘。后来我发现，当一个用户关注你之后，便会邮件一个关注者链接给你，例如 <a href="http://www.zhihu.com/l/G4gQn" target="_blank" rel="noopener">http://www.zhihu.com/l/G4gQn</a>，最后是一个 16 进制的五位数，大部分用户都是 G，E 等开头的数字。这种链接在知乎正式页面没有发现过，猜测是历史原因保留了这种形式。</li></ul><p>自增数字的方式程序更容易实现。</p><h2 id="换代理的痛点"><a href="#换代理的痛点" class="headerlink" title="换代理的痛点"></a>换代理的痛点</h2><p>剩下的问题就是反爬虫，简单的换 header 头部就不说了，因为 B 站和知乎都是根据 IP 来反爬，那么我能想到的就只有：</p><ul><li>换代理</li><li>分布式爬虫</li></ul><p>从实现方式看，代理也能看作是一个分布式爬虫。根据 HTTP 返回的状态码，可以判断：</p><ul><li>代理是否正常工作（比如连接 timeout），</li><li>对方服务器是否判定我们访问过于频繁（比如返回 403）。</li></ul><p>基于上述判断，可以实现一些复杂的代理切换策略：</p><ul><li>定时去免费代理网站下载新的代理节点，</li><li>负载均衡，将请求平均分配到每一个节点，保证不会访问过于频繁，</li><li>代理探活，将不活跃的代理清除。</li></ul><p>不幸的是，免费好用的代理非常难找，使用 scrapy 配上<a href="https://github.com/lyyyuna/bilibili_papapa/commit/632a8827c187aa051186089d715616db8ae7fd86" target="_blank" rel="noopener">我写的几百行的代理中间件</a>，也很难维持一个正常的下载速率。<br>用 Python 来爬虫几乎是网上教程的标配了 -_-，导致即使找到一个可用代理，其 IP 也早被列入服务器的黑名单。</p><h2 id="分布式爬虫"><a href="#分布式爬虫" class="headerlink" title="分布式爬虫"></a>分布式爬虫</h2><h3 id="基于-HTTP-服务器实现分布式"><a href="#基于-HTTP-服务器实现分布式" class="headerlink" title="基于 HTTP 服务器实现分布式"></a>基于 HTTP 服务器实现分布式</h3><p><img src="https://github.com/lyyyuna/blog_img/raw/master/blog/201704/http_structure.png" alt="基于 HTTP 服务器的分布式爬虫"></p><ul><li>主从之间通信借助现有的 HTTP 协议。</li><li>主控 HTTP 服务器生成用户 id，每当有一个新的 GET 请求，自增 id，并返回给客户端。</li><li>每一个爬虫在这个结构中是一个客户端，当获取到新的 id 后，便去爬取解析网页，将结果以 POST 请求的方式返回给 HTTP 服务器。</li></ul><p>优点是：</p><ul><li>HTTP 协议简单，用 Flask 框架 5 分钟即能快速搭建。</li><li>客户端数量扩展极其方便，且不需要公网 IP。</li><li>爬虫下载速率可由每个客户端各自控制。</li></ul><p>缺点是：</p><ul><li>由于 HTTP 是无状态协议，服务器端不能跟踪一个发布的 id 是否已被正确处理。有可能一个客户端只 GET 而不 POST。需要维护另外的队列来存放未完成的任务。</li><li>HTTP 服务器发布任务是被动的。</li></ul><p>这个链接<a href="https://github.com/lyyyuna/zhihu_user_papapa" target="_blank" rel="noopener">https://github.com/lyyyuna/zhihu_user_papapa</a>里是用这种思路实现的 B 站用户爬虫。</p><h3 id="基于消息队列实现分布式"><a href="#基于消息队列实现分布式" class="headerlink" title="基于消息队列实现分布式"></a>基于消息队列实现分布式</h3><p><img src="https://github.com/lyyyuna/blog_img/raw/master/blog/201704/messagequeue_structure.png" alt="基于消息队列实现分布式爬虫"></p><p>采用消息队列后，Producer 主控侧不再是被动地发布任务，而是主动推送任务。上一小节有的优点消息队列同样拥有，同时</p><ul><li>Producer 也能控制任务发布的速率。</li><li>利用消息队列的持久化能力，可以在意外退出的情况下，记录未能成功发布的任务和未能成功接收的结果。</li></ul><p>这种结构的分布式爬虫，同样需要显示地维护一来一回蓝色红色的数据流，还是稍显复杂。</p><h3 id="基于-Celery-任务队列实现分布式"><a href="#基于-Celery-任务队列实现分布式" class="headerlink" title="基于 Celery 任务队列实现分布式"></a>基于 Celery 任务队列实现分布式</h3><p><img src="https://github.com/lyyyuna/blog_img/raw/master/blog/201704/celery_structure.png" alt="基于 Celery 实现分布式爬虫"></p><p>初看上去，这个结构和上一节没有区别，但是，上图的红色数据流是不需要显示维护的！</p><p>举一个简单例子。假设在机器 A，有如下的 worker</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> celery <span class="keyword">import</span> Celery</span><br><span class="line"></span><br><span class="line">app = Celery(<span class="string">'tasks'</span>, broker=<span class="string">'pyamqp://guest@localhost//'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.task</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x + y</span><br></pre></td></tr></table></figure><p>那么在另一台机器 B，只要运行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tasks <span class="keyword">import</span> add</span><br><span class="line">result = add.delay(<span class="number">4</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>就能得到结果。同时</p><ul><li>这个被 @app.task 修饰过的方法是异步的。机器 A 可以通过 result.ready() 来来获知任务是否被执行完。通过 result.get() 得到执行的结果。</li></ul><p>基于这个思想，我完成了这个分布式爬虫<a href="https://github.com/lyyyuna/bilibili_papapa" target="_blank" rel="noopener">https://github.com/lyyyuna/bilibili_papapa</a>。只需要 Celery 的 broker 具有公网 IP，然后把程序扔给朋友让他们帮我跑就行。唯一的不便是 Celery worker 在个人电脑上部署不便，而基于 HTTP 的分布式爬虫，我只要 C# 写个 HTTP 客户端生成 exe 就可以了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;上周末写了一个 B 站用户信息爬虫，怎么爬网页实在是没什么好讲的，所以这篇描述一下简单的分布式爬虫。&lt;/p&gt;
&lt;p&gt;知乎和 B 站用户爬虫极
      
    
    </summary>
    
      <category term="网络" scheme="http://www.lyyyuna.com/categories/%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="Celery" scheme="http://www.lyyyuna.com/tags/Celery/"/>
    
      <category term="爬虫" scheme="http://www.lyyyuna.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>使用 Supervisor 管理进程</title>
    <link href="http://www.lyyyuna.com/2017/04/08/supervisor01/"/>
    <id>http://www.lyyyuna.com/2017/04/08/supervisor01/</id>
    <published>2017-04-08T09:36:25.000Z</published>
    <updated>2017-11-29T00:19:00.169Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>如果需要让某一个进程长期运行，该怎么做？</p><ul><li>开一个终端，SSH 连上之后不关机。</li><li>Shell 命令加一个 &amp;，把进程扔到后台。</li><li>写一个 daemon 进程。</li><li>..</li></ul><p>当终端关闭，终端下所有的进程也会被相应的杀死，即使是被扔到后台执行的 job。然而，要把自己的应用程序专门写成 daemon，会增加开发的负担。这时候，一种万能的、对原应用侵入最小的方法，Supervisor，便走进了我们的视线。</p><p>Supervisor 可不光具有后台长期执行程序的功能。先举两个实际的例子。</p><ul><li>我所在组的产品是一个邮件网关，内含七八个扫描引擎，每种引擎都会起数个进程。为了监控和管理这些进程，我们写了很多 Shell 脚本，并用一个看门狗进程来监控进程对应的 pid 文件，一旦进程意外死亡，会被看门狗拉起来。</li><li>上周末为了写一个 Django + celery + redis 的例子，开了四五个终端，由于是在 virtualenv 下开发的，每次开终端都是一堆重复的 activate 过程。</li></ul><p>这些都可以通过 Supervisor，以类似 rc.d 脚本的方式，一劳永逸的解决。</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>Supervisor 是由 Python 写的，安装十分简单。</p><pre><code>pip install supervisor</code></pre><p>目前只支持 Python2 (&gt;2.4)。</p><p>不过我建议使用包管理器来安装，例如 ubuntu，</p><pre><code>apt install supervisor</code></pre><p>这样安装完以后会有一个默认的配置文件生成在</p><pre><code>/etc/supervisor/supervisord.conf</code></pre><h2 id="配置一个后台进程"><a href="#配置一个后台进程" class="headerlink" title="配置一个后台进程"></a>配置一个后台进程</h2><p>Supervisor 会按以下顺序搜索配置文件，</p><ul><li>$CWD/supervisord.conf</li><li>$CWD/etc/supervisord.conf</li><li>/etc/supervisord.conf</li><li>/etc/supervisor/supervisord.conf (since Supervisor 3.3.0)</li><li>../etc/supervisord.conf (Relative to the executable)</li><li>../supervisord.conf (Relative to the executable)</li></ul><p>配置文件是 Windows 的 INI 格式，我们撇开其他节，直奔主题 [program:x] </p><p>假设有一个循环打印 hello 的程序，使用 virtualenv 中的 Python 环境运行，现在需要其在后台常驻运行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /root/test/hello.py</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Hello, world.'</span></span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>我们添加一个 [program:x] 小节为</p><pre><code>[program:hellotest]command = /root/test/venv/bin/python -u hello.pydirectory = /root/testuser = rootstdout_logfile = /root/test/hello.logredirect_stderr = trueautostart = falseautorestart = true</code></pre><p>注意要添加 -u 启动参数，不然 stdout 上的输出会被一直缓存。首先启动 Supervisor 进程本身，安装的时候其本身已经被添加为 Linux 系统的一个 service</p><pre><code># service supervisor start</code></pre><p>然后使用 supervisorctl 工具来启动我们的 hellotest</p><pre><code># supervisorctl start hellotesthellotest: started</code></pre><p>查询 hellotest 的运行状态</p><pre><code># supervisorctl status hellotesthellotest                        RUNNING   pid 898, uptime 0:02:01</code></pre><p>查看 stdout 上的输出</p><pre><code># tailf test/hello.log Hello, world.Hello, world.Hello, world.Hello, world.Hello, world.Hello, world.Hello, world.Hello, world.Hello, world.Hello, world.Hello, world.</code></pre><p>如果我们的参数配置错误，还可以查看 Supervisor 自身的 log</p><pre><code>/var/log/supervisor/supervisor.log</code></pre><h2 id="配置一组后台进程"><a href="#配置一组后台进程" class="headerlink" title="配置一组后台进程"></a>配置一组后台进程</h2><p>配置一组后台进程与之类似，首先我们需要多个 [program:x] 小节</p><pre><code>[program:hellotest]command = /root/test/venv/bin/python -u hello.pydirectory = /root/testuser = rootstdout_logfile = /root/test/hello.logredirect_stderr = trueautorestart = trueautostart = false[program:hellotest2]command = /root/test/venv/bin/python -u hello2.pydirectory = /root/testuser = rootstdout_logfile = /root/test/hello.logredirect_stderr = trueautorestart = trueautostart = false[group:hellogroup]programs = hellotest, hellotest2</code></pre><p>启动一组中所有进程时，命令有些不同</p><pre><code>supervisorctl start hellogroup:*</code></pre><p>一旦一个 program 被加入组中，你就不能再用原先的命令启动</p><pre><code># supervisorctl start hellotesthellotest: ERROR (no such process)# supervisorctl start hellogroup:hellotest</code></pre><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p>我们可以看一下进程的 pid 号来验证我们的 hello 进程确实是 Supervisor 的子进程</p><pre><code># ps -ef | grep 1182root      1182     1  0 16:07 ?        00:00:00 /usr/bin/python /usr/bin/supervisord -n -c /etc/supervisor/supervisord.confroot      1226  1182  0 16:12 ?        00:00:00 /root/test/venv/bin/python -u hello2.pyroot      1227  1182  0 16:12 ?        00:00:00 /root/test/venv/bin/python -u hello.py</code></pre><p>再用 kill 命令验证 Supervisor 具有看门狗功能</p><pre><code># kill -9 1226# ps -ef | grep 1182root      1182     1  0 16:07 ?        00:00:00 /usr/bin/python /usr/bin/supervisord -n -c /etc/supervisor/supervisord.confroot      1227  1182  0 16:12 ?        00:00:00 /root/test/venv/bin/python -u hello.pyroot      1255  1182  0 16:18 ?        00:00:00 /root/test/venv/bin/python -u hello2.py</code></pre><p>hello2.py 已经是新的 pid 号。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;如果需要让某一个进程长期运行，该怎么做？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;开一个终端，SSH 连上之后不关机。&lt;/li&gt;
&lt;li&gt;Shell 命令
      
    
    </summary>
    
      <category term="系统" scheme="http://www.lyyyuna.com/categories/%E7%B3%BB%E7%BB%9F/"/>
    
    
  </entry>
  
  <entry>
    <title>c++ 函数重载是如何实现的</title>
    <link href="http://www.lyyyuna.com/2016/12/22/insidecppmodel2/"/>
    <id>http://www.lyyyuna.com/2016/12/22/insidecppmodel2/</id>
    <published>2016-12-22T12:35:53.000Z</published>
    <updated>2017-11-29T02:08:03.768Z</updated>
    
    <content type="html"><![CDATA[<p>函数重载是 c++ 的编译时多态的一部分，也就是说，该行为在编译完成后即是确定的。事实上，这是编译器和链接器之间玩的小花招。链接器通过符号（symbol）定位各个函数，所谓符号可以简单理解为一个字符串。</p><p>编译器会给每个函数名一个符号，在 c 语言中，符号名只和函数名有关。</p><p>来一个 c 语言程序的例子，使用 Visual Studio 编译</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">add(<span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>用 Visual Studio 自带的工具 dumpbin 查看 .obj 文件的符号表</p><pre><code>017 00000000 SECT4  notype ()    External     | _add</code></pre><p>我们换一个函数声明</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">double</span> a, <span class="keyword">double</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">add(<span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>再用 dumpbin 查看 .obj 文件的符号表</p><pre><code>017 00000000 SECT4  notype ()    External     | _add</code></pre><p>还是同样的符号。所以， c 语言编译器不支持函数重载，函数名相同的话，链接器永远只能看到一个名字。</p><p>那么，c++ 呢？</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">double</span> a, <span class="keyword">double</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="built_in">string</span> a, <span class="built_in">string</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">add(<span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line">add(<span class="number">1.0</span>, <span class="number">2.0</span>);</span><br><span class="line">add(<span class="built_in">string</span>(<span class="string">"1"</span>), <span class="built_in">string</span>(<span class="string">"2"</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>再用 dumpbin 查看 .obj 文件的符号表</p><pre><code>2F3 00000000 SECT87 notype ()    External     | ?add@@YAXHH@Z (void __cdecl add(int,int))2F4 00000000 SECT89 notype ()    External     | ?add@@YAXNN@Z (void __cdecl add(double,double))2F5 00000000 SECT8B notype ()    External     | ?add@@YAXV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@0@Z (void __cdecl add(class std::basic_string&lt;char,struct std::char_traits&lt;char&gt;,class std::allocator&lt;char&gt; &gt;,class std::basic_string&lt;char,struct std::char_traits&lt;char&gt;,class std::allocator&lt;char&gt; &gt;))</code></pre><p>可以看到，每个符号都不一样啦。这时候的函数声明不仅和函数名有关，也和参数类型有关，但和返回类型无关。符号能唯一确定，编译器自然也能顺利实现重载。</p><p>顺便可以发现，同一个函数声明在 c 和 c++ 中是完全不一样的。这也是为什么 c 和 c++ 之间动静态库不能直接互相调用的原因。为此 cpp 使用了 extern “C” 语法，强制使用 c++ 编译器使用 c 语言的符号命名方法。</p><p>我们实验一下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">extern</span> <span class="string">"C"</span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">double</span> a, <span class="keyword">double</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="built_in">string</span> a, <span class="built_in">string</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">add(<span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line">add(<span class="number">1.0</span>, <span class="number">2.0</span>);</span><br><span class="line">add(<span class="built_in">string</span>(<span class="string">"1"</span>), <span class="built_in">string</span>(<span class="string">"2"</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查看 .obj 文件的符号表</p><pre><code>2F3 00000000 SECTD3 notype ()    External     | _add2F4 00000000 SECT87 notype ()    External     | ?add@@YAXNN@Z (void __cdecl add(double,double))2F5 00000000 SECT89 notype ()    External     | ?add@@YAXV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@0@Z (void __cdecl add(class std::basic_string&lt;char,struct std::char_traits&lt;char&gt;,class std::allocator&lt;char&gt; &gt;,class std::basic_string&lt;char,struct std::char_traits&lt;char&gt;,class std::allocator&lt;char&gt; &gt;))</code></pre><p>可以看到，第一个函数的符号和 c 语言一致了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;函数重载是 c++ 的编译时多态的一部分，也就是说，该行为在编译完成后即是确定的。事实上，这是编译器和链接器之间玩的小花招。链接器通过符号（symbol）定位各个函数，所谓符号可以简单理解为一个字符串。&lt;/p&gt;
&lt;p&gt;编译器会给每个函数名一个符号，在 c 语言中，符号名只和
      
    
    </summary>
    
      <category term="语言" scheme="http://www.lyyyuna.com/categories/%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="cpp" scheme="http://www.lyyyuna.com/tags/cpp/"/>
    
  </entry>
  
  <entry>
    <title>c++ 虚函数是如何实现的</title>
    <link href="http://www.lyyyuna.com/2016/12/21/insidecppmodel1/"/>
    <id>http://www.lyyyuna.com/2016/12/21/insidecppmodel1/</id>
    <published>2016-12-21T12:35:53.000Z</published>
    <updated>2017-11-29T00:19:00.164Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>探索 c++ 对象内部的实现是一件非常有趣的事情。c++ 分为编译时多态和运行时多态。运行时多态依赖于虚函数，大部分人或许听说过虚函数是由虚函数表+虚函数指针实现的，但，真的是这样吗？虽然 c++ 规范有着复杂的语言细节，但底层实现机制却任由编译器厂商想象。（没准某种特殊的处理器电路结构原生支持虚函数，没准这个处理器压根不是冯纽曼型，或者将来厂商发明了比虚函数表更有效率的数据结构。）</p><p>本篇文章就来实际检验一下 Visual Studio 2013 编译器在无优化条件下，虚函数的实现。</p><h2 id="虚函数表"><a href="#虚函数表" class="headerlink" title="虚函数表"></a>虚函数表</h2><p>封装把实例的数据和操作结合在了一起，但实例本身只有数据，没有函数，同一个类的函数是共享的。我们通过一个例子来间接证明这一点</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base1</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="keyword">int</span> a;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">func</span><span class="params">()</span> </span>&#123; <span class="built_in">cout</span> &lt;&lt; <span class="string">"heel"</span> &lt;&lt; <span class="built_in">endl</span>; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">Base1 b1;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="keyword">sizeof</span>(b1) &lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure><p>打印</p><pre><code>4</code></pre><p>如果类中有虚函数，则会在对象中加入一个虚函数指针，该指针指向一个虚函数表，表中是各个虚函数的地址。</p><pre><code>+--------+       +---------+| pvtbl  |------&gt;| vfunc1  |+--------+       +---------+| data1  |       | vfunc2  |+--------+       +---------+| ...    |       | ...     |</code></pre><p>当子类继承父类时，会依次覆盖虚函数表中的各个项，如果子类没有重写某项，那该项就保留。当实例化对象后，虚函数指针就作为一个隐藏数据存在于实例中。如果通过父类指针调用普通成员函数，由于普通函数和类型绑定在一起，所以仍会调用父类成员函数；如果通过父类指针调用虚函数，则会通过对象的虚指针找到虚函数表（即子类的虚函数表），定位虚函数项，实现多态。</p><p>原理是不是很简单？c++ 就是通过这种看似原始的方式实现高级抽象。以上是编译器的通用做法，我手上的 Visual Studio 2013 编译器就是这么做的，为了提高性能，VS 保证虚函数指针存在于对象实例中最前面位置（历史上也有编译器不这么做，好像是 Borland 的？）。</p><h2 id="Visual-Studio-2013-中的实现"><a href="#Visual-Studio-2013-中的实现" class="headerlink" title="Visual Studio 2013 中的实现"></a>Visual Studio 2013 中的实现</h2><p>来一个例子（能这么写是因为我已知了 Visual Studio 2013 编译后对象的内存布局）</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span> </span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="keyword">typedef</span> <span class="title">void</span> <span class="params">(*func)</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">func1</span><span class="params">()</span> </span>&#123; <span class="built_in">cout</span> &lt;&lt; <span class="string">"Base::func1"</span> &lt;&lt; <span class="built_in">endl</span>; &#125;</span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">func2</span><span class="params">()</span> </span>&#123; <span class="built_in">cout</span> &lt;&lt; <span class="string">"Base::func2"</span> &lt;&lt; <span class="built_in">endl</span>; &#125;</span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">func3</span><span class="params">()</span> </span>&#123; <span class="built_in">cout</span> &lt;&lt; <span class="string">"Base::func3"</span> &lt;&lt; <span class="built_in">endl</span>; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived</span>:</span> <span class="keyword">public</span> Base</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">func1</span><span class="params">()</span> </span>&#123; <span class="built_in">cout</span> &lt;&lt; <span class="string">"Derived::func1"</span> &lt;&lt; <span class="built_in">endl</span>; &#125;</span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">func3</span><span class="params">()</span> </span>&#123; <span class="built_in">cout</span> &lt;&lt; <span class="string">"Derived::func3"</span> &lt;&lt; <span class="built_in">endl</span>; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">Base b, b1;</span><br><span class="line"><span class="keyword">int</span>** pvirtualtable1 = (<span class="keyword">int</span>**)&amp;b;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"Base object vtbl address: "</span> &lt;&lt; pvirtualtable1[<span class="number">0</span>] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">int</span>** pvirtualtable11 = (<span class="keyword">int</span>**)&amp;b1;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"another Base object vtbl address: "</span> &lt;&lt; pvirtualtable11[<span class="number">0</span>] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"function in virtual table"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; (Base::func)pvirtualtable1[<span class="number">0</span>][i] != <span class="literal">NULL</span>; ++i)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">auto</span> p = (Base::func)pvirtualtable1[<span class="number">0</span>][i];</span><br><span class="line">p();</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">Derived d;</span><br><span class="line"><span class="keyword">int</span>** pvirtualtable2 = (<span class="keyword">int</span>**)&amp;d;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"Derived object vtbl address: "</span> &lt;&lt; pvirtualtable2[<span class="number">0</span>] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"function in virtual table"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; (Base::func)pvirtualtable2[<span class="number">0</span>][i] != <span class="literal">NULL</span>; ++i)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">auto</span> p = (Base::func)pvirtualtable2[<span class="number">0</span>][i];</span><br><span class="line">p();</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>打印</p><pre><code>Base object pvtbl address: 0029DA58another Base object pvtbl address: 0029DA58function address in virtual tableBase::func1Base::func2Base::func3Derived object pvtbl address: 0029DB20function address in virtual tableDerived::func1Base::func2Derived::func3</code></pre><p>可以看到，同一类型不同实例的虚函数表是相同的，继承之后，子类有了自己的虚函数表，表也有相应的更新(Derived::func1, Derived::func3)，表中未重写的项还保留为原值(Base::func2)。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;探索 c++ 对象内部的实现是一件非常有趣的事情。c++ 分为编译时多态和运行时多态。运行时多态依赖于虚函数，大部分人或许听说过虚函数是由虚
      
    
    </summary>
    
      <category term="语言" scheme="http://www.lyyyuna.com/categories/%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="cpp" scheme="http://www.lyyyuna.com/tags/cpp/"/>
    
  </entry>
  
  <entry>
    <title>Python 访问外围作用域中的变量</title>
    <link href="http://www.lyyyuna.com/2016/09/10/python-nonlocal-variable/"/>
    <id>http://www.lyyyuna.com/2016/09/10/python-nonlocal-variable/</id>
    <published>2016-09-10T13:33:31.000Z</published>
    <updated>2017-11-29T00:19:00.165Z</updated>
    
    <content type="html"><![CDATA[<p>在表达式中引用变量时，Python 会按照如下的顺序遍历各个作用域，寻找该变量：</p><ol><li>当前函数作用域</li><li>任何外围作用域（比如包含当前函数的其他函数）</li><li>global 作用域，即代码所在的模块的作用域</li></ol><p>如果上述作用域内都找不到变量，就会报 NameError 异常。</p><p>但是对变量赋值时，规则会有所不同。</p><ol><li>如果当前作用域变量已存在，那么其值会被替换。</li><li>如果不存在，则会视为在当前作用域定义新变量，而不是向外围作用域中寻找。</li></ol><p>如下函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">function</span><span class="params">()</span>:</span></span><br><span class="line">    flag = <span class="keyword">True</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">()</span>:</span></span><br><span class="line">        flag = <span class="keyword">False</span></span><br><span class="line">    helper()</span><br><span class="line">    <span class="keyword">print</span> flag</span><br><span class="line"></span><br><span class="line">function()</span><br></pre></td></tr></table></figure><p>由于 helper 中变量是赋值，这里 flag 输出仍为 True。习惯了 c 语言之类静态类型语言，这种设计起初会感到困惑，但其可以有效地防止局部变量污染函数外的环境。</p><p>需求总是多样的，一定有程序员想在赋值时访问外围作用域。如果是 Python2，他可以这么做</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">function</span><span class="params">()</span>:</span></span><br><span class="line">    flag = [<span class="keyword">True</span>]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">()</span>:</span></span><br><span class="line">        flag[<span class="number">0</span>] = <span class="keyword">False</span></span><br><span class="line">    helper()</span><br><span class="line">    <span class="keyword">print</span> flag</span><br><span class="line"></span><br><span class="line">function()</span><br></pre></td></tr></table></figure><p>先用 flag[0] 是读操作，产生一次变量引用，寻找到外围作用域中 flag，这时候再赋值 flag[0] = False 便不会新定义变量了。</p><p>如果是 Python3，则可以使用 nonlocal 关键字。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">function</span><span class="params">()</span>:</span></span><br><span class="line">    flag = <span class="keyword">True</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">nonlocal</span> flag</span><br><span class="line">        flag = <span class="keyword">False</span></span><br><span class="line">    helper()</span><br><span class="line">    <span class="keyword">print</span> flag</span><br><span class="line"></span><br><span class="line">function()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在表达式中引用变量时，Python 会按照如下的顺序遍历各个作用域，寻找该变量：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;当前函数作用域&lt;/li&gt;
&lt;li&gt;任何外围作用域（比如包含当前函数的其他函数）&lt;/li&gt;
&lt;li&gt;global 作用域，即代码所在的模块的作用域&lt;/li&gt;
&lt;/ol&gt;
      
    
    </summary>
    
      <category term="语言" scheme="http://www.lyyyuna.com/categories/%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="Python" scheme="http://www.lyyyuna.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Robot Framework 模板与数据驱动测试</title>
    <link href="http://www.lyyyuna.com/2016/07/29/robotframework-template-and-data-driven/"/>
    <id>http://www.lyyyuna.com/2016/07/29/robotframework-template-and-data-driven/</id>
    <published>2016-07-29T11:59:52.000Z</published>
    <updated>2017-11-29T00:19:00.166Z</updated>
    
    <content type="html"><![CDATA[<p>Robot Framework 是关键字驱动的测试框架，虽然关键字驱动高度抽象了底层实现，减少维护成本，降低了对测试人员编程水平的需求，但在某些类型的测试中，数据驱动导向的测试用例比重多，比如常见的用户输入框就有海量的输入可能性。Robot Framework 提供了测试模板，可以将其转换为数据驱动的测试。</p><h2 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h2><p>如果有一个接受参数的关键字，那么它就可以被用作模板。下面的例子展示了这一点。</p><pre><code>*** Settings ***Test Setup        PrepareTest Template     Compare Two Number ${one} ${three}*** Test Cases ***Template test case    1    2    1    1    2    3    2    2*** Keywords ***Compare Two Number ${one} ${three}    Should Be Equal    ${one}    ${three}</code></pre><p>这里展示了比较两个数是否相等的例子，可以看到只需填入输入数据即可。你也可以用 [Template] 为每个 Test Case 单独指定模板。</p><h2 id="与循环执行的区别"><a href="#与循环执行的区别" class="headerlink" title="与循环执行的区别"></a>与循环执行的区别</h2><p>有人会问，能否运用循环语句来模拟上述行为呢？</p><p>首先，由于 Robot Framework 是个测试框架，编程能力被弱化不少，模板语法显得简洁 ^_^，然后，在用例中混入过多的执行控制流也不是推荐的行为（或者绝对地说，用例中就不应该有循环、判断语句）。</p><p>其次，模板是处于 continue on failure 模式中，某一项输入 Fail，还会继续执行其他输入。普通 Case 一旦有个语句 Fail，该 Case 就会 tear down。比如上面给的例子，第一行和第二行就会 Fail，实际执行结果如下：</p><pre><code># logStarting test: Testcases.UI Test.Template test case20160729 12:08:56.652 :  FAIL : 1 != 220160729 12:08:56.668 :  FAIL : 2 != 3Ending test:   Testcases.UI Test.Template test case# report========================================================Testcases                                                                                                                                                                ========================================================Testcases.UI Test                                                                                                                                                        ========================================================Template test case                                                                                                                                               | FAIL |Several failures occurred:1) 1 != 22) 2 != 3--------------------------------------------------------Testcases.UI Test                                                                                                                                                | FAIL |1 critical test, 0 passed, 1 failed1 test total, 0 passed, 1 failed========================================================Testcases                                                                                                                                                        | FAIL |1 critical test, 0 passed, 1 failed1 test total, 0 passed, 1 failed========================================================</code></pre><p>单个输入中的 Fail 不会中断执行流。</p><h2 id="一些不足"><a href="#一些不足" class="headerlink" title="一些不足"></a>一些不足</h2><p>这些不足是我自己感受，可能并不准确。</p><p>我自己在自动化测试中使用数据驱动测试方法，只是希望减轻手工编写的工作量，对于执行流上的步骤不想简化。我希望每个输入都能完整地走完一遍 Test Setup | Test Execuation | Test Teardown 过程，遗憾的是好像 Robot Framework 做不到。下面是例子，这里增加了一个 Test Setup。</p><pre><code>*** Settings ***Test Setup        PrepareTest Template     Compare Two Number ${one} ${three}*** Test Cases ***Template test case    1    2    1    1    2    3    2    2*** Keywords ***Compare Two Number ${one} ${three}    Should Be Equal    ${one}    ${three}Prepare    Log    hello, world</code></pre><p>可以看到 hello, world 只打印了一次。</p><pre><code>Starting test: Testcases.UI Test.Template test case20160729 12:08:56.652 :  INFO : hello, world20160729 12:08:56.652 :  FAIL : 1 != 220160729 12:08:56.668 :  FAIL : 2 != 3Ending test:   Testcases.UI Test.Template test case</code></pre><p>这就使得框架对测试输入有要求：输入数据不能对后续输入有影响。一旦在执行过程中有 Fail 发生，就无法用 Test Teardown 恢复测试环境（Run Keyword And Continue On Failure 关键字可能可以解决该问题，但这样写逻辑上并不清晰）。</p><p>使用 Library 导入的标准库和外部库也有问题，测试框架会为每个 Case 生成一个库的实例（即 Python, Java 类的实例），模板中每一行输入都共享一个实例，若是类中有全局变量，便会在各个输入之间产生干扰。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Robot Framework 是关键字驱动的测试框架，虽然关键字驱动高度抽象了底层实现，减少维护成本，降低了对测试人员编程水平的需求，但在某些类型的测试中，数据驱动导向的测试用例比重多，比如常见的用户输入框就有海量的输入可能性。Robot Framework 提供了测试模
      
    
    </summary>
    
      <category term="自动化测试" scheme="http://www.lyyyuna.com/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/"/>
    
    
      <category term="robot framework" scheme="http://www.lyyyuna.com/tags/robot-framework/"/>
    
  </entry>
  
  <entry>
    <title>基于 BP 神经网络的手写体数字识别 - 优化</title>
    <link href="http://www.lyyyuna.com/2016/06/30/handwritten-neural-net04/"/>
    <id>http://www.lyyyuna.com/2016/06/30/handwritten-neural-net04/</id>
    <published>2016-06-30T12:25:58.000Z</published>
    <updated>2017-11-29T00:19:00.164Z</updated>
    
    <content type="html"><![CDATA[<script type="text/x-mathjax-config">MathJax.Hub.Config({  TeX: { equationNumbers: { autoNumber: "AMS" } }});</script><p>目前为止，我们论述中，似乎手写数字图像本身并没有太多篇幅。这就是神经网络的特点，那 784 个像素点只是神经网络的输入，不需要任何图像处理。</p><p>95% 的识别率看起来很高了，但还有不少提升空间。本篇文章将介绍多种优化方法。</p><h2 id="交叉熵代价函数"><a href="#交叉熵代价函数" class="headerlink" title="交叉熵代价函数"></a>交叉熵代价函数</h2><p>理想情况下我们的神经网络能够快速地从错误中学习。但实际过程中却可能学习缓慢。让我们看下面这个例子：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/tikz28.png" alt="例子"></p><p>我们期望该神经元在输入 1 时输出 0。若神经元权重初始值为 0.6，偏移初始值为 0.9，则初始输出为 0.82，离预期输出还有一段距离。我们选择学习率 $\eta=0.15$，点击 <strong>Run</strong> 观察输出变化和二次代价函数的变化动画：</p><p><script type="text/javascript" src="//cdn.bootcss.com/paper.js/0.9.25/paper-full.min.js"></script></p><p><script type="text/paperscript" src="/customjs/saturation1.js" canvas="saturation1"><br></script></p><center><br><canvas id="saturation1" width="520" height="300"></canvas><br></center><p>可以看到，神经元一直在“学习进步”，且“进步”神速，最终的输出也接近于 0。现在将权重初始值和偏移初始值都设为 2.0，再点击 <strong>Run</strong> 观察动画：</p><p><script type="text/paperscript" src="/customjs/saturation2.js" canvas="saturation2"><br></script></p><center><br><canvas id="saturation2" width="520" height="300"></canvas><br></center><p>参数未变，结果造成学习速度减慢。仔细观察，开始的 150 个 epoch 权重和偏移几乎保持不变。过了这个点，神经元又变成了“进步”神速的好孩子。</p><p>我们经常把自学习与人类的学习作比较，这里神经元的学习过程显得反常。当人类发现自己错误的离谱时会学习较快，而大部分未优化的神经元却在错误中踌躇不前。</p><p>让我们来探究一下问题的缘由。神经元学习慢，等同于权重和偏移变化慢，等同于代价函数的偏导数 $\partial C/\partial w$ 和 $\partial C / \partial b$ 较小。我们的二次代价函数为</p><p>\begin{eqnarray}<br>  C = \frac{(y-a)^2}{2},<br>\label{54}<br>\end{eqnarray}</p><p>其中，$a$ 是当训练输入 $x=1$ 时神经元的输出，$y=0$ 是期望输出。将 $a=\sigma(z), z = wx+b$ 代入上式，并求取偏导数可得</p><p>\begin{eqnarray}<br>  \frac{\partial C}{\partial w} &amp; = &amp; (a-y)\sigma’(z) x = a \sigma’(z) \label{55}\\<br>  \frac{\partial C}{\partial b} &amp; = &amp; (a-y)\sigma’(z) = a \sigma’(z),<br>\label{56}<br>\end{eqnarray}</p><p>结合我们的 $\sigma$ 函数图像，即 sigmoid 函数图像：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201605/sigmoid_function.png" alt="sigmoid 函数"></p><p>当神经元的输出接近于 0 时，曲线变得很平缓，所以 $\sigma’(z)$ 的值很小，结合公式 (\ref{55}) 和 (\ref{56}) 可知，$\partial C/\partial w$ 和 $\partial C / \partial b$ 的值很小。</p><h3 id="介绍交叉熵代价函数"><a href="#介绍交叉熵代价函数" class="headerlink" title="介绍交叉熵代价函数"></a>介绍交叉熵代价函数</h3><p>假设我们要训练如下的神经元，输入变量为 $x_1, x_2, …$，对应的权重为 $w_1, w_2, …$，偏移为 $b$：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/tikz29.png" alt="多输入神经元"></p><p>其中输出是 $a=\sigma(z), z = \sum_j w_j x_j+b$。对此，我们定义该神经元的交叉熵代价函数为</p><p>\begin{eqnarray}<br>  C = -\frac{1}{n} \sum_x \left[y \ln a + (1-y ) \ln (1-a) \right],<br>\label{57}<br>\end{eqnarray}</p><p>其中，$n$ 是所有训练数据的总和，$x$ 和 $y$ 是相应的输入和期望输出。为什么公式 (\ref{57}) 可以作为代价函数？</p><p>首先，由于 $a$ 的取值在 0, 1 之间，$y \ln a + (1-y) \ln (1-a)$ 为负，取反后公式 (\ref{57}) 非负。然后，当实际输出 $a$ 接近期望输出 $y$ 时，交叉熵接近于 0。这两点是代价函数的基本条件。将 $a = \sigma(z)$ 代入公式 (\ref{57}) 并计算交叉熵对权重的偏导，得</p><p>\begin{eqnarray}<br>  \frac{\partial C}{\partial w_j} &amp; = &amp; -\frac{1}{n} \sum_x \left(<br>    \frac{y }{\sigma(z)} -\frac{(1-y)}{1-\sigma(z)} \right)<br>  \frac{\partial \sigma}{\partial w_j} \label{58}\\<br> &amp; = &amp; -\frac{1}{n} \sum_x \left(<br>    \frac{y}{\sigma(z)}<br>    -\frac{(1-y)}{1-\sigma(z)} \right)\sigma’(z) x_j.<br>\label{59}<br>\end{eqnarray}</p><p>合并成一个分母，得</p><p>\begin{eqnarray}<br>  \frac{\partial C}{\partial w_j} &amp; = &amp; \frac{1}{n}<br>  \sum_x \frac{\sigma’(z) x_j}{\sigma(z) (1-\sigma(z))}<br>  (\sigma(z)-y).<br>\label{60}<br>\end{eqnarray}</p><p>由于 $\sigma’(z) = \sigma(z)(1-\sigma(z))$，上式还可以抵消，进一步简化为</p><p>\begin{eqnarray}<br>  \frac{\partial C}{\partial w_j} =  \frac{1}{n} \sum_x x_j(\sigma(z)-y).<br>\label{61}<br>\end{eqnarray}</p><p>权重的学习速率由 $\sigma(z)-y$ 控制，误差越大，学习越快。二次代价函数 (\ref{55}) 中，正是由于 $\sigma’(z)$ 的存在，自学习的速率减慢，而公式 (\ref{61}) 消掉了这一项。同理，可得交叉熵对权重的偏导数为</p><p>\begin{eqnarray}<br>  \frac{\partial C}{\partial b} = \frac{1}{n} \sum_x (\sigma(z)-y).<br>\label{62}<br>\end{eqnarray}</p><p>同样，恼人的 $\sigma’(z)$ 也被消掉了。</p><p>让我们再来看一下之前动画，这次使用交叉熵作为代价函数，且学习率改为 $\eta=0.005$。第一个，权重初始值是 0.6，偏移初始值是 0.9，点击 <strong>Run</strong>。</p><p><script type="text/paperscript" src="/customjs/saturation3.js" canvas="saturation3"><br></script></p><center><br><canvas id="saturation3" width="520" height="300"></canvas><br></center><p>意料之中，学习速度还是很快。第二个，权重和偏移初始值都为 2，点击 <strong>Run</strong>。</p><p><script type="text/paperscript" src="/customjs/saturation4.js" canvas="saturation4"><br></script></p><center><br><canvas id="saturation4" width="520" height="300"></canvas><br></center><p>神经元还是学习迅速。你可能注意到了 $\eta$ 的变化，这会不会影响试验结果？其实，我们关心的不是神经元学习的绝对速度，而是学习速度本身的变化。</p><p>上述结论完全可以推广到多层多神经元的网络，定义交叉熵为</p><p>\begin{eqnarray}  C = -\frac{1}{n} \sum_x<br>  \sum_j \left[y_j \ln a^L_j + (1-y_j) \ln (1-a^L_j) \right].<br>\label{63}<br>\end{eqnarray}</p><p>那什么时候该用交叉熵而不是二次代价函数？对于 sigmoid 神经元，交叉熵几乎永远是更优选择，也被实践证明。</p><h3 id="柔性最大值传输-softmax"><a href="#柔性最大值传输-softmax" class="headerlink" title="柔性最大值传输 softmax"></a>柔性最大值传输 softmax</h3><p>通过将神经网络的输出由 sigmoid 换成 softmax 层可以进一步改善学习缓慢的问题。 </p><p>对于输出层，其权重输入为 $z^L_j = \sum_{k} w^L_{jk} a^{L-1}_k + b^L_j$，施加 softmax 函数，输出层激励为</p><p>\begin{eqnarray}<br>  a^L_j = \frac{e^{z^L_j}}{\sum_k e^{z^L_k}},<br>\label{78}<br>\end{eqnarray}</p><p>其中，分母是所有输出神经元输出之和。又是一个看起来意义不明的函数。如果我们将所有激励相加，会发现其值正好等于 1，</p><p>\begin{eqnarray}<br>  \sum_j a^L_j &amp; = &amp; \frac{\sum_j e^{z^L_j}}{\sum_k e^{z^L_k}} = 1.<br>\label{79}<br>\end{eqnarray}</p><p>当某一个激励增加时，其他的激励必须相应地减少以保证和不变。换句话说，如果将 softmax 作为输出层，神经网络的所有输出符合概率分布。这又是一个方便的特性，尤其对于手写数字识别来说，每个输出代表每个数字的概率，之前 sigmoid 的方案有可能会有如下的输出</p><pre><code>[0.9, 0.3, 0.4, 0.1, 0.0, 0.4, 0.0, 0.0, 0.0, 0.1]</code></pre><p>每个概率之间并没有联系，sigmoid 输出神经元只是各顾各的训练。而且人们拿到这个结果肯定会非常疑惑，为啥概率相加不等于 1？</p><h2 id="过拟合和正则化"><a href="#过拟合和正则化" class="headerlink" title="过拟合和正则化"></a>过拟合和正则化</h2><p>诺贝尔物理学奖获得者费米曾经和他的同事讨论一个数学模型。该模型能够很好地解释实验结果，但费米仍有疑虑。他问该模型用了多少个自由变量，同事回答四个。费米回答：“我记得我朋友冯诺依曼曾经说过，四个变量我能描述一头大象，五个变量就能让他转鼻子了”。</p><p>拥有大量自由变量的模型很容易就描述大部分实验现象。但是不能说符合实验现象的模型就是好模型。有足够自由变量的模型中，几乎可以描述任何给定大小的数据集，但没有抓住现象背后的本质。这种情况下，模型只能适用于现有数据，面对新的情况却束手无策。模型的真正考验，是它有能力对未出现的现象做出预言。</p><p>费米和诺依曼对四变量的模型就产生了质疑。而我们手写数字识别系统有 30 个隐藏神经元，有将近 24000 个变量！若是 100 个隐藏神经元，那就有近 80000 个变量！这么多变量，不禁要问，结果可信么？会出现费米和诺依曼担心的问题么？</p><p>让我们来模拟一下这种情况的发生。我们使用 30 个隐藏神经元，但我们不使用 50000 个 MNIST 训练图像，相反，只是用 1000 个训练图像。这样，问题会更显著。训练使用交叉熵函数，学习率 $\eta=0.5$，mini-batch 大小为 10，训练 400 个epochs。让我们用 <a href="https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network2.py" target="_blank" rel="noopener">network2</a> 来观察变化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mnist_loader </span><br><span class="line">training_data, validation_data, test_data = mnist_loader.load_data_wrapper()</span><br><span class="line"><span class="keyword">import</span> network2 </span><br><span class="line">net = network2.Network([<span class="number">784</span>, <span class="number">30</span>, <span class="number">10</span>], cost=network2.CrossEntropyCost) </span><br><span class="line">net.large_weight_initializer()</span><br><span class="line">net.SGD(training_data[:<span class="number">1000</span>], <span class="number">400</span>, <span class="number">10</span>, <span class="number">0.5</span>, evaluation_data=test_data, monitor_evaluation_accuracy=<span class="keyword">True</span>, monitor_training_cost=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>首先是代价函数随学习进度的变化图像：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/overfitting1.png" alt="代价函数变化"></p><p>看起来不错，代价不断减小，似乎说明我们的神经网络一直在进步。但是测试集上识别率却不是那么回事：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/overfitting2.png" alt="识别率变化"></p><p>280 个 epoch 之后，识别率处于波动稳定状态，且远低于之前达到的 95% 识别率。训练数据的交叉熵和测试集的实际结果截然不同，出现了费米担心的问题。可以说，280 个 epoch 之后的学习完全无用，标准说法是<strong>过拟合 overfitting</strong>。</p><p>让我们在做一点更直观的比较：训练集和测试集的交叉熵横向对比，及识别率横向对比。</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/overfitting3.png" alt="测试集交叉熵"></p><p>交叉熵仅仅下降了 15 个 epoch，之后就一路飙高，持续恶化。这是我们模型过拟合的又一个标志。这里有个小疑问，epoch 15 和 epoch 280 哪个属于开始过拟合？从实践的角度看，我们真正的关心的是测试集（更接近真实情况）上的识别率，交叉熵只是算法的附带物，所以我们认为，epoch 280 之后，过拟合开始占据神经网络的学习过程。</p><p>下面是训练集的识别率变化：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/overfitting4.png" alt="训练集识别率"></p><p>我们的模型能够 100% 地描述 1000 个训练图像，实际却不能很好地分类测试数字。</p><p>最明显的检测过拟合的方法是观察测试集上识别率的变化。如果发现测试集识别率不再改善，就应该停止训练。这也是<a href="http://www.lyyyuna.com/2016/06/25/handwritten-neural-net02/">之前文章-代码实现</a>为什么要再引入验证集的原因，毕竟测试集是最终判定结果用的，应该与训练过程彻底分离。</p><pre><code>training_data, validation_data, test_data = mnist_loader.load_data_wrapper()</code></pre><p>我们一直在讨论 1000 个训练图片的过拟合问题，那 50000 个图片结果还是一样吗？这里给出结果：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/overfitting_full.png" alt="扩大训练集"></p><p>可以看到，过拟合不再那么明显了，训练集的识别率只比测试集高 1.5% 左右。这也间接说明，大量训练数据下神经网络难以达到过拟合。不过训练集并不是那么容易获得的。</p><h3 id="正则化-Regularization"><a href="#正则化-Regularization" class="headerlink" title="正则化 Regularization"></a>正则化 Regularization</h3><p>首先要明确一点，我们并不想减少网络中变量的数目，我们需要这种特性来描述现实世界复杂的变化。</p><p>正则化方法能够缓解过拟合问题，最常用的是权重衰减法或者叫 $L_2$ 正则化。$L_2$ 正则化只是在原先的代价函数中加入一个正则项：</p><p>\begin{eqnarray}<br>C = -\frac{1}{n} \sum_{xj} \left[ y_j \ln a^L_j+(1-y_j) \ln<br>(1-a^L_j)\right] + \frac{\lambda}{2n} \sum_w w^2.<br>\label{85}<br>\end{eqnarray}</p><p>等式右边第一项是交叉熵，第二项是网络中所有权重的平方和，并乘以系数 $\lambda /2n$，其中 $\lambda &gt; 0$，称作正则化参数。</p><p>正则化不只适用于交叉熵代价函数，二次代价函数也可以使用：</p><p>\begin{eqnarray}<br>C = \frac{1}{2n} \sum_x |y-a^L|^2 +<br>  \frac{\lambda}{2n} \sum_w w^2.<br>\label{86}<br>\end{eqnarray}</p><p>总结下来就是</p><p>\begin{eqnarray}<br>C = C_0 + \frac{\lambda}{2n}<br>\sum_w w^2,<br>\label{87}<br>\end{eqnarray}</p><p>其中 $C_0$ 是未正则化的代价函数。观察该式，可以发现正则化逼迫自学习过程选择更小的权重，权重越大，代价也越高。由于代价函数的变换，随机梯度下降法中偏导数的计算也要随之改变：</p><p>\begin{eqnarray}<br>  \frac{\partial C}{\partial w} &amp; = &amp; \frac{\partial C_0}{\partial w} +<br>  \frac{\lambda}{n} w \label{88}\\<br>  \frac{\partial C}{\partial b} &amp; = &amp; \frac{\partial C_0}{\partial b}.<br>\label{89}<br>\end{eqnarray}</p><p>$\partial C_0 / \partial w$ 和 $\partial C_0 / \partial b$ 仍让可以用<a href="http://www.lyyyuna.com/2016/06/26/handwritten-neural-net03/">上一篇</a>的反向传播算法求得。对偏移的偏导数并没有改变，所以据梯度下降法学习规则仍为：</p><p>\begin{eqnarray}<br>b &amp; \rightarrow &amp; b -\eta \frac{\partial C_0}{\partial b}.<br>\label{90}<br>\end{eqnarray}</p><p>而权重的自学习规则则变成：</p><p>\begin{eqnarray}<br>  w &amp; \rightarrow &amp; w-\eta \frac{\partial C_0}{\partial<br>    w}-\frac{\eta \lambda}{n} w \label{91}\\<br>  &amp; = &amp; \left(1-\frac{\eta \lambda}{n}\right) w -\eta \frac{\partial<br>    C_0}{\partial w}.<br>\label{92}<br>\end{eqnarray}</p><p>可以看到，权重 $w$ 乘以了一个小于 1 的系数 $1-\frac{\eta \lambda}{n}$，称为权重衰减，有减小权重的趋势。而后一项由于偏导有正有负，所以权重值并不是单调递减，两项相加，彼此制约。</p><p>以上是梯度下降法，随机梯度下降法也只要做相应的调整：</p><p>\begin{eqnarray}<br>  w \rightarrow \left(1-\frac{\eta \lambda}{n}\right) w -\frac{\eta}{m}<br>  \sum_x \frac{\partial C_x}{\partial w},<br>\label{93}<br>\end{eqnarray}</p><p>\begin{eqnarray}<br>  b \rightarrow b - \frac{\eta}{m} \sum_x \frac{\partial C_x}{\partial b},<br>\label{94}<br>\end{eqnarray}</p><p>其中，求和是对一个 mini-batch 内所有数据的求和。</p><p>让我们实验一下。这次在 <a href="https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network2.py" target="_blank" rel="noopener">network2</a> 中加入正则化参数 $\lambda=0.1$。对比之前 1000 个训练数据集的结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mnist_loader </span><br><span class="line">training_data, validation_data, test_data = mnist_loader.load_data_wrapper() </span><br><span class="line"><span class="keyword">import</span> network2 </span><br><span class="line">net = network2.Network([<span class="number">784</span>, <span class="number">30</span>, <span class="number">10</span>], cost=network2.CrossEntropyCost)</span><br><span class="line">net.large_weight_initializer()</span><br><span class="line">net.SGD(training_data[:<span class="number">1000</span>], <span class="number">400</span>, <span class="number">10</span>, <span class="number">0.5</span>, </span><br><span class="line">        evaluation_data=test_data, lmbda = <span class="number">0.1</span>, </span><br><span class="line">        monitor_evaluation_cost=<span class="keyword">True</span>, monitor_evaluation_accuracy=<span class="keyword">True</span>, </span><br><span class="line">        monitor_training_cost=<span class="keyword">True</span>, monitor_training_accuracy=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>训练集的交叉熵代价看来没什么问题：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/regularized1.png" alt="训练集的代价"></p><p>但这次识别率却是一直在上升：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/regularized2.png" alt="识别率上升"></p><p>我们在试一下 50000 个训练数据的情况：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/regularized_full.png" alt="识别率"></p><p>训练集和测试集的识别率只差 1% 左右，而正则化之前这一值是 1.5%。</p><p>用以下参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net = network2.Network([<span class="number">784</span>, <span class="number">100</span>, <span class="number">10</span>], cost=network2.CrossEntropyCost)</span><br><span class="line">net.large_weight_initializer()</span><br><span class="line">net.SGD(training_data, <span class="number">60</span>, <span class="number">10</span>, <span class="number">0.1</span>, lmbda=<span class="number">5.0</span>,</span><br><span class="line">      evaluation_data=validation_data,</span><br><span class="line">      monitor_evaluation_accuracy=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>识别率提高到 98%。你可以认为，由于过拟合的存在，神经网络模型易陷入局部最优解，正则之后，跳出局部最优，滚向全局最优，最终带来识别率的提升。</p><h3 id="为什么正则化能抑制过拟合"><a href="#为什么正则化能抑制过拟合" class="headerlink" title="为什么正则化能抑制过拟合"></a>为什么正则化能抑制过拟合</h3><p>从正则化的结果来看，似乎权重值越小越能抑制过拟合。</p><p>让我们看一个经典的例子，假设要对下图所示的点建立一个模型：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/tenpoints1.png" alt="很多点"></p><p>数一下，有 10 个点，那可以用一个 9 次函数精确地描述它，$y = a_0 x^9 + a_1 x^8 + \ldots + a_9$：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/tenpoints2.png" alt="九次函数"></p><p>如果允许一些误差，也可以使用一个简单的线性模型：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/tenpoints3.png" alt="线性模型"></p><p>那么，哪个才是更好的模型？哪个才能描述还未出现的新点？实践表明，允许一定误差的模型更符合实际情况。现实世界伴随着大量不确定性，传感器采集的噪声和仪器本身的精度都会给训练集加入一定的<strong>噪声</strong>，这样，后一个模型便在预测新点时占据了优势。</p><p>回到我们的神经网络，当输入因为某些噪声剧烈变化时，较小的权值 $w$ 能够防止网络整体特性改变过大，网络也就不会去“学习”那些没用的噪声信息了。相反，对于手写数字图像那些重复的特征，神经网络在一遍遍的 mini-batch 中，“铭记在心”。</p><p>人们也称这个思想为<strong>奥卡姆剃刀原理</strong>：当两个假说具有完全相同的解释力和预测力时，我们以那个较为简单的假说作为讨论依据。</p><h3 id="其他抑制过拟合的方法"><a href="#其他抑制过拟合的方法" class="headerlink" title="其他抑制过拟合的方法"></a>其他抑制过拟合的方法</h3><p>当然还有很多抑制过拟合的方法，比如：</p><p><strong>$L_1$ 正则化</strong>，即换一个正则函数。</p><p><strong>dropout</strong>：学习过程中随机删去一些神经元。</p><p><strong>人工扩展训练集</strong>：这也是我比较喜欢的一个方法，可以通过平移、缩放、旋转、elastic distortions 等扩展数据集。扩展数据简单粗暴有效，微软研究院的研究员用 elastic distortions 扩展数据后，就将 MNIST 识别率提高到了 99.3%。</p><h2 id="改进权重初始化"><a href="#改进权重初始化" class="headerlink" title="改进权重初始化"></a>改进权重初始化</h2><p>我们在初始化权重和偏移时，选择高斯随机，均值为 0，标准差为 1。权重输入为 $z = \sum_j w_j x_j+b$，随着输入神经元数目的增加，标准差也随之增加，例如 1000 个神经元，其正太分布曲线为</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/gauss1.png" alt="正太分布"></p><p>曲线非常平坦，意味着 $z \gg 1, z \ll -1$ 的可能性都大大增加，输出 $\sigma(z)$ 极有可能饱和，出现过拟合的现象。解决的方法也非常简单，初始化时标准差选为 $1/\sqrt{n_{\rm in}}$。</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/gauss2.png" alt="改进的正太分布"></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>以下是 network2.py 的源码（当然我不是写的啦，<a href="http://michaelnielsen.org/" target="_blank" rel="noopener">Michael Nielsen</a> 的杰作），所用技术和算法已在上文逐一阐述。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""network2.py</span></span><br><span class="line"><span class="string">~~~~~~~~~~~~~~</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">An improved version of network.py, implementing the stochastic</span></span><br><span class="line"><span class="string">gradient descent learning algorithm for a feedforward neural network.</span></span><br><span class="line"><span class="string">Improvements include the addition of the cross-entropy cost function,</span></span><br><span class="line"><span class="string">regularization, and better initialization of network weights.  Note</span></span><br><span class="line"><span class="string">that I have focused on making the code simple, easily readable, and</span></span><br><span class="line"><span class="string">easily modifiable.  It is not optimized, and omits many desirable</span></span><br><span class="line"><span class="string">features.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Libraries</span></span><br><span class="line"><span class="comment"># Standard library</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment"># Third-party libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Define the quadratic and cross-entropy cost functions</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuadraticCost</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fn</span><span class="params">(a, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return the cost associated with an output ``a`` and desired output</span></span><br><span class="line"><span class="string">        ``y``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.5</span>*np.linalg.norm(a-y)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">delta</span><span class="params">(z, a, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return the error delta from the output layer."""</span></span><br><span class="line">        <span class="keyword">return</span> (a-y) * sigmoid_prime(z)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossEntropyCost</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fn</span><span class="params">(a, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return the cost associated with an output ``a`` and desired output</span></span><br><span class="line"><span class="string">        ``y``.  Note that np.nan_to_num is used to ensure numerical</span></span><br><span class="line"><span class="string">        stability.  In particular, if both ``a`` and ``y`` have a 1.0</span></span><br><span class="line"><span class="string">        in the same slot, then the expression (1-y)*np.log(1-a)</span></span><br><span class="line"><span class="string">        returns nan.  The np.nan_to_num ensures that that is converted</span></span><br><span class="line"><span class="string">        to the correct value (0.0).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> np.sum(np.nan_to_num(-y*np.log(a)-(<span class="number">1</span>-y)*np.log(<span class="number">1</span>-a)))</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">delta</span><span class="params">(z, a, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return the error delta from the output layer.  Note that the</span></span><br><span class="line"><span class="string">        parameter ``z`` is not used by the method.  It is included in</span></span><br><span class="line"><span class="string">        the method's parameters in order to make the interface</span></span><br><span class="line"><span class="string">        consistent with the delta method for other cost classes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> (a-y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Main Network class</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sizes, cost=CrossEntropyCost)</span>:</span></span><br><span class="line">        <span class="string">"""The list ``sizes`` contains the number of neurons in the respective</span></span><br><span class="line"><span class="string">        layers of the network.  For example, if the list was [2, 3, 1]</span></span><br><span class="line"><span class="string">        then it would be a three-layer network, with the first layer</span></span><br><span class="line"><span class="string">        containing 2 neurons, the second layer 3 neurons, and the</span></span><br><span class="line"><span class="string">        third layer 1 neuron.  The biases and weights for the network</span></span><br><span class="line"><span class="string">        are initialized randomly, using</span></span><br><span class="line"><span class="string">        ``self.default_weight_initializer`` (see docstring for that</span></span><br><span class="line"><span class="string">        method).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.num_layers = len(sizes)</span><br><span class="line">        self.sizes = sizes</span><br><span class="line">        self.default_weight_initializer()</span><br><span class="line">        self.cost=cost</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">default_weight_initializer</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Initialize each weight using a Gaussian distribution with mean 0</span></span><br><span class="line"><span class="string">        and standard deviation 1 over the square root of the number of</span></span><br><span class="line"><span class="string">        weights connecting to the same neuron.  Initialize the biases</span></span><br><span class="line"><span class="string">        using a Gaussian distribution with mean 0 and standard</span></span><br><span class="line"><span class="string">        deviation 1.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Note that the first layer is assumed to be an input layer, and</span></span><br><span class="line"><span class="string">        by convention we won't set any biases for those neurons, since</span></span><br><span class="line"><span class="string">        biases are only ever used in computing the outputs from later</span></span><br><span class="line"><span class="string">        layers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> self.sizes[<span class="number">1</span>:]]</span><br><span class="line">        self.weights = [np.random.randn(y, x)/np.sqrt(x)</span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(self.sizes[:<span class="number">-1</span>], self.sizes[<span class="number">1</span>:])]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">large_weight_initializer</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Initialize the weights using a Gaussian distribution with mean 0</span></span><br><span class="line"><span class="string">        and standard deviation 1.  Initialize the biases using a</span></span><br><span class="line"><span class="string">        Gaussian distribution with mean 0 and standard deviation 1.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Note that the first layer is assumed to be an input layer, and</span></span><br><span class="line"><span class="string">        by convention we won't set any biases for those neurons, since</span></span><br><span class="line"><span class="string">        biases are only ever used in computing the outputs from later</span></span><br><span class="line"><span class="string">        layers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        This weight and bias initializer uses the same approach as in</span></span><br><span class="line"><span class="string">        Chapter 1, and is included for purposes of comparison.  It</span></span><br><span class="line"><span class="string">        will usually be better to use the default weight initializer</span></span><br><span class="line"><span class="string">        instead.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> self.sizes[<span class="number">1</span>:]]</span><br><span class="line">        self.weights = [np.random.randn(y, x)</span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(self.sizes[:<span class="number">-1</span>], self.sizes[<span class="number">1</span>:])]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(self, a)</span>:</span></span><br><span class="line">        <span class="string">"""Return the output of the network if ``a`` is input."""</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            a = sigmoid(np.dot(w, a)+b)</span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, training_data, epochs, mini_batch_size, eta,</span></span></span><br><span class="line"><span class="function"><span class="params">            lmbda = <span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            evaluation_data=None,</span></span></span><br><span class="line"><span class="function"><span class="params">            monitor_evaluation_cost=False,</span></span></span><br><span class="line"><span class="function"><span class="params">            monitor_evaluation_accuracy=False,</span></span></span><br><span class="line"><span class="function"><span class="params">            monitor_training_cost=False,</span></span></span><br><span class="line"><span class="function"><span class="params">            monitor_training_accuracy=False)</span>:</span></span><br><span class="line">        <span class="string">"""Train the neural network using mini-batch stochastic gradient</span></span><br><span class="line"><span class="string">        descent.  The ``training_data`` is a list of tuples ``(x, y)``</span></span><br><span class="line"><span class="string">        representing the training inputs and the desired outputs.  The</span></span><br><span class="line"><span class="string">        other non-optional parameters are self-explanatory, as is the</span></span><br><span class="line"><span class="string">        regularization parameter ``lmbda``.  The method also accepts</span></span><br><span class="line"><span class="string">        ``evaluation_data``, usually either the validation or test</span></span><br><span class="line"><span class="string">        data.  We can monitor the cost and accuracy on either the</span></span><br><span class="line"><span class="string">        evaluation data or the training data, by setting the</span></span><br><span class="line"><span class="string">        appropriate flags.  The method returns a tuple containing four</span></span><br><span class="line"><span class="string">        lists: the (per-epoch) costs on the evaluation data, the</span></span><br><span class="line"><span class="string">        accuracies on the evaluation data, the costs on the training</span></span><br><span class="line"><span class="string">        data, and the accuracies on the training data.  All values are</span></span><br><span class="line"><span class="string">        evaluated at the end of each training epoch.  So, for example,</span></span><br><span class="line"><span class="string">        if we train for 30 epochs, then the first element of the tuple</span></span><br><span class="line"><span class="string">        will be a 30-element list containing the cost on the</span></span><br><span class="line"><span class="string">        evaluation data at the end of each epoch. Note that the lists</span></span><br><span class="line"><span class="string">        are empty if the corresponding flag is not set.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> evaluation_data: n_data = len(evaluation_data)</span><br><span class="line">        n = len(training_data)</span><br><span class="line">        evaluation_cost, evaluation_accuracy = [], []</span><br><span class="line">        training_cost, training_accuracy = [], []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(epochs):</span><br><span class="line">            random.shuffle(training_data)</span><br><span class="line">            mini_batches = [</span><br><span class="line">                training_data[k:k+mini_batch_size]</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">0</span>, n, mini_batch_size)]</span><br><span class="line">            <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">                self.update_mini_batch(</span><br><span class="line">                    mini_batch, eta, lmbda, len(training_data))</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"Epoch %s training complete"</span> % j</span><br><span class="line">            <span class="keyword">if</span> monitor_training_cost:</span><br><span class="line">                cost = self.total_cost(training_data, lmbda)</span><br><span class="line">                training_cost.append(cost)</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Cost on training data: &#123;&#125;"</span>.format(cost)</span><br><span class="line">            <span class="keyword">if</span> monitor_training_accuracy:</span><br><span class="line">                accuracy = self.accuracy(training_data, convert=<span class="keyword">True</span>)</span><br><span class="line">                training_accuracy.append(accuracy)</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Accuracy on training data: &#123;&#125; / &#123;&#125;"</span>.format(</span><br><span class="line">                    accuracy, n)</span><br><span class="line">            <span class="keyword">if</span> monitor_evaluation_cost:</span><br><span class="line">                cost = self.total_cost(evaluation_data, lmbda, convert=<span class="keyword">True</span>)</span><br><span class="line">                evaluation_cost.append(cost)</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Cost on evaluation data: &#123;&#125;"</span>.format(cost)</span><br><span class="line">            <span class="keyword">if</span> monitor_evaluation_accuracy:</span><br><span class="line">                accuracy = self.accuracy(evaluation_data)</span><br><span class="line">                evaluation_accuracy.append(accuracy)</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Accuracy on evaluation data: &#123;&#125; / &#123;&#125;"</span>.format(</span><br><span class="line">                    self.accuracy(evaluation_data), n_data)</span><br><span class="line">            <span class="keyword">print</span></span><br><span class="line">        <span class="keyword">return</span> evaluation_cost, evaluation_accuracy, \</span><br><span class="line">            training_cost, training_accuracy</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta, lmbda, n)</span>:</span></span><br><span class="line">        <span class="string">"""Update the network's weights and biases by applying gradient</span></span><br><span class="line"><span class="string">        descent using backpropagation to a single mini batch.  The</span></span><br><span class="line"><span class="string">        ``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the</span></span><br><span class="line"><span class="string">        learning rate, ``lmbda`` is the regularization parameter, and</span></span><br><span class="line"><span class="string">        ``n`` is the total size of the training data set.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">            delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br><span class="line">            nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> zip(nabla_b, delta_nabla_b)]</span><br><span class="line">            nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> zip(nabla_w, delta_nabla_w)]</span><br><span class="line">        self.weights = [(<span class="number">1</span>-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw</span><br><span class="line">                        <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nabla_w)]</span><br><span class="line">        self.biases = [b-(eta/len(mini_batch))*nb</span><br><span class="line">                       <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nabla_b)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return a tuple ``(nabla_b, nabla_w)`` representing the</span></span><br><span class="line"><span class="string">        gradient for the cost function C_x.  ``nabla_b`` and</span></span><br><span class="line"><span class="string">        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar</span></span><br><span class="line"><span class="string">        to ``self.biases`` and ``self.weights``."""</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="comment"># feedforward</span></span><br><span class="line">        activation = x</span><br><span class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            z = np.dot(w, activation)+b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = sigmoid(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line">        <span class="comment"># backward pass</span></span><br><span class="line">        delta = (self.cost).delta(zs[<span class="number">-1</span>], activations[<span class="number">-1</span>], y)</span><br><span class="line">        nabla_b[<span class="number">-1</span>] = delta</span><br><span class="line">        nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</span><br><span class="line">        <span class="comment"># Note that the variable l in the loop below is used a little</span></span><br><span class="line">        <span class="comment"># differently to the notation in Chapter 2 of the book.  Here,</span></span><br><span class="line">        <span class="comment"># l = 1 means the last layer of neurons, l = 2 is the</span></span><br><span class="line">        <span class="comment"># second-last layer, and so on.  It's a renumbering of the</span></span><br><span class="line">        <span class="comment"># scheme in the book, used here to take advantage of the fact</span></span><br><span class="line">        <span class="comment"># that Python can use negative indices in lists.</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</span><br><span class="line">            z = zs[-l]</span><br><span class="line">            sp = sigmoid_prime(z)</span><br><span class="line">            delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * sp</span><br><span class="line">            nabla_b[-l] = delta</span><br><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())</span><br><span class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(self, data, convert=False)</span>:</span></span><br><span class="line">        <span class="string">"""Return the number of inputs in ``data`` for which the neural</span></span><br><span class="line"><span class="string">        network outputs the correct result. The neural network's</span></span><br><span class="line"><span class="string">        output is assumed to be the index of whichever neuron in the</span></span><br><span class="line"><span class="string">        final layer has the highest activation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The flag ``convert`` should be set to False if the data set is</span></span><br><span class="line"><span class="string">        validation or test data (the usual case), and to True if the</span></span><br><span class="line"><span class="string">        data set is the training data. The need for this flag arises</span></span><br><span class="line"><span class="string">        due to differences in the way the results ``y`` are</span></span><br><span class="line"><span class="string">        represented in the different data sets.  In particular, it</span></span><br><span class="line"><span class="string">        flags whether we need to convert between the different</span></span><br><span class="line"><span class="string">        representations.  It may seem strange to use different</span></span><br><span class="line"><span class="string">        representations for the different data sets.  Why not use the</span></span><br><span class="line"><span class="string">        same representation for all three data sets?  It's done for</span></span><br><span class="line"><span class="string">        efficiency reasons -- the program usually evaluates the cost</span></span><br><span class="line"><span class="string">        on the training data and the accuracy on other data sets.</span></span><br><span class="line"><span class="string">        These are different types of computations, and using different</span></span><br><span class="line"><span class="string">        representations speeds things up.  More details on the</span></span><br><span class="line"><span class="string">        representations can be found in</span></span><br><span class="line"><span class="string">        mnist_loader.load_data_wrapper.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> convert:</span><br><span class="line">            results = [(np.argmax(self.feedforward(x)), np.argmax(y))</span><br><span class="line">                       <span class="keyword">for</span> (x, y) <span class="keyword">in</span> data]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            results = [(np.argmax(self.feedforward(x)), y)</span><br><span class="line">                        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> data]</span><br><span class="line">        <span class="keyword">return</span> sum(int(x == y) <span class="keyword">for</span> (x, y) <span class="keyword">in</span> results)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">total_cost</span><span class="params">(self, data, lmbda, convert=False)</span>:</span></span><br><span class="line">        <span class="string">"""Return the total cost for the data set ``data``.  The flag</span></span><br><span class="line"><span class="string">        ``convert`` should be set to False if the data set is the</span></span><br><span class="line"><span class="string">        training data (the usual case), and to True if the data set is</span></span><br><span class="line"><span class="string">        the validation or test data.  See comments on the similar (but</span></span><br><span class="line"><span class="string">        reversed) convention for the ``accuracy`` method, above.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        cost = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> data:</span><br><span class="line">            a = self.feedforward(x)</span><br><span class="line">            <span class="keyword">if</span> convert: y = vectorized_result(y)</span><br><span class="line">            cost += self.cost.fn(a, y)/len(data)</span><br><span class="line">        cost += <span class="number">0.5</span>*(lmbda/len(data))*sum(</span><br><span class="line">            np.linalg.norm(w)**<span class="number">2</span> <span class="keyword">for</span> w <span class="keyword">in</span> self.weights)</span><br><span class="line">        <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self, filename)</span>:</span></span><br><span class="line">        <span class="string">"""Save the neural network to the file ``filename``."""</span></span><br><span class="line">        data = &#123;<span class="string">"sizes"</span>: self.sizes,</span><br><span class="line">                <span class="string">"weights"</span>: [w.tolist() <span class="keyword">for</span> w <span class="keyword">in</span> self.weights],</span><br><span class="line">                <span class="string">"biases"</span>: [b.tolist() <span class="keyword">for</span> b <span class="keyword">in</span> self.biases],</span><br><span class="line">                <span class="string">"cost"</span>: str(self.cost.__name__)&#125;</span><br><span class="line">        f = open(filename, <span class="string">"w"</span>)</span><br><span class="line">        json.dump(data, f)</span><br><span class="line">        f.close()</span><br><span class="line"></span><br><span class="line"><span class="comment">#### Loading a Network</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="string">"""Load a neural network from the file ``filename``.  Returns an</span></span><br><span class="line"><span class="string">    instance of Network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    f = open(filename, <span class="string">"r"</span>)</span><br><span class="line">    data = json.load(f)</span><br><span class="line">    f.close()</span><br><span class="line">    cost = getattr(sys.modules[__name__], data[<span class="string">"cost"</span>])</span><br><span class="line">    net = Network(data[<span class="string">"sizes"</span>], cost=cost)</span><br><span class="line">    net.weights = [np.array(w) <span class="keyword">for</span> w <span class="keyword">in</span> data[<span class="string">"weights"</span>]]</span><br><span class="line">    net.biases = [np.array(b) <span class="keyword">for</span> b <span class="keyword">in</span> data[<span class="string">"biases"</span>]]</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line"><span class="comment">#### Miscellaneous functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorized_result</span><span class="params">(j)</span>:</span></span><br><span class="line">    <span class="string">"""Return a 10-dimensional unit vector with a 1.0 in the j'th position</span></span><br><span class="line"><span class="string">    and zeroes elsewhere.  This is used to convert a digit (0...9)</span></span><br><span class="line"><span class="string">    into a corresponding desired output from the neural network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    e = np.zeros((<span class="number">10</span>, <span class="number">1</span>))</span><br><span class="line">    e[j] = <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> e</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""The sigmoid function."""</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""Derivative of the sigmoid function."""</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span><br></pre></td></tr></table></figure><h2 id="神经网络识别手写数字目录"><a href="#神经网络识别手写数字目录" class="headerlink" title="神经网络识别手写数字目录"></a>神经网络识别手写数字目录</h2><ol><li><a href="http://www.lyyyuna.com/2016/05/29/handwritten-neural-net/">基于 BP 神经网络的识别手写体数字 - 神经网络基础</a></li><li><a href="http://www.lyyyuna.com/2016/06/25/handwritten-neural-net02/">基于 BP 神经网络的手写体数字识别 - 设计与实现</a></li><li><a href="http://www.lyyyuna.com/2016/06/26/handwritten-neural-net03/">基于 BP 神经网络的手写体数字识别 - 反向传播算法</a></li><li><a href="http://www.lyyyuna.com/2016/06/30/handwritten-neural-net04/">基于 BP 神经网络的手写体数字识别 - 优化</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } }
});
&lt;/script&gt;


&lt;p&gt;目前为止，我们论述中，似
      
    
    </summary>
    
      <category term="数学" scheme="http://www.lyyyuna.com/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="Python" scheme="http://www.lyyyuna.com/tags/Python/"/>
    
      <category term="mnist" scheme="http://www.lyyyuna.com/tags/mnist/"/>
    
      <category term="neural network" scheme="http://www.lyyyuna.com/tags/neural-network/"/>
    
  </entry>
  
  <entry>
    <title>基于 BP 神经网络的手写体数字识别 - 反向传播算法</title>
    <link href="http://www.lyyyuna.com/2016/06/26/handwritten-neural-net03/"/>
    <id>http://www.lyyyuna.com/2016/06/26/handwritten-neural-net03/</id>
    <published>2016-06-26T01:10:05.000Z</published>
    <updated>2017-11-29T00:19:00.164Z</updated>
    
    <content type="html"><![CDATA[<script type="text/x-mathjax-config">MathJax.Hub.Config({  TeX: { equationNumbers: { autoNumber: "AMS" } }});</script><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><a href="http://www.lyyyuna.com/2016/06/25/handwritten-neural-net02/">上一篇文章</a>使用随机梯度下降法实现神经网络自学习的过程。但仍然留有一个问题，如何快速计算代价函数的梯度？这一篇文章将介绍反向传播算法将之解决。</p><p>反向传播算法最初是由 <a href="http://en.wikipedia.org/wiki/David_Rumelhart" target="_blank" rel="noopener">David Rumelhart</a>, <a href="http://www.cs.toronto.edu/~hinton/" target="_blank" rel="noopener">Geoffrey Hinton</a>, 和 <a href="http://en.wikipedia.org/wiki/Ronald_J._Williams" target="_blank" rel="noopener">Ronald Williams</a> 在其 1986 的<a href="http://www.nature.com/nature/journal/v323/n6088/pdf/323533a0.pdf" target="_blank" rel="noopener">论文</a> 中提出的。在该文中，作者将反向传播算法运用到多种神经网络中，其自学习过程都大大加快，并推动了神经网络算法走向实用。今天，反向传播算法可以说是神经网络算法中的“老黄牛”了。</p><p>其实反向传播不仅仅是一个快速算法那么简单，其还揭示了权重和偏移是如何影响网络的行为。</p><h2 id="热身：利用矩阵快速计算神经网络的输出"><a href="#热身：利用矩阵快速计算神经网络的输出" class="headerlink" title="热身：利用矩阵快速计算神经网络的输出"></a>热身：利用矩阵快速计算神经网络的输出</h2><p>在讨论反向传播之前，首先来看一下如何快速计算神经网络的输出。虽然上一篇文章中已经介绍了 feedforward 函数，但还是有必要熟悉反向传播算法中使用的数学符号。</p><p>首先是明确的权重定义。使用 $w^l_{jk}$ 表示权重，代表连接  $(l-1)^{\rm th}$ 层第 $k^{\rm th}$ 个神经元 和 $l^{\rm th}$ 层第 $j^{\rm th}$ 个神经元的权重。举例来说，第二层第四个神经元和第三层第二个神经元之间的权重连接为</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/tikz16.png" alt="权重连接"></p><p>对于网络的偏移和激励我们将使用类似的符号。即 $b^l_j$ 是 $l^{\rm th}$ 层第 $j^{\rm th}$ 个神经元的偏移，$a^l_j$ 是 $l^{\rm th}$ 层第 $j^{\rm th}$ 个神经元的激励。如图所示：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/tikz17.png" alt="偏移和激励"></p><p>有了这些记号，第 $l^{\rm th}$ 层第 $j^{\rm th}$ 个神经元的激励 $a^l_j$ 可以由第 $(l-1)^{\rm th}$ 层的所有激励求得：</p><p>\begin{eqnarray}<br>  a^{l}_j = \sigma\left( \sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \right),<br>\label{23}<br>\end{eqnarray}</p><p>其中，求和是对第 $(l-1)^{\rm th}$ 层所有的神经元求和。为了将表达式转换成矩阵的形式，这里为每一层 $l$ 定义一个权重矩阵 $w^l$。该矩阵中第 $j^{\rm th}$ 行第 $k^{\rm th}$ 列的元素为 $w^l_{jk}$。类似的，为每一层定义了一个偏移向量 $b^l$，每一个神经元的偏移为 $b^l_j$。最后，我们定义第 $k$ 层所有的激励 $a^l_j$ 的集合为激励向量 $a^l$。</p><p>对于公式 (\ref{23})，我们还需要向量化函数 $\sigma$。因为我们不是将整个矩阵作为一个参数传递给函数，而是对矩阵的每一个元素应用相同的函数，即 $\sigma(v)$ 的元素为 $\sigma(v)_j = \sigma(v_j)$。举例来说，假如我们有一个函数 $f(x)=x^2$，那么函数 $f$ 的向量化是：</p><p>\begin{eqnarray}<br>  f\left(\left[ \begin{array}{c} 2 \ 3 \end{array} \right] \right)<br>  = \left[ \begin{array}{c} f(2) \ f(3) \end{array} \right]<br>  = \left[ \begin{array}{c} 4 \ 9 \end{array} \right],<br>\label{24}<br>\end{eqnarray}</p><p>有了这些，我们将公式 (\ref{23})重写成下列更紧凑的形式：</p><p>\begin{eqnarray}<br>  a^{l} = \sigma(w^l a^{l-1}+b^l).<br>\label{25}<br>\end{eqnarray}</p><p>这个表达式给了我们更全局的视角：本层的激励是如何与上一层的激励相关联的。计算 $a^l$ 时，会计算出中间结构 $z^l \equiv w^l a^{l-1}+b^l$。这一项我们称之为第 $l$ 层的加权输入。$z^l$ 的每一个元素为 $z^l_j= \sum_k w^l_{jk} a^{l-1}_k+b^l_j$，即 $z^l_j$ 是第 $l$ 层第 $j$ 个神经元激励函数的加权输入。公式 (\ref{25}) 有时也会写作 $a^l =\sigma(z^l)$。</p><h2 id="代价函数的两个前提假设"><a href="#代价函数的两个前提假设" class="headerlink" title="代价函数的两个前提假设"></a>代价函数的两个前提假设</h2><p>反向传播算法是为了计算代价函数 $C$ 的两个偏导数 $\partial C / \partial w$ 和 $\partial C / \partial b$。<br>首先看我们的二次代价函数</p><p>\begin{eqnarray}<br>  C = \frac{1}{2n} \sum_x |y(x)-a^L(x)|^2,<br>\label{26}<br>\end{eqnarray}</p><p>其中 $n$ 是训练数据的总数，求和是对每一个数据求和，$y=y(x)$ 是相应的期望输出，$L$ 是网络的总层数，$a^L = a^L(x)$ 是实际的网络输出。</p><p>第一个假设是代价函数可以写作时单个训练数据误差 $C_x =\frac{1}{2} |y-a^L |^2$ 的平均值 $C = \frac{1}{n} \sum_x C_x$。这个假设对于后续介绍的其他代价函数也是成立的。</p><p>之所以这么假设是因为反向传播算法只能够计算单个训练数据的偏导数 $\partial C_x / \partial w$ 和 $\partial C_x / \partial b$。最后对所有数据求平均值得 $\partial C / \partial w$ 和 $\partial C / \partial b$。</p><p>第二个假设是误差可以写作神经网络输出的函数：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/tikz18.png" alt="输出的函数"></p><p>例如，二次代价函数就满足这一要求，单个训练数据 $x$ 的二次误差可以写成：</p><p>\begin{eqnarray}<br>  C = \frac{1}{2} |y-a^L|^2 = \frac{1}{2} \sum_j (y_j-a^L_j)^2,<br>\label{27}<br>\end{eqnarray}</p><p>其中，对单个样本来说，$x$ 是定值，期望输出 $y$ 也是定值，唯一的变量就是网络输出 $a^L$，即满足第二条假设。</p><h2 id="反向传播的四个基本公式"><a href="#反向传播的四个基本公式" class="headerlink" title="反向传播的四个基本公式"></a>反向传播的四个基本公式</h2><p>反向传播算法基于最基本的线性代数操作，比如向量相加、向量与矩阵相乘等。其中有一个比较少见，假设 $s$ 和 $t$ 是两个同维的向量，我们定义 $s \odot t$ 为两个向量对应分量的乘积。那么其每个元素为 $(s \odot t)_j = s_jt_j$。例如，</p><p>\begin{eqnarray}<br>\left[\begin{array}{c} 1 \\ 2 \end{array}\right]<br>  \odot \left[\begin{array}{c} 3 \\ 4\end{array} \right]<br>= \left[ \begin{array}{c} 1 \times 3 \\ 2 \times 4 \end{array} \right]<br>= \left[ \begin{array}{c} 3 \\ 8 \end{array} \right].<br>\label{28}<br>\end{eqnarray}</p><p>这种元素对元素的乘积也称作 Hadamard 积或者是 Schur 积。Numpy 对这种运算有着良好的优化。</p><p>然后我们继续引入一个中间变量 $\delta^l_j$，我们称作第 $l$ 层第 $j$ 个神经元的误差。为了形象地理解误差到底是什么，假设在我们的神经网络中有一个小妖精：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/tikz19.png" alt="网络中的小妖精"></p><p>这个妖精坐在 $l$ 层的第 $j$ 个神经元上。当输入到来时，它故意扰乱神经元的正常工作。它在神经元的加权输入中加入了一个小的变化 $\Delta z^l_j$，这样输出变成了 $\sigma(z^l_j+\Delta z^l_j)$。这个变化向后层神经网络传递，最终到输出层导致误差相应地改变为 $\frac{\partial C}{\partial z^l_j} \Delta z^l_j$。</p><p>现在，我们钦定了这个妖精是个好妖精，在试图帮助我们减小误差。当 $\frac{\partial C}{\partial z^l_j}$ 是个很大的正数（负数）时，通过选择 $\Delta z^l_j$ 为负值（正）即可减小误差。但如果 $\frac{\partial C}{\partial z^l_j}$ 接近于零，那么这时这个好妖精无论做什么都对误差结果影响不大，这时可以认为网络接近最优值。</p><p>以上说明 $\frac{\partial C}{\partial z^l_j}$ 是一个可以用来量化神经元误差 的量。所以我们定义</p><p>\begin{eqnarray}<br>  \delta^l_j \equiv \frac{\partial C}{\partial z^l_j}.<br>\label{29}<br>\end{eqnarray}</p><p>你可能会奇怪为什么要引入一个新的中间量–加权输入 $z^l_j$，而不是直接使用激励 $a^l_j$？纯粹是因为这个中间量让最后的反向传播表达式更简洁。</p><p><strong>输出层的误差</strong>，$\delta^L$，该误差分量由下式给出</p><p>\begin{eqnarray}<br>  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma’(z^L_j).<br>\tag{BP1}\label{BP1}<br>\end{eqnarray}</p><p>这是一个非常自然的表达式，等式右边第一项为 $\partial C / \partial a^L_j$，反映了误差在第 $j$ 个输出激励影响下变化的速度。若某个特定的输出神经元对误差 $C$ 影响不大，则 $\delta^L_j$ 很小。右边第二项为 $\sigma’(z^L_j)$，反应了 $z^L_j$ 对激励函数 $\delta$ 的影响。</p><p>公式 (\ref{BP1}) 非常容易计算。加权输入在 feedforward 正向传播时即可顺便计算，而 sigmoid 激励函数和二次代价函数的偏导也易可得。鉴于 (\ref{BP1}) 是以每个元素的形式给出的，有必要改写成矩阵的形式：</p><p>\begin{eqnarray}<br>  \delta^L = \nabla_a C \odot \sigma’(z^L).<br>\tag{BP1a}\label{BP1a}<br>\end{eqnarray}</p><p>这里，$\nabla_a C$ 是偏导数 $\partial C / \partial a^L_j$ 组成的向量。你可以认为 $\nabla_a C$ 反映了误差相对于输出激励的变化。由二次代价函数 $C = \frac{1}{2} \sum_j (y_j-a_j)^2$ 的偏导数 $\partial C / \partial a^L_j = (a_j-y_j)$ 可得：$\nabla_a C =(a^L-y)$。最终公式 (\ref{BP1}) 变成</p><p>\begin{eqnarray}<br>  \delta^L = (a^L-y) \odot \sigma’(z^L).<br>\label{30}\end{eqnarray}</p><p><strong>通过下一层误差可以求本层误差</strong>，即</p><p>\begin{eqnarray}<br>  \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma’(z^l),<br>\tag{BP2}\label{BP2}<br>\end{eqnarray}</p><p>初看上去这个公式有点复杂，但其意义显而易见。$l+1$ 层的误差 $\delta^{l+1}$，通过权重矩阵的转置 $(w^{l+1})^T$ 反向转播到第 $l$ 层，然后与 $\sigma’(z^l)$ 做 Hadamard 积即可求得 $l$ 层的误差。</p><p>通过公式 (\ref{BP1}) 和公式 (\ref{BP2}) 的组合，每一层的误差都能求得。</p><p><strong>误差相对于偏移的变化率</strong>，即偏导数为</p><p>\begin{eqnarray}  \frac{\partial C}{\partial b^l_j} =<br>  \delta^l_j.<br>\tag{BP3}\label{BP3}<br>\end{eqnarray}</p><p>这意味着其偏导数就是误差的值本身，我们也可以简写为</p><p>\begin{eqnarray}<br>  \frac{\partial C}{\partial b} = \delta,<br>\label{31}<br>\end{eqnarray}</p><p><strong>误差相对于权重的变化率</strong>，即偏导数为</p><p>\begin{eqnarray}<br>  \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j.<br>\tag{BP4}\label{BP4}<br>\end{eqnarray}</p><p>这意味着我们可以通过上一层的激励和本层的误差来求取偏导数，我们也可以简写为</p><p>\begin{eqnarray}<br>\frac{\partial C}{\partial w} = a_{\rm in} \delta_{\rm out},<br>\label{32}<br>\end{eqnarray}</p><p>上一层的激励如果接近于零，$a_{\rm in} \approx 0$，那么 $\partial C / \partial w$ 也会非常小，这种情况下，权重的自学习速度减慢了。</p><h2 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h2><p>由公式 (\ref{BP1})-(\ref{BP4}) 即可得反向传播算法的步骤如下：</p><p><strong>首先，输入一个 mini-batch 量的训练数据。</strong></p><p><strong>然后，对每一个训练数据 $x$ 施加以下步骤：</strong></p><ol><li><strong>正向传播</strong>：对每一层 $l=2,3,…,L$ 计算 $z^{x,l} = w^l a^{x,l-1}+b^l$ 和 $a^{x,l} = \sigma(z^{x,l})$。</li><li><strong>计算最后一层误差</strong> $\delta^{x,L}$：计算向量 $\delta^{x,L} = \nabla_a C_x \odot \sigma’(z^{x,L})$。</li><li><strong>反向传播误差</strong>：对每一层 $l=L-1,L-2,…,2$ 计算 $\delta^{x,l} = ((w^{l+1})^T \delta^{x,l+1})\odot \sigma’(z^{x,l})$。</li></ol><p><strong>最后，为这个 mini-batch 中的训练数据计算下降的梯度：</strong>对于每一层 $l=L,L-1,…,2$ 更新权重和偏移 $w^l \rightarrow w^l-\frac{\eta}{m} \sum_x \delta^{x,l} (a^{x,l-1})^T, b^l \rightarrow b^l-\frac{\eta}{m}\sum_x \delta^{x,l}$。</p><p>写成代码即为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">     <span class="string">"""Return a tuple "(nabla_b, nabla_w)" representing the</span></span><br><span class="line"><span class="string">     gradient for the cost function C_x.  "nabla_b" and</span></span><br><span class="line"><span class="string">     "nabla_w" are layer-by-layer lists of numpy arrays, similar</span></span><br><span class="line"><span class="string">     to "self.biases" and "self.weights"."""</span></span><br><span class="line">     nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">     nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">     <span class="comment"># feedforward</span></span><br><span class="line">     activation = x</span><br><span class="line">     activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">     zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line">     <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">         z = np.dot(w, activation)+b</span><br><span class="line">         zs.append(z)</span><br><span class="line">         activation = sigmoid(z)</span><br><span class="line">         activations.append(activation)</span><br><span class="line">     <span class="comment"># backward pass</span></span><br><span class="line">     delta = self.cost_derivative(activations[<span class="number">-1</span>], y) * \</span><br><span class="line">         sigmoid_prime(zs[<span class="number">-1</span>])</span><br><span class="line">     nabla_b[<span class="number">-1</span>] = delta</span><br><span class="line">     nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</span><br><span class="line">     <span class="comment"># Note that the variable l in the loop below is used a little</span></span><br><span class="line">     <span class="comment"># differently to the notation in Chapter 2 of the book.  Here,</span></span><br><span class="line">     <span class="comment"># l = 1 means the last layer of neurons, l = 2 is the</span></span><br><span class="line">     <span class="comment"># second-last layer, and so on.  It's a renumbering of the</span></span><br><span class="line">     <span class="comment"># scheme in the book, used here to take advantage of the fact</span></span><br><span class="line">     <span class="comment"># that Python can use negative indices in lists.</span></span><br><span class="line">     <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</span><br><span class="line">         z = zs[-l]</span><br><span class="line">         sp = sigmoid_prime(z)</span><br><span class="line">         delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * sp</span><br><span class="line">         nabla_b[-l] = delta</span><br><span class="line">         nabla_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())</span><br><span class="line">     <span class="keyword">return</span> (nabla_b, nabla_w)</span><br></pre></td></tr></table></figure><h2 id="神经网络识别手写数字目录"><a href="#神经网络识别手写数字目录" class="headerlink" title="神经网络识别手写数字目录"></a>神经网络识别手写数字目录</h2><ol><li><a href="http://www.lyyyuna.com/2016/05/29/handwritten-neural-net/">基于 BP 神经网络的识别手写体数字 - 神经网络基础</a></li><li><a href="http://www.lyyyuna.com/2016/06/25/handwritten-neural-net02/">基于 BP 神经网络的手写体数字识别 - 设计与实现</a></li><li><a href="http://www.lyyyuna.com/2016/06/26/handwritten-neural-net03/">基于 BP 神经网络的手写体数字识别 - 反向传播算法</a></li><li><a href="http://www.lyyyuna.com/2016/06/30/handwritten-neural-net04/">基于 BP 神经网络的手写体数字识别 - 优化</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } }
});
&lt;/script&gt;


&lt;h2 id=&quot;前言&quot;&gt;&lt;a 
      
    
    </summary>
    
      <category term="数学" scheme="http://www.lyyyuna.com/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="Python" scheme="http://www.lyyyuna.com/tags/Python/"/>
    
      <category term="mnist" scheme="http://www.lyyyuna.com/tags/mnist/"/>
    
      <category term="neural network" scheme="http://www.lyyyuna.com/tags/neural-network/"/>
    
  </entry>
  
  <entry>
    <title>基于 BP 神经网络的手写体数字识别 - 设计与实现</title>
    <link href="http://www.lyyyuna.com/2016/06/25/handwritten-neural-net02/"/>
    <id>http://www.lyyyuna.com/2016/06/25/handwritten-neural-net02/</id>
    <published>2016-06-25T09:56:33.000Z</published>
    <updated>2017-11-29T00:19:00.163Z</updated>
    
    <content type="html"><![CDATA[<script type="text/x-mathjax-config">MathJax.Hub.Config({  TeX: { equationNumbers: { autoNumber: "AMS" } }});</script><h2 id="手写体数字识别的神经网络结构"><a href="#手写体数字识别的神经网络结构" class="headerlink" title="手写体数字识别的神经网络结构"></a>手写体数字识别的神经网络结构</h2><p><a href="http://www.lyyyuna.com/2016/05/29/handwritten-neural-net/">上一篇文章</a>中我们简单介绍了神经网络，接下来让我们运用到主题中 —— 手写体数字识别。</p><p>手写数字识别可以分为两大子问题。第一是如何将一串数字分割成单个的数字，第二是识别单个数字。我们将专注于解决第二个问题，因为分割问题并不难，而且和我们的主题——神经网络相差甚远。</p><p>为了识别单个的数字，我们将使用三层神经网络：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/three-layers.png" alt="三层神经网络"></p><p>输入层的神经元将对输入的像素值编码。神经网络的训练数据包含了 $28 \times 28$ 像素的扫描图，所以输入层有 $784 = 28 \times 28$ 个神经元。为了简便起见，上图中没有 $784$ 个输入神经元全部画出。每一个像素是灰度值，$0.0$ 代表白色，$1.0$ 代表黑色，两者之间的为逐渐变黑的灰色。</p><p>第二层是隐藏层。让我们记隐藏层的神经元数量为 $n$，我们将对 $n$ 的不同值实验。上图中，$n=15$。</p><p>输出层包含 $10$ 个神经元。如果第一个输出被激活了，即 $\mbox{output} \approx 1$，意味着神经网络判定当前数字为 $0$。如果第二个输出神经元被激活，则神经网络认为当前数字很有可能为 $1$。以此类推。最后对 $10$ 个输出神经元进行排序，哪个最高说明神经网络认为其最有可能。比如第 $6$ 个神经元输出最高，则输入图像最有可能是数字 $6$。</p><p>你可能会想为什么会用十个输出神经元？毕竟我们的目标是判断出每一个图片对应的数字，理论上，四个输出的组合就可以编码，$2^4 = 16$ 完全可以包含 $10$ 个值。要回答这个问题还是得靠实验：我们对两种输出编码方案都做了实验，用 $10$ 个输出的效果更好。让我们做一个启发式的思考，用四个 bit 来唯一确定一个数字，意味着得百分百识别出图像，说一不二。而有些时候字确实写的难以辨认，连人类自己都只能说‘大概是 4 或者 9’这种话，这时候，给出 $10$ 个数字的概率更符合人脑的思维方式。</p><h2 id="基于梯度下降法学习"><a href="#基于梯度下降法学习" class="headerlink" title="基于梯度下降法学习"></a>基于梯度下降法学习</h2><p>现在我们已经设计出神经网络的结构了，那如何识别出数字呢？第一件事是找到训练集。我们将使用 <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">MNIST 数据集</a>。MNIST 是经过修改的 <a href="http://en.wikipedia.org/wiki/National_Institute_of_Standards_and_Technology" target="_blank" rel="noopener">NIST</a> 数据子集，NIST 即 United States’ National Institute of Standards and Technology。以下是部分数据：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/some-samples.png" alt="部分数据"></p><p>图像已经被分割成单个的数字，而且没有噪点等，不需要做图像预处理。MNIST 数据集包括两部分，第一部分包含 60000 个图像，被用作训练数据。这些图像来自 250 个人，半数来自美国人口普查局的员工，另外半数是高中生。图片是 $28 \times 28$ 的灰度图。第二部分是 10000 张图片的测试集。我们将使用测试集来评估人工神经网络的学习效果。为了取得更好的测试效果，测试集来自另外 250 个人。这样，测试集和训练集的完全不同能够更好验证结果。</p><p>我们将使用 $x$ 来标记训练输入。虽然图片是一个二维数组，不过我们输入会采用 $28 \times 28 = 784$ 的向量。向量中的每一项为图片每一个像素的灰度值。输出我们将记为输出向量 $y = y(x)$。如果一个训练图片是数字 6，则 $y(x) = (0, 0, 0, 0, 0, 0, 1, 0, 0, 0)^\mathrm{T}$。请注意，$\mathrm{T}$ 是转置操作。</p><p>算法的目标是找到权重和偏移，对于所有训练输入 $x$，网络都能够输出正确的 $y(x)$。为了量化我们与目标的接近程度，定义以下的代价函数：</p><p>\begin{eqnarray}  C(w,b) \equiv<br>  \frac{1}{2n} \sum_x | y(x) - a|^2.  \label{6}<br>\end{eqnarray}</p><p>这里，记 $w$ 为网络所有的权重集，$b$ 为所有的偏移，$n$ 为训练输入的总数，$a$ 为实际的输出向量，求和是对所有训练输入而言。当然，$a$ 应该是 $x,w,b$ 的函数。记号 $| v |$ 为向量 $v$ 的长度。我们称 $C$ 为二次代价函数，其实就是均方差啦， MSE (mean squared error)。可以看出，该代价函数为非负值，并且，当所有训练数据的 $y(x)$ 接近实际输出 $a$ 时，代价函数 $C(w,b) \approx 0$。所以，当神经网络工作良好的时候，$C(w,b)$ 很小，相反，该值会很大，即训练集中有很大一部分预期输出 $y(x)$ 和实际输出 $a$ 不符。我们训练算法的目标，就是找到一组权重和偏移，让误差尽可能的小。</p><p>为什么会用二次函数误差？毕竟我们感兴趣的是能被正确分类的图片数量。为什么不直接以图片数量为目标，比如目标是识别的数量最大化？问题是，正确分类图像的数量不是权值和偏置的光滑函数，大部分情况下，变量的变化不会导致识别数量的变化，也就难以调整权重和偏移。</p><p>你可能还会好奇，二次函数是最好的选择吗？其他的代价函数会不会得到完全不同的权值和偏移，效果更好呢？事实上，确实有更好的代价函数，以后的文章还会继续探讨。不过，本文继续使用二次函数，它对我们理解神经网络的基本自学习过程非常有益。</p><p>接下来，我们介绍梯度下降法，它可以用来解决最小化问题。</p><p>假设我们要最小化函数 $C(v), v=v_1,v_2$。该函数图像为：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/func_plot.png" alt="函数图像"></p><p>我们要找到 $C$ 的全局最小值。一种做法是计算函数的导数，找到各个极值。课本上的导数很好求解。不幸的是，现实生活中，问题所代表的函数经常包含过多的变量。尤其是神经网络中，有数以万计的权值和偏移，不可能直接求取极值。</p><p>幸好还有其他方法。让我们换种思维方式，对比那个函数图像，将函数看作一个山谷，并假设有一个小球沿着山谷斜坡滑动。直觉告诉我们小球最终会滑向坡底。也许这就能用来找到最小值？我们为“小球”随机选择一个起点，然后模拟“小球”沿斜坡滑动。“小球”运动的方向可以通过偏导数确定，这些偏导数包含了山谷形状的信息。</p><p>是否需要牛顿力学公式来获取小球的运动，考虑摩擦力和重力呢？并不需要，我们是在制定一个最小化 $C$ 的算法，而不是去精确模拟物理学规律。</p><p>让我们记球在 $v_1$ 方向移动了很小的量 $\Delta v_1$，在 $v_2$ 的方向移动了 $\Delta v_2$，总的 $C$ 的改变量为：</p><p>\begin{eqnarray}<br>  \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 +<br>  \frac{\partial C}{\partial v_2} \Delta v_2.<br>\label{7}<br>\end{eqnarray}</p><p>我们目标是找到一组 $\Delta v_1$ 和 $\Delta v_2$，使 $\Delta C$ 为负，即使球向谷底滚去。接下来记 $\Delta v \equiv (\Delta v_1, \Delta v_2)^T$ 为 $v$ 的总变化，并且记梯度向量 $\nabla C$ 为：</p><p>\begin{eqnarray}<br>  \nabla C \equiv \left( \frac{\partial C}{\partial v_1},<br>  \frac{\partial C}{\partial v_2} \right)^T.<br>\label{8}<br>\end{eqnarray}</p><p>其中梯度向量符号 $\nabla C$ 会比较抽象，因为它纯粹就是一个数学上的概念。有了梯度符号，我们可以将式 (\ref{8}) 改写为</p><p>\begin{eqnarray}<br>  \Delta C \approx \nabla C \cdot \Delta v.<br>\label{9}<br>\end{eqnarray}</p><p>从该式可以看出，梯度向量 $\nabla C$ 将 $v$ 的变化反应到 $C$ 中，且我们也找到了如何让 $\Delta C$ 为负的方法。尤其是当我们选择</p><p>\begin{eqnarray}<br>  \Delta v = -\eta \nabla C,<br>\label{10}<br>\end{eqnarray}</p><p>当 $\eta$ 是一个很小的正参数时（其实该参数就是学习率），公式 (\ref{9}) 表明 $\Delta C \approx -\eta\nabla C \cdot \nabla C = -\eta |\nabla C|^2$。因为 $| \nabla C|^2 \geq 0$，能保证 $\Delta C \leq 0$。这正是我们需要的特性！在梯度下降学习法中，我们使用公式 (\ref{10}) 计算 $\Delta v$，然后移动球到新的位置：</p><p>\begin{eqnarray}<br>  v \rightarrow v’ = v -\eta \nabla C.<br>\label{11}<br>\end{eqnarray}</p><p>然后不停使用这一公式计算下一步，直到 $C$ 不再减小为止，即找到了全局最小值。</p><p>总结一下，首先重复计算梯度 $\nabla C$，然后向相反方向移动，画成图就是</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/gradient.png" alt="梯度学习法"></p><p>请注意，上述梯度下降的规则并没有复制出真正的物理运动。在真实生活中，球有动量，滚向谷底后还会继续滚上去，在摩擦力的作用下才会最终停下。但我们的算法中没这么复杂。</p><p>为了使算法正常工作，公式 (\ref{9}) 中的学习率 $\eta$ 要尽可能的小，不然最终可能 $\Delta C &gt; 0$。同时学习率不能过小，不然会导致每一次迭代中 $\Delta v$ 过小，算法工作会非常慢。</p><p>尽管到现在我们一直用两个变量的函数 $C$ 举例。但其实对于多变量的函数，这仍是适用的。假设 $C$ 有 $m$ 个变量，$v_1,…, v_m$，那么变化 $\Delta C$ 为</p><p>\begin{eqnarray}<br>  \Delta C \approx \nabla C \cdot \Delta v,<br>\label{12}<br>\end{eqnarray}</p><p>其中 $\Delta v = (\Delta v_1,\ldots, \Delta v_m)^T$，梯度向量 $\nabla C$ 为</p><p>\begin{eqnarray}<br>  \nabla C \equiv \left(\frac{\partial C}{\partial v_1}, \ldots,<br>  \frac{\partial C}{\partial v_m}\right)^T.<br>\label{13}<br>\end{eqnarray}</p><p>梯度下降法虽然简单，但实际上很有效，也是一个经典的最优化方法，在神经网络中我们将用它来寻找代价函数的最小值。</p><p>同时，人们研究了大量梯度下降法的变种，包括去模拟真实的物理学，但都效果不好。因为这些变种算法不光计算一次偏导数，还需要计算二次偏导，这对计算机来说是巨大的挑战，尤其是有着上百万神经元的神经网络。</p><p>我们将使用梯度下降法来来找到权重 $w_k$ 和偏移 $b_l$。类比上述的梯度法，这里变量 $v_j$ 即为权重和偏移，而梯度 $\nabla C$ 为 $\partial C / \partial w_k$ 和 $\partial C/ \partial b_l$。梯度下降的更新规则如下：</p><p>\begin{eqnarray}<br>  w_k &amp; \rightarrow &amp; w_k’ = w_k-\eta \frac{\partial C}{\partial w_k} \label{16}\\<br>  b_l &amp; \rightarrow &amp; b_l’ = b_l-\eta \frac{\partial C}{\partial b_l}.<br>\label{17}<br>\end{eqnarray}</p><p>重复上述规则，找到代价函数的最小值，实现神经网络的自学习。</p><p>在适用梯度下降法时仍有一些挑战，比如公式 (\ref{6}) 中代价函数是每一个训练样本的误差 $C_x \equiv \frac{|y(x)-a|^2}{2}$ 的平均值。这意味着计算梯度时，需要每一个训练样本计算梯度。当样本数量很多时，就会造成性能的问题。</p><p>随机梯度下降法能够解决这一问题，加速学习过程。算法每次随机从训练输入中选取 $m$ 个数据，$X_1, X_2,\ldots, X_m $，记为 mini-batch。当 $m$ 足够大时，$\nabla C_{X_j}$ 将十分接近所有样本的平均值 $\nabla C_x$，即</p><p>\begin{eqnarray}<br>  \frac{\sum_{j=1}^m \nabla C_{X_{j}}}{m} \approx \frac{\sum_x \nabla C_x}{n} = \nabla C,<br>\label{18}<br>\end{eqnarray}</p><p>交换等式两边得</p><p>\begin{eqnarray}<br>  \nabla C \approx \frac{1}{m} \sum_{j=1}^m \nabla C_{X_{j}},<br>\label{19}<br>\end{eqnarray}</p><p>这样，我们把总体的梯度转换成计算随机选取的 mini-batch 的梯度。将随机梯度下降法运用到神经网络中，则权重和偏移为</p><p>\begin{eqnarray}<br>  w_k  \rightarrow &amp; w_k’ = w_k-\frac{\eta}{m}  \sum_j \frac{\partial C_{X_j}}{\partial w_k}<br>\label{20}<br>\end{eqnarray}</p><p>\begin{eqnarray}<br>  b_l  \rightarrow &amp; b_l’ = b_l-\frac{\eta}{m}  \sum_j \frac{\partial C_{X_j}}{\partial b_l},<br>\label{21}<br>\end{eqnarray}</p><p>其中，求和是对当前 mini-batch 中训练样本 $X_j$ 求和。然后选取下一组 mini-batch 重复上述过程，直到所有的训练输入全部选取完成，训练的一个 epoch 完成。<br>接着我们可以开始一个新的 epoch。</p><p>对于 MNIST 数据集来说，一共有 $n=60000$ 个数据，如果选择 mini-batch 的大小 $m=10$，则计算梯度的速度可以比原先快 $6000$ 倍。当然，加速计算的结果只是近似值，尽管会有一些统计学上的波动，但精确的梯度计算并不重要，只要小球下降的大方向不错就行。为什么我敢这么说，因为实践证明了啊，随机梯度也是大多数自学习算法的基石。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>我们将官方的 MNIST 数据分成三部分，50000 个训练集，10000 个验证集，10000 个测试集。验证集用于在训练过程中实时观察神经网络正确率的变化，测试集用于测试最终神经网络的正确率。</p><p>下面介绍一下代码的核心部分。首先是 Network 类的初始化部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sizes)</span>:</span></span><br><span class="line">        self.num_layers = len(sizes)</span><br><span class="line">        self.sizes = sizes</span><br><span class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]</span><br><span class="line">        self.weights = [np.random.randn(y, x) </span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(sizes[:<span class="number">-1</span>], sizes[<span class="number">1</span>:])]</span><br></pre></td></tr></table></figure><p>列表 sizes 表示神经网络每一层所包含的神经元个数。比如我们想创建一个 2 个输入神经元，3 个神经元在中间层，一个输出神经元的网络，那么可以</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = Network([<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>偏移和权重都是使用 np.random.randn 函数随机初始化的，平均值为 0，标准差为 1。这种随机初始化并不是最佳方案，后续文章会逐步优化。</p><p>请注意，偏移和权重被初始化为 Numpy 矩阵。net.weights[1] 代表连接第二和第三层神经元的权重。</p><p>下面是 sigmoid 函数的定义：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br></pre></td></tr></table></figure><p>注意到虽然输入 $z$ 是向量，但 Numpy 能够自动处理，为向量中的每一个元素作相同的 sigmoid 运算。</p><p>然后是计算 sigmoid 函数的导数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""Derivative of the sigmoid function."""</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span><br></pre></td></tr></table></figure><p>每一层相对于前一层的输出为</p><p>\begin{eqnarray}<br>  a’ = \sigma(w a + b).<br>\label{22}<br>\end{eqnarray}</p><p>对应的是 feedforward 函数当：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(self, a)</span>:</span></span><br><span class="line">    <span class="string">"""Return the output of the network if "a" is input."""</span></span><br><span class="line">    <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">        a = sigmoid(np.dot(w, a)+b)</span><br><span class="line">    <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure><p>当然，Network 对象最重要的任务还是自学习。下面的 SGD 函数实现了随机梯度下降法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, training_data, epochs, mini_batch_size, eta,</span></span></span><br><span class="line"><span class="function"><span class="params">        test_data=None)</span>:</span></span><br><span class="line">    <span class="string">"""Train the neural network using mini-batch stochastic</span></span><br><span class="line"><span class="string">    gradient descent.  The "training_data" is a list of tuples</span></span><br><span class="line"><span class="string">    "(x, y)" representing the training inputs and the desired</span></span><br><span class="line"><span class="string">    outputs.  The other non-optional parameters are</span></span><br><span class="line"><span class="string">    self-explanatory.  If "test_data" is provided then the</span></span><br><span class="line"><span class="string">    network will be evaluated against the test data after each</span></span><br><span class="line"><span class="string">    epoch, and partial progress printed out.  This is useful for</span></span><br><span class="line"><span class="string">    tracking progress, but slows things down substantially."""</span></span><br><span class="line">    <span class="keyword">if</span> test_data: n_test = len(test_data)</span><br><span class="line">    n = len(training_data)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> xrange(epochs):</span><br><span class="line">        random.shuffle(training_data)</span><br><span class="line">        mini_batches = [</span><br><span class="line">            training_data[k:k+mini_batch_size]</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">0</span>, n, mini_batch_size)]</span><br><span class="line">        <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">            self.update_mini_batch(mini_batch, eta)</span><br><span class="line">        <span class="keyword">if</span> test_data:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;"</span>.format(</span><br><span class="line">                j, self.evaluate(test_data), n_test)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125; complete"</span>.format(j)</span><br></pre></td></tr></table></figure><p>列表 training_data 是由 (x,y) 元组构成，代表训练集的输入和期望输出。而 test_data 则是验证集（非测试集，这里变量名有些歧义），在每一个 epoch 结束时对神经网络正确率做一下检测。其他变量的含义比较显见，不再赘述。</p><p>SGD 函数在每一个 epoch 开始时随机打乱训练集，然后按照 mini-batch 的大小对数据分割。在每一步中对一个 mini_batch 计算梯度，在 self.update_mini_batch(mini_batch, eta) 更新权重和偏移：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta)</span>:</span></span><br><span class="line">    <span class="string">"""Update the network's weights and biases by applying</span></span><br><span class="line"><span class="string">    gradient descent using backpropagation to a single mini batch.</span></span><br><span class="line"><span class="string">    The "mini_batch" is a list of tuples "(x, y)", and "eta"</span></span><br><span class="line"><span class="string">    is the learning rate."""</span></span><br><span class="line">    nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">    nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">        delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br><span class="line">        nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> zip(nabla_b, delta_nabla_b)]</span><br><span class="line">        nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> zip(nabla_w, delta_nabla_w)]</span><br><span class="line">    self.weights = [w-(eta/len(mini_batch))*nw </span><br><span class="line">                    <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nabla_w)]</span><br><span class="line">    self.biases = [b-(eta/len(mini_batch))*nb </span><br><span class="line">                   <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nabla_b)]</span><br></pre></td></tr></table></figure><p>其中最关键的是</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br></pre></td></tr></table></figure><p>这就是反向转播算法，这是一个快速计算代价函数梯度的方法。所以 update_mini_batch 仅仅是计算这些梯度，然后用来更新 self.weights 和 self.biases。这里暂时不介绍它，它牵涉较多内容，下一篇文章会重点阐述。</p><p>让我们看一下 <a href="https://github.com/mnielsen/neural-networks-and-deep-learning/tree/master/src" target="_blank" rel="noopener">完整的程序</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#### Libraries</span></span><br><span class="line"><span class="comment"># Standard library</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># Third-party libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sizes)</span>:</span></span><br><span class="line">        <span class="string">"""The list ``sizes`` contains the number of neurons in the</span></span><br><span class="line"><span class="string">        respective layers of the network.  For example, if the list</span></span><br><span class="line"><span class="string">        was [2, 3, 1] then it would be a three-layer network, with the</span></span><br><span class="line"><span class="string">        first layer containing 2 neurons, the second layer 3 neurons,</span></span><br><span class="line"><span class="string">        and the third layer 1 neuron.  The biases and weights for the</span></span><br><span class="line"><span class="string">        network are initialized randomly, using a Gaussian</span></span><br><span class="line"><span class="string">        distribution with mean 0, and variance 1.  Note that the first</span></span><br><span class="line"><span class="string">        layer is assumed to be an input layer, and by convention we</span></span><br><span class="line"><span class="string">        won't set any biases for those neurons, since biases are only</span></span><br><span class="line"><span class="string">        ever used in computing the outputs from later layers."""</span></span><br><span class="line">        self.num_layers = len(sizes)</span><br><span class="line">        self.sizes = sizes</span><br><span class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]</span><br><span class="line">        self.weights = [np.random.randn(y, x)</span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(sizes[:<span class="number">-1</span>], sizes[<span class="number">1</span>:])]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(self, a)</span>:</span></span><br><span class="line">        <span class="string">"""Return the output of the network if ``a`` is input."""</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            a = sigmoid(np.dot(w, a)+b)</span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, training_data, epochs, mini_batch_size, eta,</span></span></span><br><span class="line"><span class="function"><span class="params">            test_data=None)</span>:</span></span><br><span class="line">        <span class="string">"""Train the neural network using mini-batch stochastic</span></span><br><span class="line"><span class="string">        gradient descent.  The ``training_data`` is a list of tuples</span></span><br><span class="line"><span class="string">        ``(x, y)`` representing the training inputs and the desired</span></span><br><span class="line"><span class="string">        outputs.  The other non-optional parameters are</span></span><br><span class="line"><span class="string">        self-explanatory.  If ``test_data`` is provided then the</span></span><br><span class="line"><span class="string">        network will be evaluated against the test data after each</span></span><br><span class="line"><span class="string">        epoch, and partial progress printed out.  This is useful for</span></span><br><span class="line"><span class="string">        tracking progress, but slows things down substantially."""</span></span><br><span class="line">        <span class="keyword">if</span> test_data: n_test = len(test_data)</span><br><span class="line">        n = len(training_data)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(epochs):</span><br><span class="line">            random.shuffle(training_data)</span><br><span class="line">            mini_batches = [</span><br><span class="line">                training_data[k:k+mini_batch_size]</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">0</span>, n, mini_batch_size)]</span><br><span class="line">            <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">                self.update_mini_batch(mini_batch, eta)</span><br><span class="line">            <span class="keyword">if</span> test_data:</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;"</span>.format(</span><br><span class="line">                    j, self.evaluate(test_data), n_test)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125; complete"</span>.format(j)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta)</span>:</span></span><br><span class="line">        <span class="string">"""Update the network's weights and biases by applying</span></span><br><span class="line"><span class="string">        gradient descent using backpropagation to a single mini batch.</span></span><br><span class="line"><span class="string">        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``</span></span><br><span class="line"><span class="string">        is the learning rate."""</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">            delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br><span class="line">            nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> zip(nabla_b, delta_nabla_b)]</span><br><span class="line">            nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> zip(nabla_w, delta_nabla_w)]</span><br><span class="line">        self.weights = [w-(eta/len(mini_batch))*nw</span><br><span class="line">                        <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nabla_w)]</span><br><span class="line">        self.biases = [b-(eta/len(mini_batch))*nb</span><br><span class="line">                       <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nabla_b)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return a tuple ``(nabla_b, nabla_w)`` representing the</span></span><br><span class="line"><span class="string">        gradient for the cost function C_x.  ``nabla_b`` and</span></span><br><span class="line"><span class="string">        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar</span></span><br><span class="line"><span class="string">        to ``self.biases`` and ``self.weights``."""</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="comment"># feedforward</span></span><br><span class="line">        activation = x</span><br><span class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            z = np.dot(w, activation)+b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = sigmoid(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line">        <span class="comment"># backward pass</span></span><br><span class="line">        delta = self.cost_derivative(activations[<span class="number">-1</span>], y) * \</span><br><span class="line">            sigmoid_prime(zs[<span class="number">-1</span>])</span><br><span class="line">        nabla_b[<span class="number">-1</span>] = delta</span><br><span class="line">        nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</span><br><span class="line">        <span class="comment"># Note that the variable l in the loop below is used a little</span></span><br><span class="line">        <span class="comment"># differently to the notation in Chapter 2 of the book.  Here,</span></span><br><span class="line">        <span class="comment"># l = 1 means the last layer of neurons, l = 2 is the</span></span><br><span class="line">        <span class="comment"># second-last layer, and so on.  It's a renumbering of the</span></span><br><span class="line">        <span class="comment"># scheme in the book, used here to take advantage of the fact</span></span><br><span class="line">        <span class="comment"># that Python can use negative indices in lists.</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</span><br><span class="line">            z = zs[-l]</span><br><span class="line">            sp = sigmoid_prime(z)</span><br><span class="line">            delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * sp</span><br><span class="line">            nabla_b[-l] = delta</span><br><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())</span><br><span class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(self, test_data)</span>:</span></span><br><span class="line">        <span class="string">"""Return the number of test inputs for which the neural</span></span><br><span class="line"><span class="string">        network outputs the correct result. Note that the neural</span></span><br><span class="line"><span class="string">        network's output is assumed to be the index of whichever</span></span><br><span class="line"><span class="string">        neuron in the final layer has the highest activation."""</span></span><br><span class="line">        test_results = [(np.argmax(self.feedforward(x)), y)</span><br><span class="line">                        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_data]</span><br><span class="line">        <span class="keyword">return</span> sum(int(x == y) <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_results)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span><span class="params">(self, output_activations, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return the vector of partial derivatives \partial C_x /</span></span><br><span class="line"><span class="string">        \partial a for the output activations."""</span></span><br><span class="line">        <span class="keyword">return</span> (output_activations-y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#### Miscellaneous functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""The sigmoid function."""</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""Derivative of the sigmoid function."""</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span><br></pre></td></tr></table></figure><p>让我们来看一下这个最简单的 BP 神经网络在手写数字识别上效果如何。初始化一个含有 30 个神经元的隐藏层。训练 30 个 epoch，mini-batch 大小为 10，学习率 $\eta=3.0$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mnist_loader</span><br><span class="line">training_data, validation_data, test_data = mnist_loader.load_data_wrapper()</span><br><span class="line"><span class="keyword">import</span> network</span><br><span class="line">net = network.Network([<span class="number">784</span>, <span class="number">30</span>, <span class="number">10</span>])</span><br><span class="line"><span class="keyword">import</span> network</span><br><span class="line">net = network.Network([<span class="number">784</span>, <span class="number">30</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>结果为</p><pre><code>Epoch 0: 9129 / 10000Epoch 1: 9295 / 10000Epoch 2: 9348 / 10000...Epoch 27: 9528 / 10000Epoch 28: 9542 / 10000Epoch 29: 9534 / 10000</code></pre><p>看来不错！识别率达到了 95.42%！如果将隐藏层的神经元增加到 100 个，最终识别率达到了 96.59%！</p><h2 id="神经网络识别手写数字目录"><a href="#神经网络识别手写数字目录" class="headerlink" title="神经网络识别手写数字目录"></a>神经网络识别手写数字目录</h2><ol><li><a href="http://www.lyyyuna.com/2016/05/29/handwritten-neural-net/">基于 BP 神经网络的识别手写体数字 - 神经网络基础</a></li><li><a href="http://www.lyyyuna.com/2016/06/25/handwritten-neural-net02/">基于 BP 神经网络的手写体数字识别 - 设计与实现</a></li><li><a href="http://www.lyyyuna.com/2016/06/26/handwritten-neural-net03/">基于 BP 神经网络的手写体数字识别 - 反向传播算法</a></li><li><a href="http://www.lyyyuna.com/2016/06/30/handwritten-neural-net04/">基于 BP 神经网络的手写体数字识别 - 优化</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } }
});
&lt;/script&gt;


&lt;h2 id=&quot;手写体数字识别
      
    
    </summary>
    
      <category term="数学" scheme="http://www.lyyyuna.com/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="Python" scheme="http://www.lyyyuna.com/tags/Python/"/>
    
      <category term="mnist" scheme="http://www.lyyyuna.com/tags/mnist/"/>
    
      <category term="neural network" scheme="http://www.lyyyuna.com/tags/neural-network/"/>
    
  </entry>
  
  <entry>
    <title>基于 BP 神经网络的识别手写体数字 - 神经网络基础</title>
    <link href="http://www.lyyyuna.com/2016/05/29/handwritten-neural-net/"/>
    <id>http://www.lyyyuna.com/2016/05/29/handwritten-neural-net/</id>
    <published>2016-05-29T00:57:52.000Z</published>
    <updated>2017-11-29T00:19:00.163Z</updated>
    
    <content type="html"><![CDATA[<script type="text/x-mathjax-config">MathJax.Hub.Config({  TeX: { equationNumbers: { autoNumber: "AMS" } }});</script><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>上个月，公司内部举办了机器学习比赛，内容是识别手写体数字。</p><p>我提交的方案参考 <a href="http://michaelnielsen.org/" target="_blank" rel="noopener">Michael Nielsen</a>。以下大部分内容也参考了<a href="http://neuralnetworksanddeeplearning.com/chap1.html" target="_blank" rel="noopener">他写的深度学习在线电子书</a>。</p><p>人类视觉系统其实非常神奇，恐怕自己都没意识到，考虑以下的手写数字：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201605/504192.png" alt="504192"></p><p>大部分人能够识别出数字为 504192。人脑每一半球都有着近 1.4 亿个神经元，之间有数以百亿的连接，能够进行复杂的图像处理。相当于每个人随身携带了一台超级计算机，数百万年的进化让该系统训练有素，能够适应并理解视觉世界。</p><p>当真正开始编写程序时，就会意识到手写数字中复杂之处。在教小孩认数字 9 时，可能会 “这个数字顶上有个圈，右下角有个垂直的竖线。。”，或者你给他看一眼写的 9，他就能学会了。但这些步骤根本无法用传统的算法来描述，因为一个手写数字有着无限的细节。</p><p>神经网络算法则用另一种方法来解决问题。首先，会准备如下的训练数据，</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201605/train_data.png" alt="训练数据"></p><p>然后，系统便以此为基础<strong>学习</strong>。换句话说，神经网络能够使用这些训练数据自动推导出识别手写数字的规则。并且，训练集越多，神经网络准确率越高。所以虽然上图只展示了 100 个数字，但如果有上百万个训练集的话，我们的手写数字识别器效果会更好。</p><p>神经网络是一个简单易实现的算法，不会超过 100 行代码。我们也会在将来探讨更为复杂的深度学习算法。</p><h2 id="感知器-Perceptron"><a href="#感知器-Perceptron" class="headerlink" title="感知器 Perceptron"></a>感知器 Perceptron</h2><p>什么是神经网络？我们从<strong>感知器</strong>说起。</p><p>感知器是上世纪 50 年代，<a href="http://en.wikipedia.org/wiki/Frank_Rosenblatt" target="_blank" rel="noopener">Frank Rosenblatt</a> 受 <a href="http://en.wikipedia.org/wiki/Warren_McCulloch" target="_blank" rel="noopener">Warren McCulloch</a> 和 <a href="http://en.wikipedia.org/wiki/Walter_Pitts" target="_blank" rel="noopener">Walter Pitts</a> <a href="http://scholar.google.ca/scholar?cluster=4035975255085082870" target="_blank" rel="noopener">工作</a>的启发，所提出的概念。如今，其他的人工神经元模型更常用，最广泛的是 <strong>sigmoid</strong> 神经元。现在先让我们看看感知器模型，它将帮助我们了解为什么 sigmoid 神经元更受欢迎。</p><p>感知器如何工作呢？一个感知器有多个二进制输入，$x_1, x_2, …$，并只有一个二进制的输出：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201605/perceptron.png" alt="感知器模型"></p><p>这个例子中，感知器有三个输入，$x_1, x_2, x_3$。通常输入数目由需要而定。Rosenblatt 给每一个输入引入一个权重，$w_1, w_2, …$，在输出增加一个阈值，超过阈值时才会输出 1，以下为输出与输入的关系：</p><p>\begin{equation}<br>output = \left\{\begin{aligned}<br>0, \sum_jw_jx_j \leq threshold \\<br>1, \sum_jw_jx_j &gt; threshold<br>\end{aligned}\right.<br>\end{equation}</p><p>这个简单的公式就是感知器的工作原理！</p><p>下面给出一个简单的模型，虽然不是实际例子，但易于理解。假设周末即将来临，你听说自己所在的城市会举办奶酪节。你太喜欢奶酪了，但还是得考虑一下周末的天气情况。你将根据下面三个因素来做决定：</p><ol><li>天气怎样？</li><li>你的女朋友和你一起去吗？</li><li>节日举办地驾车方便吗？</li></ol><p>将这三种因素量化成二进制数 $x_1, x_2, x_3$。比如如果天气好，则 $x_1=1$，否则为 $x_1=0$。其他三种因素同理。现在假设你太喜欢奶酪了，以至于女朋友和交通不遍都不太影响你，但你又怕糟糕的天气弄脏衣服。我们可以将感知器设计为：天气权重$w_1=6$，女朋友权重 $w_2=2$ 和交通状况权重 $w_3=2$。可以看到天气占了很大的权重。最后将感知器阈值设为 5 便得到了我们需要的决策模型。一旦天气不好，感知器输出为 0，天气晴朗就输出 1。而女朋友同去与否和交通状况都没法影响感知器输出。</p><p>通过改变加权系数和阈值，便能得到不同的决策系统。比如将阈值调整为 3，这样女朋友就对你很重要啦，她要是想去，天气再糟你也得跟着一起受罪。</p><p>虽然感知器并不是人类决策系统的完整模型，但其能对各种条件做加权。而且似乎越复杂的网路越能做出微妙的决策：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201605/complex_perceptron.png" alt="复杂网络"></p><p>在这个网络中，第一列感知器 - 我们也称作感知器第一层，只是简单地对输入做加权。而第二层感知器则对第一层决策的结果再一步加权，做出更复杂更抽象的决定。同样还可以增加神经网络的层数来作出更复杂的决定。</p><p>顺便提一句，上述定义中，感知器只有一个输出，但是上述网络似乎有多个输出。事实上，这仍然是单输出系统，只是单个输出连接到了下一层的多个输入而已。</p><p>让我们来简化一下感知器的数学表达式，原来的判断条件 $\sum_jw_jx_j &gt; threshold$ 略显累赘。首先用点积形式简化，记 $w \cdot x \equiv \sum_j w_j x_j$，其中 $w$ 是权重向量，$x$ 是输入向量。然后将阈值移到不等式左边，并用偏移的概念取代它，记 $b \equiv - threshold$。感知器规则可重写如下：</p><p>\begin{equation}<br>output = \left\{\begin{aligned}<br>0, w \cdot x + b \leq 0 \\<br>1, w \cdot x + b &gt; 0<br>\end{aligned}\right.<br>\end{equation}</p><p>偏移的概念可用来度量感知器的“兴奋”程度，假如偏移值很大，那么很小的输入就会在输出端反应出来。但若偏移值很小，则说明感知器比较“迟钝”，输入很大时，输出才有变化。接下来的文章中，都会使用偏移而不是阈值的概念。</p><h2 id="sigmoid-神经元"><a href="#sigmoid-神经元" class="headerlink" title="sigmoid 神经元"></a>sigmoid 神经元</h2><p>自学习的 idea 听起来太棒了。如何为神经网络设计算法呢？假设我们的神经网络全部由感知器构成，输入为手写体数字扫描图的每一个原始像素点。我们希望神经网络能够自调整权重和偏移值，从而能对手写数字准确分类。为了解自学习过程，我们来做一个思想实验，假设我们在权重或偏移做一个小的改变，我们期望输出也会有相应小的变化：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201605/neural_net.png" alt="神经网络"></p><p>比如神经网络错误地将数字 9 认为为数字 8，我们就可以对参数做微调（可能某个人写的 9 像 8），修正输出，不断重复上述过程，从而使输出符合我们的预期。</p><p>实际中，由感知器组成的神经网络并不如所愿。由于感知器的输出不是连续的，0 到 1 是阶跃变化，上述参数的微调往往带来输出的剧烈变化。这下便导致自学习过程完全不可控，有时一点小小的噪声，输出就天壤之变。</p><p>针对这个问题，我们可以换用 sigmoid 神经元。sigmoid 神经元和感知器是类似的，但输出是连续且变化缓慢的。这个微小的不同使神经网络算法化成为了可能。</p><p>好，让我来描述一下 sigmoid 神经元。其结构和感知器一样：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201605/sigmoid.png" alt="sigmoid 神经元"></p><p>同样有输入 $x_1, x_2, …$。不同是，输入可以取 0 到 1 之间的任何值，比如 0.638。sigmoid 对每一个输入有一个权重，$w_1, w_2, …$，以及全局的偏移 $b$。但是 sigmoid 的输出不再限于 0 和 1，而是</p><p>\begin{equation}<br>  \sigma(z) \equiv \frac{1}{1+e^{-z}}.<br>\end{equation}</p><p>将 $z=w \cdot x+b$ 展开，可得</p><p>\begin{equation}<br>  \frac{1}{1+\exp(-\sum_j w_j x_j-b)}. \label{eq4}<br>\end{equation}</p><p>初看上去，sigmoid 神经元似乎与感知器有着天壤之别，其代数表达式也显得晦涩难懂。然而他们之间是有很多相似之处的。</p><p>假设当 $z\equiv w \cdot x + b$ 趋向于正无穷，则 $e^{-z}\approx 0$ 和 $\sigma(z) \approx 1$。换句话说，当输入很大时，sigmoid 神经元的输出趋向于 1，这和感知器是一样的。相反的，当 $z\equiv w \cdot x + b$ 趋向于负无穷，则 $e^{-z} \rightarrow \infty$，且 $\sigma(z) \approx 0$。这和感知器又是一样的。只有当输入不大时，才会与感知器表现不同。</p><p>让我们看一下 sigmoid 函数和阶跃函数的图像：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201605/sigmoid_function.png" alt="sigmoid 函数"></p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201605/step_function.png" alt="阶跃函数"></p><p>如果 $\sigma$ 是阶跃函数，那么 sigmoid 神经元就会退化成感知器，也就是说 sigmoid 神经元是平滑了的感知器。函数 $\sigma$ 的平滑度意味着，权重的微小变化 $\Delta w_j$ 和偏移的微小变化 $\Delta b$ 会在输出有相应的变化 $\Delta \mbox{output}$，运用泰勒公式可得：</p><p>\begin{eqnarray}<br>  \Delta \mbox{output} \approx \sum_j \frac{\partial \, \mbox{output}}{\partial w_j}<br>  \Delta w_j + \frac{\partial \, \mbox{output}}{\partial b} \Delta b,<br>\label{5}<br>\end{eqnarray}</p><p>其中，求和是对所有的权重和偏移变化求和。 $\partial \,\mbox{output} / \partial w_j$ 是 $\mbox{output}$ 对 $w_j$ 的偏导数，$\partial \, \mbox{output} /\partial b$ 是 $\mbox{output}$ 对 $b$ 的偏导数。从这个近似表达式可以看出，$\Delta \mbox{ouput}$ 是 $\Delta w_j, \Delta b$ 的线性函数。比起感知器那种非线性的输出输入关系，线性化便于调试，也有利于算法化。</p><p>如何理解 sigmoid 神经元的输出呢？显然最大的不同是 sigmoid 神经元不只输出 0 或 1，而是 0，1 之间所有的实数，比如 0.4 来指出一幅图片是 9 的概率为 40%，60% 的概率不是 9。</p><h2 id="神经网络的结构"><a href="#神经网络的结构" class="headerlink" title="神经网络的结构"></a>神经网络的结构</h2><p>神经网络的结构：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201605/net_structure.png" alt="神经网络的结构"></p><p>如上所述，最左边的那一层被称做输入层，其中的神经元是输入神经元。最右或者输出层包含了输出神经元，该例中只有一个输出神经元。由于中间的神经元既不是输入也不是输出，中间那层被称为隐藏层。该例中只有一个隐藏层，有些神经网络有多个隐藏层，比如下面这张图中有两个隐藏层：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201605/multi-layer_net_structure.png" alt="两个隐藏层"></p><p>神经网络输入输出的设计通常很直接。比如手写数字，假设扫描图是 $28 \times 28=784$ 的灰度图像，输入就有 784 个神经元，输出就是每个数字的概率，一共 10 个输出神经元。</p><h2 id="神经网络识别手写数字目录"><a href="#神经网络识别手写数字目录" class="headerlink" title="神经网络识别手写数字目录"></a>神经网络识别手写数字目录</h2><ol><li><a href="http://www.lyyyuna.com/2016/05/29/handwritten-neural-net/">基于 BP 神经网络的识别手写体数字 - 神经网络基础</a></li><li><a href="http://www.lyyyuna.com/2016/06/25/handwritten-neural-net02/">基于 BP 神经网络的手写体数字识别 - 设计与实现</a></li><li><a href="http://www.lyyyuna.com/2016/06/26/handwritten-neural-net03/">基于 BP 神经网络的手写体数字识别 - 反向传播算法</a></li><li><a href="http://www.lyyyuna.com/2016/06/30/handwritten-neural-net04/">基于 BP 神经网络的手写体数字识别 - 优化</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } }
});
&lt;/script&gt;

&lt;h2 id=&quot;前言&quot;&gt;&lt;a h
      
    
    </summary>
    
      <category term="数学" scheme="http://www.lyyyuna.com/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="mnist" scheme="http://www.lyyyuna.com/tags/mnist/"/>
    
      <category term="neural network" scheme="http://www.lyyyuna.com/tags/neural-network/"/>
    
  </entry>
  
  <entry>
    <title>Robot Framework 教程 - 循环，条件判断，字符串和列表（译）</title>
    <link href="http://www.lyyyuna.com/2016/05/28/robotframework-tutorial-loops-conditional-execution-and-more/"/>
    <id>http://www.lyyyuna.com/2016/05/28/robotframework-tutorial-loops-conditional-execution-and-more/</id>
    <published>2016-05-28T01:05:09.000Z</published>
    <updated>2017-11-29T00:19:00.167Z</updated>
    
    <content type="html"><![CDATA[<p>目前为止，<a href="http://robotframework.org/" target="_blank" rel="noopener">Robot Framework</a>教程一直关注于高阶抽象的概念，所以这次有必要介绍一下框架本身的基础概念。所有这些特性都直接来自于标准库，而本文的例子再安装额外的库。</p><h2 id="有关循环的关键字"><a href="#有关循环的关键字" class="headerlink" title="有关循环的关键字"></a>有关循环的关键字</h2><p>让我们从循环开始。<a href="http://robotframework.org/" target="_blank" rel="noopener">Robot Framework</a>为循环提供了多种方案。</p><ul><li>在一系列元素中循环</li><li>根据数字的范围来循环</li><li>重复执行某一个关键字多次</li></ul><p>最后一个和真正的循环是有差别的，意味着你得将所有操作封装到一个关键字中。并且，在执行结束之前无法退出循环。</p><p>让我们来看一些例子：</p><p><strong>测试</strong>：</p><pre><code>*** Settings ***Library           String*** Test Cases ***For-Loop-In-Range    : FOR    ${INDEX}    IN RANGE    1    3    \    Log    ${INDEX}    \    ${RANDOM_STRING}=    Generate Random String    ${INDEX}    \    Log    ${RANDOM_STRING}For-Loop-Elements    @{ITEMS}    Create List    Star Trek    Star Wars    Perry Rhodan    :FOR    ${ELEMENT}    IN    @{ITEMS}    \    Log    ${ELEMENT}    \    ${ELEMENT}    Replace String    ${ELEMENT}    ${SPACE}    ${EMPTY}    \    Log    ${ELEMENT}For-Loop-Exiting    @{ITEMS}    Create List    Good Element 1    Break On Me    Good Element 2    :FOR    ${ELEMENT}    IN    @{ITEMS}    \    Log    ${ELEMENT}    \    Run Keyword If    &apos;${ELEMENT}&apos; == &apos;Break On Me&apos;    Exit For Loop    \    Log    Do more actions here ...Repeat-Action    Repeat Keyword    2    Log    Repeating this ...</code></pre><p> <strong>输出</strong>：</p><pre><code>Starting test: StandardLoopDemo.For-Loop-In-Range20130426 11:24:14.389 :  INFO : 120130426 11:24:14.390 :  INFO : ${RANDOM_STRING} = B20130426 11:24:14.390 :  INFO : B20130426 11:24:14.391 :  INFO : 220130426 11:24:14.392 :  INFO : ${RANDOM_STRING} = ih20130426 11:24:14.392 :  INFO : ihEnding test:   StandardLoopDemo.For-Loop-In-RangeStarting test: StandardLoopDemo.For-Loop-Elements20130426 11:24:14.394 :  INFO : @{ITEMS} = [ Star Trek | Star Wars | Perry Rhodan ]20130426 11:24:14.395 :  INFO : Star Trek20130426 11:24:14.396 :  INFO : ${ELEMENT} = StarTrek20130426 11:24:14.396 :  INFO : StarTrek20130426 11:24:14.397 :  INFO : Star Wars20130426 11:24:14.398 :  INFO : ${ELEMENT} = StarWars20130426 11:24:14.398 :  INFO : StarWars20130426 11:24:14.399 :  INFO : Perry Rhodan20130426 11:24:14.400 :  INFO : ${ELEMENT} = PerryRhodan20130426 11:24:14.400 :  INFO : PerryRhodanEnding test:   StandardLoopDemo.For-Loop-ElementsStarting test: StandardLoopDemo.For-Loop-Exiting20130426 11:24:14.402 :  INFO : @{ITEMS} = [ Good Element 1 | Break On Me | Good Element 2 ]20130426 11:24:14.402 :  INFO : Good Element 120130426 11:24:14.403 :  INFO : Do more actions here ...20130426 11:24:14.404 :  INFO : Break On MeEnding test:   StandardLoopDemo.For-Loop-ExitingStarting test: StandardLoopDemo.Repeat-Action20130426 11:24:14.408 :  INFO : Repeating keyword, round 1/220130426 11:24:14.408 :  INFO : Repeating this ...20130426 11:24:14.408 :  INFO : Repeating keyword, round 2/220130426 11:24:14.409 :  INFO : Repeating this ...Ending test:   StandardLoopDemo.Repeat-Action   </code></pre><p>语法非常直接，不需要过多解释。唯一需要注意的是，循环体内的关键字必须用 ‘\’ 来进行转义。</p><h2 id="有关条件判断的关键字"><a href="#有关条件判断的关键字" class="headerlink" title="有关条件判断的关键字"></a>有关条件判断的关键字</h2><p>在测试代码中使用条件判断会带来不少争议。不用担心，唯一应该记住的是，测试实现应该尽可能简单明了，不要混杂过多条件逻辑。</p><p>在下面的例子中将使用以下相关的关键字。</p><ul><li><strong>Run Keyword</strong> - 这个关键字将其他关键字作为一个变量传入。这意味着，测试能够动态地改变执行时所使用的关键字，比如执行其他函数返回的关键字。</li><li><strong>Run Keyword If</strong> - 在测试复杂结构时非常有用，比如被测的 web 页面在输入不同时会有不同的选项。但是在测试中混有过多的程序结构会使 troubleshooting 变得困难。</li><li><strong>Run Keyword And Ignore Error</strong> - 哈，我还没有找到对应的实际例子。</li><li><strong>Run Keyword If Test Failed</strong> - 如果测试失败了可以用这个打一些 log，或者打一个快照。在 troubleshooting 时有用。</li></ul><p><strong>测试</strong>：</p><pre><code>*** Test Cases ***Run-Keyword    ${MY_KEYWORD}=    Set Variable    Log    Run Keyword    ${MY_KEYWORD}    TestRun-Keyword-If    ${TYPE}=    Set Variable    V1    Run Keyword If    &apos;${TYPE}&apos; == &apos;V1&apos;    Log     Testing Variant 1    Run Keyword If    &apos;${TYPE}&apos; == &apos;V2&apos;    Log    Testing Variant 2    Run Keyword If    &apos;${TYPE}&apos; == &apos;V3&apos;    Log    Testing Variant 3Run-Keyword-Ignore-Error    @{CAPTAINS}    Create List    Picard    Kirk    Archer    Run Keyword And Ignore Error    Should Be Empty    ${CAPTAINS}    Log    Reached this point despite of error</code></pre><p><strong>输出</strong>：</p><pre><code>Starting test: Robot Blog.StandardConditionDemo.Run-Keyword20130426 13:34:50.840 :  INFO : ${MY_KEYWORD} = Log20130426 13:34:50.841 :  INFO : TestEnding test:   Robot Blog.StandardConditionDemo.Run-KeywordStarting test: Robot Blog.StandardConditionDemo.Run-Keyword-If20130426 13:34:50.843 :  INFO : ${TYPE} = V120130426 13:34:50.844 :  INFO : Testing Variant 1Ending test:   Robot Blog.StandardConditionDemo.Run-Keyword-IfStarting test: Robot Blog.StandardConditionDemo.Run-Keyword-Ignore-Error20130426 13:34:50.847 :  INFO : @{CAPTAINS} = [ Picard | Kirk | Archer ]20130426 13:34:50.848 :  INFO : Length is 320130426 13:34:50.849 :  FAIL : &apos;[u&apos;Picard&apos;, u&apos;Kirk&apos;, u&apos;Archer&apos;]&apos; should be empty20130426 13:34:50.850 :  INFO : Reached this point despite of errorEnding test:   Robot Blog.StandardConditionDemo.Run-Keyword-Ignore-Error</code></pre><h2 id="字符串和列表"><a href="#字符串和列表" class="headerlink" title="字符串和列表"></a>字符串和列表</h2><p>可以看到，<a href="http://robotframework.org/" target="_blank" rel="noopener">Robot Framework</a>框架包含了完整的可编程结构。而一些高级语言特有的字符串和列表也能通过 Collection Library 和 String Library 来实现。</p><p><strong>测试</strong>：</p><pre><code>*** Settings ***Library           StringLibrary           Collections*** Test Cases ***StringsAndLists    ${SOME_VALUE}=    Set Variable    &quot;Test Value&quot;    Log    ${SOME_VALUE}    @{WORDS}=    Split String    ${SOME_VALUE}    ${SPACE}    ${FIRST}=    Get From List    ${WORDS}    0    Log    ${FIRST}</code></pre><p><strong>输出</strong>：</p><pre><code>Starting test: Robot Blog.StandardStringsAndListsDemo.StringsAndLists20130506 21:21:05.880 :  INFO : ${SOME_VALUE} = &quot;Test Value&quot;20130506 21:21:05.881 :  INFO : &quot;Test Value&quot;20130506 21:21:05.882 :  INFO : @{WORDS} = [ &quot;Test | Value&quot; ]20130506 21:21:05.882 :  INFO : ${FIRST} = &quot;Test20130506 21:21:05.883 :  INFO : &quot;TestEnding test:   Robot Blog.StandardStringsAndListsDemo.StringsAndLists</code></pre><h2 id="Robot-Framework-教程目录"><a href="#Robot-Framework-教程目录" class="headerlink" title="Robot Framework 教程目录"></a>Robot Framework 教程目录</h2><p><a href="https://blog.codecentric.de/en/2013/05/robot-framework-tutorial-loops-conditional-execution-and-more/" target="_blank" rel="noopener">原文链接</a></p><ol><li><a href="http://www.lyyyuna.com/2016/01/07/robotframework-tutorial-overview/">Robot Framework 教程 - 概览（译）</a></li><li><a href="http://www.lyyyuna.com/2016/04/09/robotframework-tutorial-a-complete-example/">Robot Framework 教程 - 一个完整的例子（译）</a></li><li><a href="http://www.lyyyuna.com/2016/04/30/robotframework-ide-ride-overview/">Robot Framework 教程 - 集成开发环境 RIDE 概览 (译)</a></li><li><a href="http://www.lyyyuna.com/2016/05/15/robotframework-tutorial-how-to-structure-a-scalable-and-maintainable-acceptance-test-suite/">Robot Framework 教程 - 如何组织一个可伸缩可维护的验收测试套件（译）</a></li><li><a href="http://www.lyyyuna.com/2016/05/28/robotframework-tutorial-loops-conditional-execution-and-more/">Robot Framework 教程 - 循环，条件判断，字符串和列表（译）</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;目前为止，&lt;a href=&quot;http://robotframework.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Robot Framework&lt;/a&gt;教程一直关注于高阶抽象的概念，所以这次有必要介绍一下框架本身的基础概念。所有这些特性都直接
      
    
    </summary>
    
      <category term="自动化测试" scheme="http://www.lyyyuna.com/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/"/>
    
    
      <category term="robot framework" scheme="http://www.lyyyuna.com/tags/robot-framework/"/>
    
  </entry>
  
  <entry>
    <title>Robot Framework 教程 - 如何组织一个可伸缩可维护的验收测试套件（译）</title>
    <link href="http://www.lyyyuna.com/2016/05/15/robotframework-tutorial-how-to-structure-a-scalable-and-maintainable-acceptance-test-suite/"/>
    <id>http://www.lyyyuna.com/2016/05/15/robotframework-tutorial-how-to-structure-a-scalable-and-maintainable-acceptance-test-suite/</id>
    <published>2016-05-15T02:56:18.000Z</published>
    <updated>2017-11-29T00:19:00.167Z</updated>
    
    <content type="html"><![CDATA[<p>当你在一个 sprint 中开始写自动化验收测试时，你没必要重新测试之前每个 sprint 的结果。但经过几轮自动化测试之后，整个测试看起来不再像一个精心设计过的测试套件，而是乱七八糟。这些你一定经历过。这篇文章将展示一些最佳的模式和经验，让你写出可伸缩可维护的测试结构。</p><p>我们只考虑测试框架本身，忽略和执行有关的问题，比如日志系统、并发和测试硬件。由于我们一直使用 <a href="http://robotframework.org/" target="_blank" rel="noopener">Robot Framework</a> 来实现自动化，所以本文的解决方法会有一些局限性。但其他测试框架的用户也可以参考，比如 FitNesse, Cucumber, Concordion, etc。</p><p>好，一个单独的，精心设计的验收测试并不会存在太长时间，但如何写一个可维护的测试套件？这里我用测试套件这个术语，意味着并不是单单指测试用例，还包括了库、启动脚本和框架等。</p><p>由于经常改变项目需求和源码实现，测试套件也需要跟着改变。如何让测试套件尽可能地适应各种变化？显然，需要在其中分离出可变和稳定的部分。</p><p>稳定的部分是指测试框架本身和附加的库。测试用例也会尽可能作为不变的部分，除非需求改变没理由要改变它们。当然，这里允许添加新的测试用例。这里不经要问了，既然框架和测试用例都为稳定的部分，那什么是可变的部分呢？下图展示了一个可伸缩可维护的测试套件的结构。下面会对图中每一层作详细的解释。简单的说，如果系统的上下两头都需要稳定，那么可变的部分只有中间那层。</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201605/SMAT_structure.png" alt="测试套件结构"></p><p>不同的颜色代表了 Robot Framework 中不同的文件：</p><ol><li><strong>红色</strong>：测试用例和套件。</li><li><strong>绿色</strong>：资源文件。资源文件包括关键字。一个关键字可以是一个方法，或者是其他方法的组合。</li><li><strong>蓝色</strong>：框架自身的内容。</li></ol><p>在图的左边，你可以看到对每一层稳定性和可变性的度量。在最顶层和最底层，稳定性最高，而在最中间那一层，可变性又变的最高。这意味着中间那层的元素经常改变，而其他相对恒定。</p><h2 id="每一层"><a href="#每一层" class="headerlink" title="每一层"></a>每一层</h2><h3 id="测试用例"><a href="#测试用例" class="headerlink" title="测试用例"></a>测试用例</h3><p>我们推荐的是 “Given/When/Then” 方式来写用例。这意味着只有当需求变化时，才需要动测试用例。</p><h3 id="测试套件"><a href="#测试套件" class="headerlink" title="测试套件"></a>测试套件</h3><p>测试用例应该按功能组织为测试套件。开头，我们按照用户需求对测试用例分类。在回顾时，发现当相同的用户需求增多时，得花越来越多的时间搜索特定的用例。所以，对用户需求和分类的用例打上<strong>标签</strong>，将有利于搜索。</p><h3 id="导入"><a href="#导入" class="headerlink" title="导入"></a>导入</h3><p>下一层是“导入”。这一层包含了自动化验收测试所需的关键字，能够建立测试用例和资源文件之间的对应关系。既然用例不应经常改变，那么关键字就需要持续重构了。由于测试套件之间包含一些类似的测试用例，它们理应具有相同的导入，这样只需为所有的测试套件写一份资源文件。</p><h3 id="聚合对象"><a href="#聚合对象" class="headerlink" title="聚合对象"></a>聚合对象</h3><p>聚合对象这层改动最为频繁。比如在测试 web UI 时，会建立页面对象的模式。从一个web页抽象的概念，在这一层如何构建经典的软件设计与工程：结构的灵活性和可维护的代码？</p><h3 id="库适配器"><a href="#库适配器" class="headerlink" title="库适配器"></a>库适配器</h3><p>我们增加了库适配器层。每一个库都应该由资源文件来导入，这样能保证系统中只有一个库的实例。而且有时候库需要用不同的参数来初始化。并且，我们随时会在适配器层增加关键字来扩展功能，而对上层的测试用例保持黑盒状态。</p><h3 id="平台"><a href="#平台" class="headerlink" title="平台"></a>平台</h3><p>这一层是指 RObot Framework 框架本身和标准库。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>一方面，我们构建测试套件的方法产生了深远的影响。在另一方面，我们继续学习新的东西，我相信新的经验和教训将进一步影响测试用例和套件的结构和设计。希望其他项目能从中受惠。</p><h2 id="Robot-Framework-教程目录"><a href="#Robot-Framework-教程目录" class="headerlink" title="Robot Framework 教程目录"></a>Robot Framework 教程目录</h2><p><a href="https://blog.codecentric.de/en/2010/07/how-to-structure-a-scalable-and-maintainable-acceptance-test-suite/" target="_blank" rel="noopener">原文链接</a></p><ol><li><a href="http://www.lyyyuna.com/2016/01/07/robotframework-tutorial-overview/">Robot Framework 教程 - 概览（译）</a></li><li><a href="http://www.lyyyuna.com/2016/04/09/robotframework-tutorial-a-complete-example/">Robot Framework 教程 - 一个完整的例子（译）</a></li><li><a href="http://www.lyyyuna.com/2016/04/30/robotframework-ide-ride-overview/">Robot Framework 教程 - 集成开发环境 RIDE 概览 (译)</a></li><li><a href="http://www.lyyyuna.com/2016/05/15/robotframework-tutorial-how-to-structure-a-scalable-and-maintainable-acceptance-test-suite/">Robot Framework 教程 - 如何组织一个可伸缩可维护的验收测试套件（译）</a></li><li><a href="http://www.lyyyuna.com/2016/05/28/robotframework-tutorial-loops-conditional-execution-and-more/">Robot Framework 教程 - 循环，条件判断，字符串和列表（译）</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;当你在一个 sprint 中开始写自动化验收测试时，你没必要重新测试之前每个 sprint 的结果。但经过几轮自动化测试之后，整个测试看起来不再像一个精心设计过的测试套件，而是乱七八糟。这些你一定经历过。这篇文章将展示一些最佳的模式和经验，让你写出可伸缩可维护的测试结构。&lt;
      
    
    </summary>
    
      <category term="自动化测试" scheme="http://www.lyyyuna.com/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/"/>
    
    
      <category term="robot framework" scheme="http://www.lyyyuna.com/tags/robot-framework/"/>
    
  </entry>
  
  <entry>
    <title>DHT 公网嗅探器实现（DHT 爬虫）</title>
    <link href="http://www.lyyyuna.com/2016/05/14/dht-sniffer/"/>
    <id>http://www.lyyyuna.com/2016/05/14/dht-sniffer/</id>
    <published>2016-05-14T01:02:03.000Z</published>
    <updated>2017-11-29T00:19:00.162Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这里实现的 DHT 嗅探器会在公网捕获种子的 infohash，源码见 <a href="https://github.com/lyyyuna/DHT_sniffer" target="_blank" rel="noopener">GitHub</a>。</p><p>DHT 协议介绍见 <a href="http://www.lyyyuna.com/2016/03/26/dht01/">DHT 协议 - 译</a>。</p><p>其实大部分代码来自了 <a href="https://github.com/Fuck-You-GFW/simDHT/blob/master/simDHT.py" target="_blank" rel="noopener">simDHT</a>，我只是改成了 gevent 版本。</p><p>为什么我没有用 Python 3 的 asyncio 呢？最大的原因是我没有在官方 API 中找到方便使用 UDP 协议的接口。asyncio 底层有着类似 Twisted 的事件驱动编程，对于 TCP 协议，官方又封装了一层 Stream，可以用 await 类似同步的方式异步编程。但不知道为啥就是没有 UDP 的封装，见 <a href="https://groups.google.com/forum/#!topic/python-tulip/xYgQRXkb83g" target="_blank" rel="noopener">谷歌讨论组</a>，Guido van Rossum 他自己觉得不需要。然而事件编程我不习惯，在这个项目里要用违反直觉的方式封装，所以还是放弃了 asyncio。</p><h2 id="gevent-中的-UDP"><a href="#gevent-中的-UDP" class="headerlink" title="gevent 中的 UDP"></a>gevent 中的 UDP</h2><p>使用 gevent，对于原程序改动很小。使用 DatagramServer 作为 UDP server。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gevent.server <span class="keyword">import</span> DatagramServer</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DHTServer</span><span class="params">(DatagramServer)</span>:</span> </span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, max_node_qsize, bind_ip)</span>:</span> </span><br><span class="line">         s = <span class="string">':'</span> + str(bind_ip) </span><br><span class="line">         self.bind_ip = bind_ip </span><br><span class="line">         DatagramServer.__init__(self, s)</span><br></pre></td></tr></table></figure><h2 id="协议相关"><a href="#协议相关" class="headerlink" title="协议相关"></a>协议相关</h2><h3 id="如何捕获-infohash"><a href="#如何捕获-infohash" class="headerlink" title="如何捕获 infohash"></a>如何捕获 infohash</h3><p>虽是嗅探，但 DHT 的流量不会无缘故的跑过来，必须把自己伪装成一个客户端才能捕获到 DHT 流量。</p><p>以下设我们伪造的客户端为 X。</p><ol><li>首先向其他 node 发送 find_node 请求，所发送的 node 可以随机生成，我们的目的只是为了让对方 node 能在其路由表中记录下我们伪造的 X。</li><li>当其他 node 想要下载 torrent 时，便会向其路由表中最近的 node 依次发送 get_peers/announce_peer 请求。这样其必会向我们伪造的 X 发送 get_peers/announce_peer 请求，即包含真实的 infohash。</li></ol><p>这样，一个真实的 infohash 就到手了。总结就是，不断和其他 node 交朋友，然后等着他们发送 infohash 过来。</p><p>既然是伪造，意味着不需要实现完整的 DHT 协议。只需要 find_node 和 get_peers/announce_peer 请求即可。</p><h3 id="路由表"><a href="#路由表" class="headerlink" title="路由表"></a>路由表</h3><p>有必要实现完整的路由表吗？协议中的路由表需要维护一个较复杂的数据结构，监控一个 node 的健康程度，若不活跃则需将其删除。作为 DHT 嗅探器这是多余的，因为原数据结构是要保证路由表中是健康的节点，以提高下载速度。而我们是为了认识更多的节点，向 node 发送一次 find_node 请求之后，即可删除该数据。</p><p>队列，更符合伪造的客户端对 node 管理的需求。</p><pre><code>self.nodes = deque(maxlen=max_node_qsize)</code></pre><p>其他 node 除了会发送 get_peers 请求来获取 torrent 之外，也会发送 find_node 来获取节点信息。这意味着，其他 node 的 find_node 请求便是我们更新 node 信息的来源。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_find_node_response</span><span class="params">(self, msg, address)</span>:</span></span><br><span class="line">    <span class="comment"># print 'find node' + str(msg)</span></span><br><span class="line">    nodes = decode_nodes(msg[<span class="string">"r"</span>][<span class="string">"nodes"</span>])</span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">        (nid, ip, port) = node</span><br><span class="line">        <span class="keyword">if</span> len(nid) != <span class="number">20</span>: <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> ip == self.bind_ip: <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> port &lt; <span class="number">1</span> <span class="keyword">or</span> port &gt; <span class="number">65535</span>: <span class="keyword">continue</span></span><br><span class="line">        n = KNode(nid, ip, port)</span><br><span class="line">        self.nodes.append(n)</span><br></pre></td></tr></table></figure><p>我们伪造的 X 从队列中取出一个 node，然后发送 find_node，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">auto_send_find_node</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line">    wait = <span class="number">1.0</span> / self.max_node_qsize / <span class="number">5.0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            node = self.nodes.popleft()</span><br><span class="line">            self.send_find_node((node.ip, node.port), node.nid)</span><br><span class="line">        <span class="keyword">except</span> IndexError:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        sleep(wait)</span><br></pre></td></tr></table></figure><p>实际测试表明，运行时队列一直是满的，所以不用担心 node 不够用。</p><h3 id="解析-infohash"><a href="#解析-infohash" class="headerlink" title="解析 infohash"></a>解析 infohash</h3><p>get_peers 和 announce_peer 请求都含有 infohash。虽然 announce_peer 请求质量更高，但数量少。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_get_peers_request</span><span class="params">(self, msg, address)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        infohash = msg[<span class="string">"a"</span>][<span class="string">"info_hash"</span>]</span><br><span class="line">        tid = msg[<span class="string">"t"</span>]</span><br><span class="line">        nid = msg[<span class="string">"a"</span>][<span class="string">"id"</span>]</span><br><span class="line">        token = infohash[:TOKEN_LENGTH]</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'get peers: '</span> + infohash.encode(<span class="string">"hex"</span>),  address[<span class="number">0</span>], address[<span class="number">1</span>]</span><br><span class="line">        msg = &#123;</span><br><span class="line">            <span class="string">"t"</span>: tid,</span><br><span class="line">            <span class="string">"y"</span>: <span class="string">"r"</span>,</span><br><span class="line">            <span class="string">"r"</span>: &#123;</span><br><span class="line">                <span class="string">"id"</span>: get_neighbor(infohash, self.nid),</span><br><span class="line">                <span class="string">"nodes"</span>: <span class="string">""</span>,</span><br><span class="line">                <span class="string">"token"</span>: token</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        self.send_krpc(msg, address)</span><br><span class="line">    <span class="keyword">except</span> KeyError:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_announce_peer_request</span><span class="params">(self, msg, address)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'announce peer'</span></span><br><span class="line">        infohash = msg[<span class="string">"a"</span>][<span class="string">"info_hash"</span>]</span><br><span class="line">        token = msg[<span class="string">"a"</span>][<span class="string">"token"</span>]</span><br><span class="line">        nid = msg[<span class="string">"a"</span>][<span class="string">"id"</span>]</span><br><span class="line">        tid = msg[<span class="string">"t"</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> infohash[:TOKEN_LENGTH] == token:</span><br><span class="line">            <span class="keyword">if</span> msg[<span class="string">"a"</span>].has_key(<span class="string">"implied_port"</span>) <span class="keyword">and</span> msg[<span class="string">"a"</span>][<span class="string">"implied_port"</span>] != <span class="number">0</span>:</span><br><span class="line">                port = address[<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                port = msg[<span class="string">"a"</span>][<span class="string">"port"</span>]</span><br><span class="line">                <span class="keyword">if</span> port &lt; <span class="number">1</span> <span class="keyword">or</span> port &gt; <span class="number">65535</span>: <span class="keyword">return</span></span><br><span class="line">            <span class="keyword">print</span> <span class="string">'announce peer: '</span> + infohash.encode(<span class="string">"hex"</span>),  address[<span class="number">0</span>], port</span><br><span class="line">    <span class="keyword">except</span> Exception:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        self.ok(msg, address)</span><br></pre></td></tr></table></figure><p>对着 <a href="http://www.lyyyuna.com/2016/03/26/dht01/">DHT 协议 - 译</a> 很容易看懂。</p><h3 id="嗅探器启动"><a href="#嗅探器启动" class="headerlink" title="嗅探器启动"></a>嗅探器启动</h3><p>看到这里，你会发现嗅探器启动时，队列是空的。所以必须先放几个已知的 node。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">BOOTSTRAP_NODES = (</span><br><span class="line">    (<span class="string">"router.bittorrent.com"</span>, <span class="number">6881</span>),</span><br><span class="line">    (<span class="string">"dht.transmissionbt.com"</span>, <span class="number">6881</span>),</span><br><span class="line">    (<span class="string">"router.utorrent.com"</span>, <span class="number">6881</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">join_DHT</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> address <span class="keyword">in</span> BOOTSTRAP_NODES:</span><br><span class="line">            self.send_find_node(address)</span><br></pre></td></tr></table></figure><p>启动之后，队列就会被其他 node 发送的 find_node 所填满。</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>公网捕获，一小时 10000 个左右。</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201605/psb.jpg" alt="效果图"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;这里实现的 DHT 嗅探器会在公网捕获种子的 infohash，源码见 &lt;a href=&quot;https://github.com/lyyyun
      
    
    </summary>
    
      <category term="网络" scheme="http://www.lyyyuna.com/categories/%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="DHT" scheme="http://www.lyyyuna.com/tags/DHT/"/>
    
  </entry>
  
  <entry>
    <title>Robot Framework 教程 - 集成开发环境 RIDE 概览 (译)</title>
    <link href="http://www.lyyyuna.com/2016/04/30/robotframework-ide-ride-overview/"/>
    <id>http://www.lyyyuna.com/2016/04/30/robotframework-ide-ride-overview/</id>
    <published>2016-04-30T07:30:49.000Z</published>
    <updated>2017-11-29T00:19:00.165Z</updated>
    
    <content type="html"><![CDATA[<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>Robot Framework IDE (RIDE) 是该框架本身的集成开发环境。<a href="http://code.google.com/p/robotframework/" target="_blank" rel="noopener">Robot Framework</a> 是一个通用的自动化测试框架，<a href="http://www.lyyyuna.com/2015/12/28/robotframework-quickstartguide/">这里有其简单介绍</a>。</p><p>改项目原本托管在 <a href="http://code.google.com/p/robotframework-ride/" target="_blank" rel="noopener">Google Code</a> 上，现托管在 <a href="https://github.com/robotframework/RIDE/downloads" target="_blank" rel="noopener">GitHub</a> 上。 </p><p>下载和安装这种事就不用我再重复了。当你打开 RIDE，导入一个测试套件或者是包含几个测试套件的目录时，就会在编辑器的左方展现一个树形结构。可以针对该树形结构，选择每一个测试套件的每一个测试用例。而且，每一个被引用的资源文件都会自动被装载，并显示在树形结构的 External Resources 中。只要测试套件能够被选中，就能修改其全局属性，例如 Suite-Setuo 和 Suite-Teardown。</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201605/RIDE_1.png" alt="RIDE 1"></p><p>使用 RIDE 的一个巨大优势是可以从各个方面去配置测试套件。如果什么都是手写的，你很有可能会遗漏某些特性。而且 RIDE 中还自带完整的 Robot Framework 文档，在写关键字的时候很有帮助。当按下编辑按钮时，会弹出新窗口供你编辑。对于一些特殊的语法，比如关键字的参数使用惯导符号分割，不用担心，窗口中都会有文字提示。</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201605/ride_3.png" alt="argument"></p><p>一旦你适应了如何编辑这些项。。。实际上也很容易。编辑单个测试用例时，你必须从树形结构中选取。每个用例的通用项 - 比如文档和标签 - 可以在编辑器的上方填入。在编辑器下方，是由关键字组成的用例步骤，用表格的形式给出。关键一般写在第一列，第二列开始是对应的参数。如图所示，都比较直观。</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201605/RIDE_7.png" alt="编辑器"></p><p>接下来是文本编辑器部分，你可以从上方的面板中切换。RIDE 并不适用 HTML 格式来存储用例，而是采用纯粹的文本文件格式。这估计是因为 HTML 源文件难编辑的多的原因。RIDE 内部会对文本解析，展现在可视化编辑区。</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201605/RIDE_2.png" alt="源文件"></p><p>在文本编辑和可是还编辑之间切换平滑，改动会自动同步。</p><p>习惯了像 Visual Studio 这么方便的 IDE 的同学，肯定也希望 RIDE 有自动补全功能，当然它有。你只需要在写关键字时按下 ‘CTRL-Space’ 就可以了。在空白处，RIDE 会显示库中所有的可能项。请注意，下面截图的显示并不完整。</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201605/RIDE_33.png" alt="关键字自动补全"></p><p>对于那些在资源文件中定义的关键字，你可以在测试用例中双击，并跳转到定义的地方。相反的，你还可以看到所有使用该关键字的地方。你可以从菜单栏上选择 ‘Tools -&gt; Search Keywords -&gt; Find Usages’ 来找到它们。</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201605/RIDE_4.png" alt="关键字所有的引用"></p><p>这个功能在重构测试用例时非常有用，通过单击每一项搜索结果，可以直接在编辑器中跳转到相应位置。</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201605/RIDE_51.png" alt="搜索结果"></p><p>最后来看一下第三个面板，<strong>Run</strong>。它可以让用户直接运行打开的测试套件中的用例。运行的脚本可以是 pybot, jybot 或者是自定义脚本。对于小型项目，前两个选项够用了。但是大型项目需要额外的运行参数，和更多的独立的启动配置脚本。</p><p>脚本的运行结果可以在编辑器中看到。下面的例子可以看到，所有的测试用例都没通过:-)。</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201605/RIDE_6.png" alt="测试失败"></p><p>我个人对 RIDE 的评价是，作为集成开发环境，它刚好够格，因为也没别的更好选择。这个工具提供了很多指南和内部文档，对于非技术人员无疑是有利的。</p><h2 id="Robot-Framework-教程目录"><a href="#Robot-Framework-教程目录" class="headerlink" title="Robot Framework 教程目录"></a>Robot Framework 教程目录</h2><p><a href="https://blog.codecentric.de/en/2012/01/robot-framework-ide-ride-overview/" target="_blank" rel="noopener">原文链接</a></p><ol><li><a href="http://www.lyyyuna.com/2016/01/07/robotframework-tutorial-overview/">Robot Framework 教程 - 概览（译）</a></li><li><a href="http://www.lyyyuna.com/2016/04/09/robotframework-tutorial-a-complete-example/">Robot Framework 教程 - 一个完整的例子（译）</a></li><li><a href="http://www.lyyyuna.com/2016/04/30/robotframework-ide-ride-overview/">Robot Framework 教程 - 集成开发环境 RIDE 概览 (译)</a></li><li><a href="http://www.lyyyuna.com/2016/05/15/robotframework-tutorial-how-to-structure-a-scalable-and-maintainable-acceptance-test-suite/">Robot Framework 教程 - 如何组织一个可伸缩可维护的验收测试套件（译）</a></li><li><a href="http://www.lyyyuna.com/2016/05/28/robotframework-tutorial-loops-conditional-execution-and-more/">Robot Framework 教程 - 循环，条件判断，字符串和列表（译）</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;正文&quot;&gt;&lt;a href=&quot;#正文&quot; class=&quot;headerlink&quot; title=&quot;正文&quot;&gt;&lt;/a&gt;正文&lt;/h2&gt;&lt;p&gt;Robot Framework IDE (RIDE) 是该框架本身的集成开发环境。&lt;a href=&quot;http://code.google.c
      
    
    </summary>
    
      <category term="自动化测试" scheme="http://www.lyyyuna.com/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/"/>
    
    
      <category term="robot framework" scheme="http://www.lyyyuna.com/tags/robot-framework/"/>
    
  </entry>
  
  <entry>
    <title>Robot Framework 教程 - 一个完整的例子（译）</title>
    <link href="http://www.lyyyuna.com/2016/04/09/robotframework-tutorial-a-complete-example/"/>
    <id>http://www.lyyyuna.com/2016/04/09/robotframework-tutorial-a-complete-example/</id>
    <published>2016-04-09T06:50:08.000Z</published>
    <updated>2017-11-29T00:19:00.166Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>用 <a href="http://code.google.com/p/robotframework/" target="_blank" rel="noopener">Robot Framework</a> 时有太多的选择：</p><ul><li>使用 Python, Jython 还是 Java？</li><li>测试用例使用哪种输入格式（HTML, Text, BDD）？</li><li>要使用 <a href="http://www.lyyyuna.com/2016/01/07/robotframework-tutorial-overview/">Robot IDE(RIDE)</a> 吗？</li><li>如何在本地和持续集成环境中运行相同的测试？</li><li>如何运行所有的测试 (scripting, ANT, Maven)？</li></ul><p>那什么是最好的选择呢？我见过的世面太多了。当然，在 Eclipse 中用 Maven 做 Robot 测试非常酷。BDD 相比较 HTML 更适合敏捷开发。</p><p>所有这些有着相同的共性：简单！这不仅意味着设置和运行简单，还意味着更容易排错。在不同技术背景的团队合作间，这尤其重要。</p><p>接下来我们用一个完整的例子展示 Robot Framework 的使用方法。</p><h2 id="测试准备"><a href="#测试准备" class="headerlink" title="测试准备"></a>测试准备</h2><p>开始测试工程前，首先要想好被测系统需要哪些测试库：</p><ul><li>是测 web 应用？那你可能需要 SeleniumLibrary 或者 Selenium2Library。</li><li>是测数据库？Python 和 Java 都有相应的数据库测试库。</li><li>是测试 SSH/SFTP？那你可能需要 SSHLibrary。</li></ul><p>这个列表可以继续列下去，直到没有可用的测试库为止。这时候你就需要自己写啦（需要单独写一篇文章来阐述）。</p><p>为什么如此重要？测试库的选择直接影响到了是使用 Python, Jython 还是 Java 版的 Robot Framework。某些测试库只有 Java 的实现，如果要用纯 Python 来调用此库，则要求其实现 <strong>Remote Library 接口</strong>。因此，在测之前，需要好好想想。</p><blockquote><p>本文的代码在 <a href="https://github.com/lyyyuna/Robot-Framework-Sample-Project" target="_blank" rel="noopener">GitHub</a></p></blockquote><p>我们假设被测系统是一个利用 MySQL 数据库做存储的 web 应用（非常普遍）。浏览器使用 Python 的 SeleniumLibrary，数据库使用 Java 版本的 DatabaseLibrary，并用 <strong>Remote Library 接口</strong>。</p><h2 id="测试框架"><a href="#测试框架" class="headerlink" title="测试框架"></a>测试框架</h2><p>下图是测试框架的概览：</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201604/Sample_Overview1.png" alt="测试框架概览"></p><p>Selenium 需要 Selenium Server。这可以是 Robot Framework 所在的同一台机器，也可以是另一台可通过 TCP/IP 连接的服务器。Database Library Server 同理。虽然 DatabaseLibrary 可以本地使用，但那就意味着必须使用 Jython 来测试了。当然也可以在同一台机器上运行多个服务器。在一些正式的测试环境中，Robot Framework 和 CI (持续集成) 服务器经常部署在一起。然后，Selenium Server 通常跑在 Windows 服务器上，因为需要尽量模拟用户的使用场景。DatabaseLibrary Server 也可以部署在 CI 服务器上。</p><h2 id="测试实现和管理"><a href="#测试实现和管理" class="headerlink" title="测试实现和管理"></a>测试实现和管理</h2><p>最后让我们来实现该测试。不是每一个细节都会 cover，具体可以看 <a href="https://github.com/lyyyuna/Robot-Framework-Sample-Project" target="_blank" rel="noopener">GitHub</a>。</p><p>但在此之前，让我们再多做一些常规性考虑。比如用哪种格式来组织测试用例，是否使用 RIDE。而 RIDE 的使用又会直接影响到测试用例的格式。团队成员的技术背景，以及不同团队合作潜在的维护成本，对上述选择都有影响。</p><blockquote><p>Tips: 如果你已经使用 Excel 来管理则是用例，你可以直接复制粘贴进 RIDE。</p></blockquote><p>要我在本文的例子中选择，我会选择 HTML 格式和 RIDE，理由如下：</p><ul><li>RIDE 相比较于最初版本已经有了十足的进步，支持关键字自动补全，实现 Test Suites 和 Resource Files 也十分便利。</li><li>使用 RIDE 不用特意考虑 BDD 风格。但其中有一些我不喜欢的语法元素。而且，非技术团队成员编写和维护测试用例比较困难，因为现在机器还不能完全看懂人类语言。并且我认为，如果 BDD 是唯一或者最重要的需求，其他那些只支持 BDD 的测试框架才会有优势。</li><li>HTML 格式有着简单粗暴的优点。你可以直接在浏览器中可视化这些测试用例，尤其是那些熟悉 Excel 的非技术团队成员，看到这些会感到非常亲切。</li><li>HTML 格式也有着缺点，在版本管理时，HTML 会带来各种各样的问题。</li></ul><p>在实现测试时最重要的就是能够同时在本地和正式测试环境（CI 服务器）中运行。幸运的是，Robot Framework 支持向关键字传入参数，这样便能轻松切换环境：</p><ul><li>参数为 web 应用的 URL</li><li>Selenium 服务器的 IP 地址与端口</li><li>Database Library 所使用的 JDBC 连接字符串</li></ul><p>这些参数可以存储在变量文件中。这些变量文件可以在命令行中传入 Robot Framework。由于在本地测试和 CI 服务器中有不同启动脚本，这样便能实现不同环境的快速切换。</p><p>这意味着最好以如下的目录树来组织你的测试工程。</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201604/Sample_DirectoryStructure2.png" alt="测试工程目录树"></p><p>定义一个通用的目录树结构有助于工程的复用。上述的目录结构对我来说工作的不错，我很早就开始用啦。</p><blockquote><p>Tips: 所有路径都应该用相对路径。</p></blockquote><p>首先，我们将所有文件放于顶层文件 robot 中。然后将测试的实现和执行分开存放。在实现这边，testsuites 和 resource 分开存放。当然在一些大型工程中，还需要额外的子目录来更好的组织测试用例。更重要的是，最好用相对路径来引用这些文件。使用相对路径能够更好的在不同系统间移植，项目成员间通过版本管理系统也能更好地共享工程。</p><p>执行分支这边必须处理不同运行目标环境的问题，比如本地开发环境和正式的 CI 环境。若还有其他的部署环境需要在此目录中实现。scripts 目录用于保存执行用的脚本（robot 本身，Selenium Server, Database Library Remote Server），setting 目录放置特殊的变量文件。请注意，这些脚本写完之后就不应该频繁改动，对于配置文件亦是如此，除非执行环境有变化。</p><p>最后是 lib 文件夹，这取决于项目是否需要自己编写库文件。</p><h2 id="执行测试"><a href="#执行测试" class="headerlink" title="执行测试"></a>执行测试</h2><p>当执行测试时，我坚持使用 shell 脚本。易于理解，历史悠久且不出问题，在 CI 环境中使用方便。当然，我们很可能需要两套不同的启动脚本，因为本地测试通常在 Windows 电脑上，而正式的 CI 环境是一些 bash 或者 csh 脚本。但需注意，这些写了“写了一遍就忘记”的脚本，其实并不复杂。</p><p>在最初，我们需要三个脚本：</p><ol><li>robot 测试的启动脚本</li><li>Selenium Server 的启动脚本</li><li>Database Library Remote Server 的启动脚本</li></ol><p>我们也可以把三个脚本合并成一个，但为什么不这么做呢，因为其实后两个服务器只需启动一次，只有测试才需要重复执行。</p><h2 id="整合"><a href="#整合" class="headerlink" title="整合"></a>整合</h2><p>首先我们需要在开发机器上安装 Robot Framework 和库。我们假设平台是 Windows，在 Unix 上安装也不会太复杂。</p><blockquote><p>Robot Framework 同时支持 2.x 和 3.x。</p></blockquote><p>安装如下的工具包：</p><ul><li>Python 2.7</li><li>Robot Framwork</li><li>wxPython</li><li>RIDE</li><li>Selenium2Library</li><li>Database Library Server</li></ul><p>按顺序安装，然后配置 PATH 目录为  “C:\Python27;C:\Python27\Scripts”。现在你可以用</p><pre><code>pybot</code></pre><p>来运行 Robot Framework，用</p><pre><code>ride</code></pre><p>来启动 RIDE。示意图如下，</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201604/RF_Ride.png" alt="测试工程目录树"></p><p>Selenium Library 通常会包含一个对应的 Selenium Server JAR 包。为了独立使用不同目录下的 Selenium Server（比如其他小组成员安装的），你需要指定一个新的环境变量 RF_HOME，该变量指向 Python 的安装目录。该变量用于 selenium 服务器的本地启动脚本。</p><p>对于本地的 MySQL 数据库，其配置<a href="https://github.com/ThomasJaspers/robotframework-dblibrary/tree/master/sample" target="_blank" rel="noopener">在此</a>。然后安装 MySQL，创建测试 schema 和相应的用户：</p><pre><code>C:\xampp\mysql\bin&gt;mysql -u root -pmysql&gt; create database databaselibrarydemo;mysql&gt; create user ‘dblib’@’localhost’ identified by ‘dblib’;mysql&gt; grant all privileges on databaselibrarydemo.* to ‘dblib’;</code></pre><p>这里是工程的源码 <a href="https://github.com/lyyyuna/Robot-Framework-Sample-Project" target="_blank" rel="noopener">GitHub</a>。</p><p>在 robot/execution/local/scripts 是执行测试前所有需要运行的脚本。测试的实现在 robot/implementation/testsuites 目录中。测试用例可以直接用 RIDE 打开 implementation 目录，然后直接查看和修改。</p><p>为了运行测试，必须先启动 Selenium 服务器和 DBLibrary 服务器。然后运行 Testsuite。Windows 的批处理脚本在 robot\execution\local\scripts 目录中。因为都使用相对路径，一切应该按计划顺利运行。这里虽然在被测服务器上部署文件，但本地可以很容易地适配。</p><h2 id="结论和感想"><a href="#结论和感想" class="headerlink" title="结论和感想"></a>结论和感想</h2><p>我们已经看到，Robot Framework 提供了众多功能和可能，即使同一件事也能用不同方法来完成。所以，在正式开始测试前做些基本分析是很有意义的。</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201604/impression_ride1.png" alt="在 RIDE 中编辑 Testsuites 和 Resource 文件"></p><p>使用 RIDE 使得实现测试功能更简单，尤其是那些非技术团队。简单意味着好维护（不只是 Robot Framework 测试哦 ;-)）。</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201604/impression_seleniumserver.png" alt="Selenium Server 启动和运行"></p><p>顺便说一下，我还没有明确指出过，Robot Framework 的 <strong>报表</strong> 和 logging 非常棒，在 troubleshooting 时非常有用。</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201604/impression_logfile.png" alt="Robot Framework 的 log 文件，其中含有浏览器屏幕截图"></p><p>Robot Framework 在各种不同的测试库中提供大量的测试功能。一旦决定哪个测试库最好用时，大大加速了写测试的过程，提高了生产力。</p><p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201604/impression_dbserver.png" alt="Database Library Server 运行"></p><p>尤其是在许多不同工程工作时，一个通用的工程结构和工具非常有用。在一些公共的资源文件中也需要实现一些产品相关的关键字。</p><p>希望本文有助于你开始使用 Robot Framework，并有效地组织你的测试工程结构。当然，本例还有许多增强的地方，希望这是一个良好的起点。</p><h2 id="Robot-Framework-教程目录"><a href="#Robot-Framework-教程目录" class="headerlink" title="Robot Framework 教程目录"></a>Robot Framework 教程目录</h2><p><a href="https://blog.codecentric.de/en/2012/04/robot-framework-tutorial-a-complete-example/" target="_blank" rel="noopener">原文链接</a></p><ol><li><a href="http://www.lyyyuna.com/2016/01/07/robotframework-tutorial-overview/">Robot Framework 教程 - 概览（译）</a></li><li><a href="http://www.lyyyuna.com/2016/04/09/robotframework-tutorial-a-complete-example/">Robot Framework 教程 - 一个完整的例子（译）</a></li><li><a href="http://www.lyyyuna.com/2016/04/30/robotframework-ide-ride-overview/">Robot Framework 教程 - 集成开发环境 RIDE 概览 (译)</a></li><li><a href="http://www.lyyyuna.com/2016/05/15/robotframework-tutorial-how-to-structure-a-scalable-and-maintainable-acceptance-test-suite/">Robot Framework 教程 - 如何组织一个可伸缩可维护的验收测试套件（译）</a></li><li><a href="http://www.lyyyuna.com/2016/05/28/robotframework-tutorial-loops-conditional-execution-and-more/">Robot Framework 教程 - 循环，条件判断，字符串和列表（译）</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;用 &lt;a href=&quot;http://code.google.com/p/robotframework/&quot; target=&quot;_blank&quot; r
      
    
    </summary>
    
      <category term="自动化测试" scheme="http://www.lyyyuna.com/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/"/>
    
    
      <category term="robot framework" scheme="http://www.lyyyuna.com/tags/robot-framework/"/>
    
  </entry>
  
  <entry>
    <title>用 asyncio 封装文件读写</title>
    <link href="http://www.lyyyuna.com/2016/03/27/asyncfileread01/"/>
    <id>http://www.lyyyuna.com/2016/03/27/asyncfileread01/</id>
    <published>2016-03-27T09:36:25.000Z</published>
    <updated>2017-11-29T00:19:00.161Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>和网络 IO 一样，文件读写同样是一个费事的操作。</p><p>默认情况下，Python 使用的是系统的阻塞读写。这意味着在 asyncio 中如果调用了 </p><pre><code>f = file(&apos;xx&apos;)f.read()</code></pre><p>会阻塞事件循环。</p><p>本篇简述如何用 asyncio.Future 对象来封装文件的异步读写。</p><p>代码在 <a href="https://github.com/lyyyuna/script_collection/blob/master/aysncfile/asyncfile.py" target="_blank" rel="noopener">GitHub</a>。目前仅支持 Linux。</p><h2 id="阻塞和非阻塞"><a href="#阻塞和非阻塞" class="headerlink" title="阻塞和非阻塞"></a>阻塞和非阻塞</h2><p>首先需要将文件的读写改为非阻塞的形式。在非阻塞情况下，每次调用 read 都会立即返回，如果返回值为空，则意味着文件操作还未完成，反之则是读取的文件内容。</p><p>阻塞和非阻塞的切换与操作系统有关，所以本篇暂时只写了 Linux 版本。如果有过 Unix 系统编程经验，会发现 Python 的操作是类似的。</p><pre><code>flag = fcntl.fcntl(self.fd, fcntl.F_GETFL) if fcntl.fcntl(self.fd, fcntl.F_SETFL, flag | os.O_NONBLOCK) != 0:     raise OSError() </code></pre><h2 id="Future-对象"><a href="#Future-对象" class="headerlink" title="Future 对象"></a>Future 对象</h2><p>Future 对象类似 Javascript 中的 Promise 对象。它是一个占位符，其值会在将来被计算出来。我们可以使用</p><pre><code>result = await future</code></pre><p>在 future 得到值之后返回。而使用</p><pre><code>future.set_result(xxx)</code></pre><p>就可以设置 future 的值，也意味着 future 可以被返回了。await 操作符会自动调用 future.result() 来得到值。</p><h2 id="loop-call-soon"><a href="#loop-call-soon" class="headerlink" title="loop.call_soon"></a>loop.call_soon</h2><p>通过 loop.call_soon 方法可以将一个函数插入到事件循环中。</p><p>至此，我们的异步文件读写思路也就出来了。通过 loop.call_soon 调用非阻塞读写文件的函数。若一次文件读写没有完成，则计算剩余所学读写的字节数，并再次插入事件循环直至读写完毕。</p><p>可以发现其就是把传统 Unix 编程里，非阻塞文件读写的 while 循环换成了 asyncio 的事件循环。</p><p>下面是这一过程的示意代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_step</span><span class="params">(self, future, n, total)</span>:</span></span><br><span class="line">    res = self.fd.read(n)</span><br><span class="line">    <span class="keyword">if</span> res <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        self.loop.call_soon(self.read_step, future, n, total)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> res: <span class="comment"># EOF</span></span><br><span class="line">        future.set_result(bytes(self.rbuffer))</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    self.rbuffer.extend(res)</span><br><span class="line">    self.loop.call_soon(self.read_step, future, self.BLOCK_SIZE, total)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read</span><span class="params">(self, n=<span class="number">-1</span>)</span>:</span></span><br><span class="line">    future = asyncio.Future(loop=self.loop)</span><br><span class="line"></span><br><span class="line">    self.rbuffer.clear()</span><br><span class="line">    self.loop.call_soon(self.read_step, future, min(self.BLOCK_SIZE, n), n)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> future</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;和网络 IO 一样，文件读写同样是一个费事的操作。&lt;/p&gt;
&lt;p&gt;默认情况下，Python 使用的是系统的阻塞读写。这意味着在 asynci
      
    
    </summary>
    
      <category term="杂" scheme="http://www.lyyyuna.com/categories/%E6%9D%82/"/>
    
    
      <category term="Python" scheme="http://www.lyyyuna.com/tags/Python/"/>
    
      <category term="asyncio" scheme="http://www.lyyyuna.com/tags/asyncio/"/>
    
  </entry>
  
  <entry>
    <title>DHT 协议 - 译</title>
    <link href="http://www.lyyyuna.com/2016/03/26/dht01/"/>
    <id>http://www.lyyyuna.com/2016/03/26/dht01/</id>
    <published>2016-03-26T12:02:00.000Z</published>
    <updated>2017-11-29T00:19:00.162Z</updated>
    
    <content type="html"><![CDATA[<p>BitTorrent 使用“分布式哈希表” (DHT) 来存储种子的 peer 信息，且不需要专门的服务器。这样，每一个 peer 都变成一个 tracker。DHT 协议基于 Kademila，且用 UDP 实现。</p><p>请注意文档中的术语以免产生误解。peer 实现了 BitTorrent 协议，既是服务器也是客户端，且监听在 TCP 端口。node 实现了 DHT 协议，同样既是服务器也是客户端，监听在 UDP 端口。DHT 由 node 和 peer 的位置信息构成。BitTorrent 客户端有一个 DHT node，用来和 DHT 中的其他 node 通信用，从而获取 peer 的位置。</p><h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>每一个 node 都有一个全球唯一标识符，称作 node ID。Node ID 是从 160-bit 的空间中随机选取的，这和 BitTorrent 的<br>infohashes 一致。距离参数（Distance metric）用来比较两个 node ID 之间的接近程度。Nodes 必须维护一个包含其他 node 联系信息的路由表。对那些和自身接近的其他 Node ID，路由表中会有跟多细节。所以 DHT 中，接近自己的 ID 有很多，远的 ID 则较少。</p><p>在 Kademila 中，距离参数使用异或计算，结果为无符号整形，即</p><pre><code>distance(A, B) = | A XOR B |</code></pre><p>值越小越接近。</p><p>当一个 node 想要获取种子的其他 peer 时，它先计算种子的 infohash 和路由表中已有 nodes 的距离参数。然后向距离种子最近的 node 询问 peer 信息。如果恰巧该 node 有，则返回给询问的 node。否则，该 node 把自己路由表中离种子最近的 node 信息返回。本 node 拿到返回信息之后，不停重复上述步骤直到没有新 node 信息返回。搜索结束时，客户端将<strong>自己</strong>的 peer 信息返回给离种子最近的 node。 </p><p>peer 查询的返回值必须包含令牌(token)。如果一个 node 宣称自己拥有的 peer 正在下载种子时，必须使用最近接收到的 token。当本 node 试图下载种子时，被请求的 node 检查 IP 地址对应的 token 是否一致。这么做是为了防止恶意主机从其他主机下载种子。由于 token 仅仅由请求 node 返回给被请求 node，故协议并未规定其实现。token 在分发后一段时间内必须被接受。BitTorrent 使用 IP + secret 的 SHA1 值作为 token，每 5 分钟改变一次，有 10 分钟有效期。 </p><h2 id="路由表"><a href="#路由表" class="headerlink" title="路由表"></a>路由表</h2><p>每一个 node 维护了一个好 node 的路由表。路由表的中 node 被用作查询的起始节点，并返回查询的响应信息。</p><p>并非所有 node 是等价的，有些是“好”的，有些则不。大部分 node 能够使用 DHT 协议发送查询和接受响应，但却不能响应其他 node 的查询。所以在路由表中，必须只包含好的 node。一个好的 node 被定义为能在 15 分钟内响应查询，或者 15 分钟内曾经响应过。若 15 钟没响应，则是可疑的。若对一系列请求都失去响应，则为坏的 node。已知的好的 node 拥有更高的优先级。</p><p>路由表覆盖从 0 到 2^160 完整的 node ID 空间。路由表又被分为 bucket，每一个拥有一部分子空间。空的表只有一个 bucket，其 ID 范围为 min=0 到 max=2^160。当 ID 为 N 的 node 插入到表中时，它必须放在 min=0 到 max=2^160 之间。由于空的表只有一个 bucket，所有 node 都在该 bucket 中。每个 bucket 可以存放 K 个 node，目前 K &lt;= 8。当 bucket 存满好的 node 时，不允许再插入，除非其本身就在该 bucket 中。在这种情况下，原 bucket 分裂为两个相同大小的 bucket，比如原始 bucket 分裂为 0 到 2^159，2^159 到 2^160。</p><p>当 bucket 装满了好的 node，新的 node 会被丢弃。一旦 bucket 中的某个 node 变坏，就会用新的  node 来替换这个坏的 node。如果 bucket 中有在 15 分钟内都没有活跃过的 bucket，则其为可疑的 node，这时我们向最久没有联系的 node 发送 ping。如果有回复，那么我们向下一个可疑的 node 发送 ping，不断这样循环下去，直到有某一个 node 没有给出 ping 的回复，或者当前 bucket 中的所有 node 都是好的 (即都不是可疑节点，且在过去 15 分钟内都有活动)。如果 bucket 中的某个 node 没有对我们的 ping 给出回复，还会再试一次 (再发送一次 ping，也许仍然是活跃的，但由于网络拥塞，所以发生了丢包现象，注意 DHT 的包都是 UDP 的)，而不是立即丢弃该 node 或者直接用新 node 来替代它。这样，路由表中永远是活跃的 node。</p><p>每个 bucket 都应该维护一个 lastchange 字段来表明自身的”新鲜”程度。当 bucket 中的 node 被 ping 并给出了回复，或者一个 node 被加入到了 bucket，或者有新的 node 替代了旧的，bucket 的 lastchange 字段都应当更新。如果一个 bucket 的 lastchange 在过去的 15 分钟内都没有变化，那么将重更新它。这个重更新为：从这个 bucket 所覆盖的范围中随机选择一个 node ID，并对这个 ID 执行 find_nodes 搜索。请注意，收到请求的 node 通常不需要常常更新自己所在的 bucket。反之，不常常收到请求的 node 才需要周期性地更新所在 bucket。有了上述措施，在 DHT 时，才能有足够多的好的 node。</p><p>当插入第一个 node 到路由表并启动时，这个 node 应试着查找 DHT 中离自己最近的 node。这个查找工作是通过不断的发出 find_node 消息给越来越近的 node 来完成的，当没有更近的 node 时，搜索停止。路由表应当由客户端软件保存。</p><h2 id="BitTorrent-协议扩展"><a href="#BitTorrent-协议扩展" class="headerlink" title="BitTorrent 协议扩展"></a>BitTorrent 协议扩展</h2><p>BitTorrent 协议被扩展为可通过 tracker 得到的 peer 之间互换 node 的 UDP 端口号。在这种方式下，当下载种子时，客户端能够自动更新路由表。新安装的客户端在无 tracker 的种子中获取不到路由表，必须在种子文件中找到联系信息。</p><p>Peers 如果支持 DHT 协议，会将 BitTorrent 协议握手消息的 8 字节保留位的最后一位置为 1。如果 peer 收到一个握手消息，表明对方支持 DHT 协议，则应该发送 PORT 消息。它由字节 0x09 开始，payload 的长度是 2 个字节，包含了这个 peer 的 DHT 所使用的 UDP 端口号。当 peer 收到这样的消息是应当向对方的 IP 和消息中指定的端口号的节点发送 ping。如果收到了 ping 的回复，则将新 node 的联系信息加入到路由表中。</p><h2 id="种子文件扩展"><a href="#种子文件扩展" class="headerlink" title="种子文件扩展"></a>种子文件扩展</h2><p>无 tracker 的种子文件中不含有 announce 关键字，相反，其包含有 nodes 关键字。该关键字应该包含种子创建者路由表的 K 个最近 node。也可以选择设置成已知的可用 node，比如这个种子的创建者。请不要加入 router.bittorrent.com。</p><pre><code>nodes = [[&quot;&lt;host&gt;&quot;, &lt;port&gt;], [&quot;&lt;host&gt;&quot;, &lt;port&gt;], ...]nodes = [[&quot;127.0.0.1&quot;, 6881], [&quot;your.router.node&quot;, 4804]]</code></pre><h2 id="KRPC-协议"><a href="#KRPC-协议" class="headerlink" title="KRPC 协议"></a>KRPC 协议</h2><p>KRPC 协议是一个简单的 RPC 通信框架，其在 UDP 上使用 bencoded 编码的字典，包含请求与回复，但没有重试。有三种消息类型：query, response, error。对于 DHT 协议来说，有 4 种 query: ping, find_node, get_peers, announce_peer。</p><p>KRPC 消息是一个简单字典，包含两个必填关键字，附加的关键字取决于消息类型。第一个必填关键字是 t，这是一个字符串表示的 transaction ID。它由请求 node 产生，且包含在回复中，所以回复有可能对应单个 node 的多个请求。transaction ID 应该被编码成字符串表示的二进制数字，通常是两个字符，这样就能包含 2^16 种请求。另一个必填关键字是 y，其对应值表示消息类型，为 q, r 或 e。</p><h2 id="联系信息编码"><a href="#联系信息编码" class="headerlink" title="联系信息编码"></a>联系信息编码</h2><p>peer 的联系信息被编码成 6 字节的字符串。4 字节为 IP 地址，2 字节为端口号，均用网络字节序表示。</p><p>node 的联系信息被编码成 26 字节的字符串。20 字节为 node ID，剩余为 IP 和端口号信息，均用网络字节序表示。</p><h2 id="Queries"><a href="#Queries" class="headerlink" title="Queries"></a>Queries</h2><p>query 为键值对 y:q，含有两个附加关键字，q 和 a。关键字 q 的值包含了请求类型的字符串表示。关键字 a 的值包含了一个所有返回值的字典。</p><h2 id="Responses"><a href="#Responses" class="headerlink" title="Responses"></a>Responses</h2><p>response 为键值对 y:r，含有一个附加关键字 r。关键字 r 的值包含了一个所有返回值的字典。</p><h2 id="Errors"><a href="#Errors" class="headerlink" title="Errors"></a>Errors</h2><p>error 为键值对 y:e，含有一个附加关键字 e。e 为一个列表。第一个元素是整形表示的错误码。第二个元素是字符串表示的错误信息。以下为可能的错误码，</p><pre><code>Code        Description =======================201         Generic Error 202         Server Error 203         Protocol Error, such as a malformed             packet, invalid arguments, or bad             token 204         Method Unknown </code></pre><p>示例，</p><pre><code>generic error = {&quot;t&quot;:&quot;aa&quot;, &quot;y&quot;:&quot;e&quot;, &quot;e&quot;:[201, &quot;A Generic Error Ocurred&quot;]}bencoded = d1:eli201e23:A Generic Error Ocurrede1:t2:aa1:y1:ee</code></pre><h2 id="DHT-Queries"><a href="#DHT-Queries" class="headerlink" title="DHT Queries"></a>DHT Queries</h2><p>所有的 query 都有一个键值对 ‘id:请求 node ID’。所有的 response 也有一个键值对 ‘id:响应的 node ID’。</p><h3 id="ping"><a href="#ping" class="headerlink" title="ping"></a>ping</h3><p>最基本的 query 是 ping。这时候 q=ping，id 为 20 字节网络字节序表示的发送者 node ID。该 query 的响应为 id=响应者 node ID。</p><pre><code>arguments:  {&quot;id&quot; : &quot;&lt;querying nodes id&gt;&quot;}response: {&quot;id&quot; : &quot;&lt;queried nodes id&gt;&quot;}</code></pre><p>示例，</p><pre><code>ping Query = {&quot;t&quot;:&quot;aa&quot;, &quot;y&quot;:&quot;q&quot;, &quot;q&quot;:&quot;ping&quot;, &quot;a&quot;:{&quot;id&quot;:&quot;abcdefghij0123456789&quot;}}bencoded = d1:ad2:id20:abcdefghij0123456789e1:q4:ping1:t2:aa1:y1:qeResponse = {&quot;t&quot;:&quot;aa&quot;, &quot;y&quot;:&quot;r&quot;, &quot;r&quot;: {&quot;id&quot;:&quot;mnopqrstuvwxyz123456&quot;}}bencoded = d1:rd2:id20:mnopqrstuvwxyz123456e1:t2:aa1:y1:re</code></pre><h3 id="find-node"><a href="#find-node" class="headerlink" title="find_node"></a>find_node</h3><p>find_node 被用来查找给定 ID 的 node 的联系信息。这时 q == find_node。find_node 请求包含 2 个参数，第一个参数是 id，包含了请求 node ID。第二个参数是 target，包含了请求者正在查找的 node ID。当一个 node 接收到了 find_node 的 query，他应该给出对应的回复，回复中包含 2 个关键字 id 和 nodes，nodes 是字符串类型，包含了被请求 node 的路由表中最接近目标 node 的 K(8) 个最接近的 node 的联系信息。</p><pre><code>arguments:  {&quot;id&quot; : &quot;&lt;querying nodes id&gt;&quot;, &quot;target&quot; : &quot;&lt;id of target node&gt;&quot;}response: {&quot;id&quot; : &quot;&lt;queried nodes id&gt;&quot;, &quot;nodes&quot; : &quot;&lt;compact node info&gt;&quot;}</code></pre><p>示例，</p><pre><code>find_node Query = {&quot;t&quot;:&quot;aa&quot;, &quot;y&quot;:&quot;q&quot;, &quot;q&quot;:&quot;find_node&quot;, &quot;a&quot;: {&quot;id&quot;:&quot;abcdefghij0123456789&quot;, &quot;target&quot;:&quot;mnopqrstuvwxyz123456&quot;}}bencoded = d1:ad2:id20:abcdefghij01234567896:target20:mnopqrstuvwxyz123456e1:q9:find_node1:t2:aa1:y1:qeResponse = {&quot;t&quot;:&quot;aa&quot;, &quot;y&quot;:&quot;r&quot;, &quot;r&quot;: {&quot;id&quot;:&quot;0123456789abcdefghij&quot;, &quot;nodes&quot;: &quot;def456...&quot;}}bencoded = d1:rd2:id20:0123456789abcdefghij5:nodes9:def456...e1:t2:aa1:y1:re</code></pre><h3 id="get-peers"><a href="#get-peers" class="headerlink" title="get_peers"></a>get_peers</h3><p>get_peers 与种子文件的 infohash 有关。这时 q=get_peers。get_peers 请求包含 2 个参数。第一个参数是 id，包含了请求 node 的 ID。第二个参数是 info_hash，它代表种子文件的 infohash。如果被请求的 node 有对应 info_hash 的 peers，他将返回一个关键字 values，这是一个列表类型的字符串。每一个字符串包含了 CompactIP-address/portinfo 格式的 peers 信息。如果被请求的 node 没有这个 infohash 的 peers，那么他将返回关键字 nodes，这个关键字包含了被请求 node 的路由表中离 info_hash 最近的 K 个 node，使用 Compactnodeinfo 格式回复。在这两种情况下，关键字 token 都将被返回。之后的 annouce_peer 请求中必须包含 token。token 是一个短的二进制字符串。</p><pre><code>arguments:  {&quot;id&quot; : &quot;&lt;querying nodes id&gt;&quot;, &quot;info_hash&quot; : &quot;&lt;20-byte infohash of target torrent&gt;&quot;}response: {&quot;id&quot; : &quot;&lt;queried nodes id&gt;&quot;, &quot;token&quot; :&quot;&lt;opaque write token&gt;&quot;, &quot;values&quot; : [&quot;&lt;peer 1 info string&gt;&quot;, &quot;&lt;peer 2 info string&gt;&quot;]}or: {&quot;id&quot; : &quot;&lt;queried nodes id&gt;&quot;, &quot;token&quot; :&quot;&lt;opaque write token&gt;&quot;, &quot;nodes&quot; : &quot;&lt;compact node info&gt;&quot;}</code></pre><p>示例，</p><pre><code>get_peers Query = {&quot;t&quot;:&quot;aa&quot;, &quot;y&quot;:&quot;q&quot;, &quot;q&quot;:&quot;get_peers&quot;, &quot;a&quot;: {&quot;id&quot;:&quot;abcdefghij0123456789&quot;, &quot;info_hash&quot;:&quot;mnopqrstuvwxyz123456&quot;}}bencoded = d1:ad2:id20:abcdefghij01234567899:info_hash20:mnopqrstuvwxyz123456e1:q9:get_peers1:t2:aa1:y1:qeResponse with peers = {&quot;t&quot;:&quot;aa&quot;, &quot;y&quot;:&quot;r&quot;, &quot;r&quot;: {&quot;id&quot;:&quot;abcdefghij0123456789&quot;, &quot;token&quot;:&quot;aoeusnth&quot;, &quot;values&quot;: [&quot;axje.u&quot;, &quot;idhtnm&quot;]}}bencoded = d1:rd2:id20:abcdefghij01234567895:token8:aoeusnth6:valuesl6:axje.u6:idhtnmee1:t2:aa1:y1:reResponse with closest nodes = {&quot;t&quot;:&quot;aa&quot;, &quot;y&quot;:&quot;r&quot;, &quot;r&quot;: {&quot;id&quot;:&quot;abcdefghij0123456789&quot;, &quot;token&quot;:&quot;aoeusnth&quot;, &quot;nodes&quot;: &quot;def456...&quot;}}bencoded = d1:rd2:id20:abcdefghij01234567895:nodes9:def456...5:token8:aoeusnthe1:t2:aa1:y1:re</code></pre><h3 id="announce-peer"><a href="#announce-peer" class="headerlink" title="announce_peer"></a>announce_peer</h3><p>announce_peer 表示某个端口正在下载种子文件。announce_peer 包含 4 个参数。第一个参数是 id，包含了请求 node ID；第二个参数是 info_hash，包含了种子文件的 infohash；第三个参数是 port 包含了整型的端口号，表明 peer 在哪个端口下载；第四个参数是 token，这是在之前的 get_peers 请求中收到的回复中所包含的。收到 announce_peer 请求的 node 必须检查这个 token 与回复的 token 是否相同。如果相同，那么被请求的 node 将记录发送者的 IP 和端口号，记录在 peer 联系信息中对应的 infohash 下。</p><pre><code>arguments:  {&quot;id&quot; : &quot;&lt;querying nodes id&gt;&quot;,&quot;implied_port&quot;: &lt;0 or 1&gt;,&quot;info_hash&quot; : &quot;&lt;20-byte infohash of target torrent&gt;&quot;,&quot;port&quot; : &lt;port number&gt;,&quot;token&quot; : &quot;&lt;opaque token&gt;&quot;}response: {&quot;id&quot; : &quot;&lt;queried nodes id&gt;&quot;}</code></pre><p>示例，</p><pre><code>announce_peers Query = {&quot;t&quot;:&quot;aa&quot;, &quot;y&quot;:&quot;q&quot;, &quot;q&quot;:&quot;announce_peer&quot;, &quot;a&quot;: {&quot;id&quot;:&quot;abcdefghij0123456789&quot;, &quot;implied_port&quot;: 1, &quot;info_hash&quot;:&quot;mnopqrstuvwxyz123456&quot;, &quot;port&quot;: 6881, &quot;token&quot;: &quot;aoeusnth&quot;}}bencoded = d1:ad2:id20:abcdefghij01234567899:info_hash20:&lt;br /&gt;mnopqrstuvwxyz1234564:porti6881e5:token8:aoeusnthe1:q13:announce_peer1:t2:aa1:y1:qeResponse = {&quot;t&quot;:&quot;aa&quot;, &quot;y&quot;:&quot;r&quot;, &quot;r&quot;: {&quot;id&quot;:&quot;mnopqrstuvwxyz123456&quot;}}bencoded = d1:rd2:id20:mnopqrstuvwxyz123456e1:t2:aa1:y1:re</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;BitTorrent 使用“分布式哈希表” (DHT) 来存储种子的 peer 信息，且不需要专门的服务器。这样，每一个 peer 都变成一个 tracker。DHT 协议基于 Kademila，且用 UDP 实现。&lt;/p&gt;
&lt;p&gt;请注意文档中的术语以免产生误解。peer 
      
    
    </summary>
    
      <category term="网络" scheme="http://www.lyyyuna.com/categories/%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="DHT" scheme="http://www.lyyyuna.com/tags/DHT/"/>
    
  </entry>
  
</feed>
