
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  
    <title>基于 BP 神经网络的手写体数字识别 - 设计与实现 | lyyyuna 的小花园</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="lyyyuna">
    

    
    <meta name="description" content="MathJax.Hub.Config({   TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } } });    手写体数字识别的神经网络结构上一篇文章中我们简单介绍了神经网络，接下来让我们运用到主题中 —— 手写体数字识别。 手写数字识别可以分为两大子问题">
<meta name="keywords" content="Python,mnist,neural network">
<meta property="og:type" content="article">
<meta property="og:title" content="基于 BP 神经网络的手写体数字识别 - 设计与实现">
<meta property="og:url" content="http://www.lyyyuna.com/2016/06/25/handwritten-neural-net02/index.html">
<meta property="og:site_name" content="lyyyuna 的小花园">
<meta property="og:description" content="MathJax.Hub.Config({   TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } } });    手写体数字识别的神经网络结构上一篇文章中我们简单介绍了神经网络，接下来让我们运用到主题中 —— 手写体数字识别。 手写数字识别可以分为两大子问题。第一是如何将一串数字分割成单个的数字，第二是识别单个数字。我们将专注于解决第二个问题，因为分割问">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/three-layers.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/some-samples.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/func_plot.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/gradient.png">
<meta property="og:updated_time" content="2017-11-29T00:19:00.163Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="基于 BP 神经网络的手写体数字识别 - 设计与实现">
<meta name="twitter:description" content="MathJax.Hub.Config({   TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } } });    手写体数字识别的神经网络结构上一篇文章中我们简单介绍了神经网络，接下来让我们运用到主题中 —— 手写体数字识别。 手写数字识别可以分为两大子问题。第一是如何将一串数字分割成单个的数字，第二是识别单个数字。我们将专注于解决第二个问题，因为分割问">
<meta name="twitter:image" content="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/three-layers.png">

    
    <link rel="alternative" href="/atom.xml" title="lyyyuna 的小花园" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css">
</head>

  <body>
    <header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="lyyyuna 的小花园" title="lyyyuna 的小花园"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="lyyyuna 的小花园">lyyyuna 的小花园</a></h1>
				<h2 class="blog-motto">动静中之动</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
						<li><a href="/categories">Categories</a></li>
					
					<li>
 					
					<form class="search" action="/search/index.html" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" autocomplete="off" name="q" maxlength="20" placeholder="搜索" />
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2016/06/25/handwritten-neural-net02/" title="基于 BP 神经网络的手写体数字识别 - 设计与实现" itemprop="url">基于 BP 神经网络的手写体数字识别 - 设计与实现</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="lyyyuna" target="_blank" itemprop="author">lyyyuna</a>
		
  <p class="article-time">
    <time datetime="2016-06-25T09:56:33.000Z" itemprop="datePublished"> 发表于 2016-06-25</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">文章目录</strong>
		
			<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#手写体数字识别的神经网络结构"><span class="toc-number">1.</span> <span class="toc-text">手写体数字识别的神经网络结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#基于梯度下降法学习"><span class="toc-number">2.</span> <span class="toc-text">基于梯度下降法学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#代码实现"><span class="toc-number">3.</span> <span class="toc-text">代码实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络识别手写数字目录"><span class="toc-number">4.</span> <span class="toc-text">神经网络识别手写数字目录</span></a></li></ol>
		
		</div>
		
		<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>


<h2 id="手写体数字识别的神经网络结构"><a href="#手写体数字识别的神经网络结构" class="headerlink" title="手写体数字识别的神经网络结构"></a>手写体数字识别的神经网络结构</h2><p><a href="http://www.lyyyuna.com/2016/05/29/handwritten-neural-net/">上一篇文章</a>中我们简单介绍了神经网络，接下来让我们运用到主题中 —— 手写体数字识别。</p>
<p>手写数字识别可以分为两大子问题。第一是如何将一串数字分割成单个的数字，第二是识别单个数字。我们将专注于解决第二个问题，因为分割问题并不难，而且和我们的主题——神经网络相差甚远。</p>
<p>为了识别单个的数字，我们将使用三层神经网络：</p>
<p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/three-layers.png" alt="三层神经网络"></p>
<p>输入层的神经元将对输入的像素值编码。神经网络的训练数据包含了 $28 \times 28$ 像素的扫描图，所以输入层有 $784 = 28 \times 28$ 个神经元。为了简便起见，上图中没有 $784$ 个输入神经元全部画出。每一个像素是灰度值，$0.0$ 代表白色，$1.0$ 代表黑色，两者之间的为逐渐变黑的灰色。</p>
<p>第二层是隐藏层。让我们记隐藏层的神经元数量为 $n$，我们将对 $n$ 的不同值实验。上图中，$n=15$。</p>
<p>输出层包含 $10$ 个神经元。如果第一个输出被激活了，即 $\mbox{output} \approx 1$，意味着神经网络判定当前数字为 $0$。如果第二个输出神经元被激活，则神经网络认为当前数字很有可能为 $1$。以此类推。最后对 $10$ 个输出神经元进行排序，哪个最高说明神经网络认为其最有可能。比如第 $6$ 个神经元输出最高，则输入图像最有可能是数字 $6$。</p>
<p>你可能会想为什么会用十个输出神经元？毕竟我们的目标是判断出每一个图片对应的数字，理论上，四个输出的组合就可以编码，$2^4 = 16$ 完全可以包含 $10$ 个值。要回答这个问题还是得靠实验：我们对两种输出编码方案都做了实验，用 $10$ 个输出的效果更好。让我们做一个启发式的思考，用四个 bit 来唯一确定一个数字，意味着得百分百识别出图像，说一不二。而有些时候字确实写的难以辨认，连人类自己都只能说‘大概是 4 或者 9’这种话，这时候，给出 $10$ 个数字的概率更符合人脑的思维方式。</p>
<h2 id="基于梯度下降法学习"><a href="#基于梯度下降法学习" class="headerlink" title="基于梯度下降法学习"></a>基于梯度下降法学习</h2><p>现在我们已经设计出神经网络的结构了，那如何识别出数字呢？第一件事是找到训练集。我们将使用 <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">MNIST 数据集</a>。MNIST 是经过修改的 <a href="http://en.wikipedia.org/wiki/National_Institute_of_Standards_and_Technology" target="_blank" rel="noopener">NIST</a> 数据子集，NIST 即 United States’ National Institute of Standards and Technology。以下是部分数据：</p>
<p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/some-samples.png" alt="部分数据"></p>
<p>图像已经被分割成单个的数字，而且没有噪点等，不需要做图像预处理。MNIST 数据集包括两部分，第一部分包含 60000 个图像，被用作训练数据。这些图像来自 250 个人，半数来自美国人口普查局的员工，另外半数是高中生。图片是 $28 \times 28$ 的灰度图。第二部分是 10000 张图片的测试集。我们将使用测试集来评估人工神经网络的学习效果。为了取得更好的测试效果，测试集来自另外 250 个人。这样，测试集和训练集的完全不同能够更好验证结果。</p>
<p>我们将使用 $x$ 来标记训练输入。虽然图片是一个二维数组，不过我们输入会采用 $28 \times 28 = 784$ 的向量。向量中的每一项为图片每一个像素的灰度值。输出我们将记为输出向量 $y = y(x)$。如果一个训练图片是数字 6，则 $y(x) = (0, 0, 0, 0, 0, 0, 1, 0, 0, 0)^\mathrm{T}$。请注意，$\mathrm{T}$ 是转置操作。</p>
<p>算法的目标是找到权重和偏移，对于所有训练输入 $x$，网络都能够输出正确的 $y(x)$。为了量化我们与目标的接近程度，定义以下的代价函数：</p>
<p>\begin{eqnarray}  C(w,b) \equiv<br>  \frac{1}{2n} \sum_x | y(x) - a|^2.  \label{6}<br>\end{eqnarray}</p>
<p>这里，记 $w$ 为网络所有的权重集，$b$ 为所有的偏移，$n$ 为训练输入的总数，$a$ 为实际的输出向量，求和是对所有训练输入而言。当然，$a$ 应该是 $x,w,b$ 的函数。记号 $| v |$ 为向量 $v$ 的长度。我们称 $C$ 为二次代价函数，其实就是均方差啦， MSE (mean squared error)。可以看出，该代价函数为非负值，并且，当所有训练数据的 $y(x)$ 接近实际输出 $a$ 时，代价函数 $C(w,b) \approx 0$。所以，当神经网络工作良好的时候，$C(w,b)$ 很小，相反，该值会很大，即训练集中有很大一部分预期输出 $y(x)$ 和实际输出 $a$ 不符。我们训练算法的目标，就是找到一组权重和偏移，让误差尽可能的小。</p>
<p>为什么会用二次函数误差？毕竟我们感兴趣的是能被正确分类的图片数量。为什么不直接以图片数量为目标，比如目标是识别的数量最大化？问题是，正确分类图像的数量不是权值和偏置的光滑函数，大部分情况下，变量的变化不会导致识别数量的变化，也就难以调整权重和偏移。</p>
<p>你可能还会好奇，二次函数是最好的选择吗？其他的代价函数会不会得到完全不同的权值和偏移，效果更好呢？事实上，确实有更好的代价函数，以后的文章还会继续探讨。不过，本文继续使用二次函数，它对我们理解神经网络的基本自学习过程非常有益。</p>
<p>接下来，我们介绍梯度下降法，它可以用来解决最小化问题。</p>
<p>假设我们要最小化函数 $C(v), v=v_1,v_2$。该函数图像为：</p>
<p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/func_plot.png" alt="函数图像"></p>
<p>我们要找到 $C$ 的全局最小值。一种做法是计算函数的导数，找到各个极值。课本上的导数很好求解。不幸的是，现实生活中，问题所代表的函数经常包含过多的变量。尤其是神经网络中，有数以万计的权值和偏移，不可能直接求取极值。</p>
<p>幸好还有其他方法。让我们换种思维方式，对比那个函数图像，将函数看作一个山谷，并假设有一个小球沿着山谷斜坡滑动。直觉告诉我们小球最终会滑向坡底。也许这就能用来找到最小值？我们为“小球”随机选择一个起点，然后模拟“小球”沿斜坡滑动。“小球”运动的方向可以通过偏导数确定，这些偏导数包含了山谷形状的信息。</p>
<p>是否需要牛顿力学公式来获取小球的运动，考虑摩擦力和重力呢？并不需要，我们是在制定一个最小化 $C$ 的算法，而不是去精确模拟物理学规律。</p>
<p>让我们记球在 $v_1$ 方向移动了很小的量 $\Delta v_1$，在 $v_2$ 的方向移动了 $\Delta v_2$，总的 $C$ 的改变量为：</p>
<p>\begin{eqnarray}<br>  \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 +<br>  \frac{\partial C}{\partial v_2} \Delta v_2.<br>\label{7}<br>\end{eqnarray}</p>
<p>我们目标是找到一组 $\Delta v_1$ 和 $\Delta v_2$，使 $\Delta C$ 为负，即使球向谷底滚去。接下来记 $\Delta v \equiv (\Delta v_1, \Delta v_2)^T$ 为 $v$ 的总变化，并且记梯度向量 $\nabla C$ 为：</p>
<p>\begin{eqnarray}<br>  \nabla C \equiv \left( \frac{\partial C}{\partial v_1},<br>  \frac{\partial C}{\partial v_2} \right)^T.<br>\label{8}<br>\end{eqnarray}</p>
<p>其中梯度向量符号 $\nabla C$ 会比较抽象，因为它纯粹就是一个数学上的概念。有了梯度符号，我们可以将式 (\ref{8}) 改写为</p>
<p>\begin{eqnarray}<br>  \Delta C \approx \nabla C \cdot \Delta v.<br>\label{9}<br>\end{eqnarray}</p>
<p>从该式可以看出，梯度向量 $\nabla C$ 将 $v$ 的变化反应到 $C$ 中，且我们也找到了如何让 $\Delta C$ 为负的方法。尤其是当我们选择</p>
<p>\begin{eqnarray}<br>  \Delta v = -\eta \nabla C,<br>\label{10}<br>\end{eqnarray}</p>
<p>当 $\eta$ 是一个很小的正参数时（其实该参数就是学习率），公式 (\ref{9}) 表明 $\Delta C \approx -\eta\nabla C \cdot \nabla C = -\eta |\nabla C|^2$。因为 $| \nabla C|^2 \geq 0$，能保证 $\Delta C \leq 0$。这正是我们需要的特性！在梯度下降学习法中，我们使用公式 (\ref{10}) 计算 $\Delta v$，然后移动球到新的位置：</p>
<p>\begin{eqnarray}<br>  v \rightarrow v’ = v -\eta \nabla C.<br>\label{11}<br>\end{eqnarray}</p>
<p>然后不停使用这一公式计算下一步，直到 $C$ 不再减小为止，即找到了全局最小值。</p>
<p>总结一下，首先重复计算梯度 $\nabla C$，然后向相反方向移动，画成图就是</p>
<p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/gradient.png" alt="梯度学习法"></p>
<p>请注意，上述梯度下降的规则并没有复制出真正的物理运动。在真实生活中，球有动量，滚向谷底后还会继续滚上去，在摩擦力的作用下才会最终停下。但我们的算法中没这么复杂。</p>
<p>为了使算法正常工作，公式 (\ref{9}) 中的学习率 $\eta$ 要尽可能的小，不然最终可能 $\Delta C &gt; 0$。同时学习率不能过小，不然会导致每一次迭代中 $\Delta v$ 过小，算法工作会非常慢。</p>
<p>尽管到现在我们一直用两个变量的函数 $C$ 举例。但其实对于多变量的函数，这仍是适用的。假设 $C$ 有 $m$ 个变量，$v_1,…, v_m$，那么变化 $\Delta C$ 为</p>
<p>\begin{eqnarray}<br>  \Delta C \approx \nabla C \cdot \Delta v,<br>\label{12}<br>\end{eqnarray}</p>
<p>其中 $\Delta v = (\Delta v_1,\ldots, \Delta v_m)^T$，梯度向量 $\nabla C$ 为</p>
<p>\begin{eqnarray}<br>  \nabla C \equiv \left(\frac{\partial C}{\partial v_1}, \ldots,<br>  \frac{\partial C}{\partial v_m}\right)^T.<br>\label{13}<br>\end{eqnarray}</p>
<p>梯度下降法虽然简单，但实际上很有效，也是一个经典的最优化方法，在神经网络中我们将用它来寻找代价函数的最小值。</p>
<p>同时，人们研究了大量梯度下降法的变种，包括去模拟真实的物理学，但都效果不好。因为这些变种算法不光计算一次偏导数，还需要计算二次偏导，这对计算机来说是巨大的挑战，尤其是有着上百万神经元的神经网络。</p>
<p>我们将使用梯度下降法来来找到权重 $w_k$ 和偏移 $b_l$。类比上述的梯度法，这里变量 $v_j$ 即为权重和偏移，而梯度 $\nabla C$ 为 $\partial C / \partial w_k$ 和 $\partial C/ \partial b_l$。梯度下降的更新规则如下：</p>
<p>\begin{eqnarray}<br>  w_k &amp; \rightarrow &amp; w_k’ = w_k-\eta \frac{\partial C}{\partial w_k} \label{16}\\<br>  b_l &amp; \rightarrow &amp; b_l’ = b_l-\eta \frac{\partial C}{\partial b_l}.<br>\label{17}<br>\end{eqnarray}</p>
<p>重复上述规则，找到代价函数的最小值，实现神经网络的自学习。</p>
<p>在适用梯度下降法时仍有一些挑战，比如公式 (\ref{6}) 中代价函数是每一个训练样本的误差 $C_x \equiv \frac{|y(x)-a|^2}{2}$ 的平均值。这意味着计算梯度时，需要每一个训练样本计算梯度。当样本数量很多时，就会造成性能的问题。</p>
<p>随机梯度下降法能够解决这一问题，加速学习过程。算法每次随机从训练输入中选取 $m$ 个数据，$X_1, X_2,\ldots, X_m $，记为 mini-batch。当 $m$ 足够大时，$\nabla C_{X_j}$ 将十分接近所有样本的平均值 $\nabla C_x$，即</p>
<p>\begin{eqnarray}<br>  \frac{\sum_{j=1}^m \nabla C_{X_{j}}}{m} \approx \frac{\sum_x \nabla C_x}{n} = \nabla C,<br>\label{18}<br>\end{eqnarray}</p>
<p>交换等式两边得</p>
<p>\begin{eqnarray}<br>  \nabla C \approx \frac{1}{m} \sum_{j=1}^m \nabla C_{X_{j}},<br>\label{19}<br>\end{eqnarray}</p>
<p>这样，我们把总体的梯度转换成计算随机选取的 mini-batch 的梯度。将随机梯度下降法运用到神经网络中，则权重和偏移为</p>
<p>\begin{eqnarray}<br>  w_k  \rightarrow &amp; w_k’ = w_k-\frac{\eta}{m}  \sum_j \frac{\partial C_{X_j}}{\partial w_k}<br>\label{20}<br>\end{eqnarray}</p>
<p>\begin{eqnarray}<br>  b_l  \rightarrow &amp; b_l’ = b_l-\frac{\eta}{m}  \sum_j \frac{\partial C_{X_j}}{\partial b_l},<br>\label{21}<br>\end{eqnarray}</p>
<p>其中，求和是对当前 mini-batch 中训练样本 $X_j$ 求和。然后选取下一组 mini-batch 重复上述过程，直到所有的训练输入全部选取完成，训练的一个 epoch 完成。<br>接着我们可以开始一个新的 epoch。</p>
<p>对于 MNIST 数据集来说，一共有 $n=60000$ 个数据，如果选择 mini-batch 的大小 $m=10$，则计算梯度的速度可以比原先快 $6000$ 倍。当然，加速计算的结果只是近似值，尽管会有一些统计学上的波动，但精确的梯度计算并不重要，只要小球下降的大方向不错就行。为什么我敢这么说，因为实践证明了啊，随机梯度也是大多数自学习算法的基石。</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>我们将官方的 MNIST 数据分成三部分，50000 个训练集，10000 个验证集，10000 个测试集。验证集用于在训练过程中实时观察神经网络正确率的变化，测试集用于测试最终神经网络的正确率。</p>
<p>下面介绍一下代码的核心部分。首先是 Network 类的初始化部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sizes)</span>:</span></span><br><span class="line">        self.num_layers = len(sizes)</span><br><span class="line">        self.sizes = sizes</span><br><span class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]</span><br><span class="line">        self.weights = [np.random.randn(y, x) </span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(sizes[:<span class="number">-1</span>], sizes[<span class="number">1</span>:])]</span><br></pre></td></tr></table></figure>
<p>列表 sizes 表示神经网络每一层所包含的神经元个数。比如我们想创建一个 2 个输入神经元，3 个神经元在中间层，一个输出神经元的网络，那么可以</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = Network([<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>偏移和权重都是使用 np.random.randn 函数随机初始化的，平均值为 0，标准差为 1。这种随机初始化并不是最佳方案，后续文章会逐步优化。</p>
<p>请注意，偏移和权重被初始化为 Numpy 矩阵。net.weights[1] 代表连接第二和第三层神经元的权重。</p>
<p>下面是 sigmoid 函数的定义：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br></pre></td></tr></table></figure>
<p>注意到虽然输入 $z$ 是向量，但 Numpy 能够自动处理，为向量中的每一个元素作相同的 sigmoid 运算。</p>
<p>然后是计算 sigmoid 函数的导数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""Derivative of the sigmoid function."""</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span><br></pre></td></tr></table></figure>
<p>每一层相对于前一层的输出为</p>
<p>\begin{eqnarray}<br>  a’ = \sigma(w a + b).<br>\label{22}<br>\end{eqnarray}</p>
<p>对应的是 feedforward 函数当：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(self, a)</span>:</span></span><br><span class="line">    <span class="string">"""Return the output of the network if "a" is input."""</span></span><br><span class="line">    <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">        a = sigmoid(np.dot(w, a)+b)</span><br><span class="line">    <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure>
<p>当然，Network 对象最重要的任务还是自学习。下面的 SGD 函数实现了随机梯度下降法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, training_data, epochs, mini_batch_size, eta,</span></span></span><br><span class="line"><span class="function"><span class="params">        test_data=None)</span>:</span></span><br><span class="line">    <span class="string">"""Train the neural network using mini-batch stochastic</span></span><br><span class="line"><span class="string">    gradient descent.  The "training_data" is a list of tuples</span></span><br><span class="line"><span class="string">    "(x, y)" representing the training inputs and the desired</span></span><br><span class="line"><span class="string">    outputs.  The other non-optional parameters are</span></span><br><span class="line"><span class="string">    self-explanatory.  If "test_data" is provided then the</span></span><br><span class="line"><span class="string">    network will be evaluated against the test data after each</span></span><br><span class="line"><span class="string">    epoch, and partial progress printed out.  This is useful for</span></span><br><span class="line"><span class="string">    tracking progress, but slows things down substantially."""</span></span><br><span class="line">    <span class="keyword">if</span> test_data: n_test = len(test_data)</span><br><span class="line">    n = len(training_data)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> xrange(epochs):</span><br><span class="line">        random.shuffle(training_data)</span><br><span class="line">        mini_batches = [</span><br><span class="line">            training_data[k:k+mini_batch_size]</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">0</span>, n, mini_batch_size)]</span><br><span class="line">        <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">            self.update_mini_batch(mini_batch, eta)</span><br><span class="line">        <span class="keyword">if</span> test_data:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;"</span>.format(</span><br><span class="line">                j, self.evaluate(test_data), n_test)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125; complete"</span>.format(j)</span><br></pre></td></tr></table></figure>
<p>列表 training_data 是由 (x,y) 元组构成，代表训练集的输入和期望输出。而 test_data 则是验证集（非测试集，这里变量名有些歧义），在每一个 epoch 结束时对神经网络正确率做一下检测。其他变量的含义比较显见，不再赘述。</p>
<p>SGD 函数在每一个 epoch 开始时随机打乱训练集，然后按照 mini-batch 的大小对数据分割。在每一步中对一个 mini_batch 计算梯度，在 self.update_mini_batch(mini_batch, eta) 更新权重和偏移：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta)</span>:</span></span><br><span class="line">    <span class="string">"""Update the network's weights and biases by applying</span></span><br><span class="line"><span class="string">    gradient descent using backpropagation to a single mini batch.</span></span><br><span class="line"><span class="string">    The "mini_batch" is a list of tuples "(x, y)", and "eta"</span></span><br><span class="line"><span class="string">    is the learning rate."""</span></span><br><span class="line">    nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">    nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">        delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br><span class="line">        nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> zip(nabla_b, delta_nabla_b)]</span><br><span class="line">        nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> zip(nabla_w, delta_nabla_w)]</span><br><span class="line">    self.weights = [w-(eta/len(mini_batch))*nw </span><br><span class="line">                    <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nabla_w)]</span><br><span class="line">    self.biases = [b-(eta/len(mini_batch))*nb </span><br><span class="line">                   <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nabla_b)]</span><br></pre></td></tr></table></figure>
<p>其中最关键的是</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br></pre></td></tr></table></figure>
<p>这就是反向转播算法，这是一个快速计算代价函数梯度的方法。所以 update_mini_batch 仅仅是计算这些梯度，然后用来更新 self.weights 和 self.biases。这里暂时不介绍它，它牵涉较多内容，下一篇文章会重点阐述。</p>
<p>让我们看一下 <a href="https://github.com/mnielsen/neural-networks-and-deep-learning/tree/master/src" target="_blank" rel="noopener">完整的程序</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#### Libraries</span></span><br><span class="line"><span class="comment"># Standard library</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># Third-party libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sizes)</span>:</span></span><br><span class="line">        <span class="string">"""The list ``sizes`` contains the number of neurons in the</span></span><br><span class="line"><span class="string">        respective layers of the network.  For example, if the list</span></span><br><span class="line"><span class="string">        was [2, 3, 1] then it would be a three-layer network, with the</span></span><br><span class="line"><span class="string">        first layer containing 2 neurons, the second layer 3 neurons,</span></span><br><span class="line"><span class="string">        and the third layer 1 neuron.  The biases and weights for the</span></span><br><span class="line"><span class="string">        network are initialized randomly, using a Gaussian</span></span><br><span class="line"><span class="string">        distribution with mean 0, and variance 1.  Note that the first</span></span><br><span class="line"><span class="string">        layer is assumed to be an input layer, and by convention we</span></span><br><span class="line"><span class="string">        won't set any biases for those neurons, since biases are only</span></span><br><span class="line"><span class="string">        ever used in computing the outputs from later layers."""</span></span><br><span class="line">        self.num_layers = len(sizes)</span><br><span class="line">        self.sizes = sizes</span><br><span class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]</span><br><span class="line">        self.weights = [np.random.randn(y, x)</span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(sizes[:<span class="number">-1</span>], sizes[<span class="number">1</span>:])]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(self, a)</span>:</span></span><br><span class="line">        <span class="string">"""Return the output of the network if ``a`` is input."""</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            a = sigmoid(np.dot(w, a)+b)</span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, training_data, epochs, mini_batch_size, eta,</span></span></span><br><span class="line"><span class="function"><span class="params">            test_data=None)</span>:</span></span><br><span class="line">        <span class="string">"""Train the neural network using mini-batch stochastic</span></span><br><span class="line"><span class="string">        gradient descent.  The ``training_data`` is a list of tuples</span></span><br><span class="line"><span class="string">        ``(x, y)`` representing the training inputs and the desired</span></span><br><span class="line"><span class="string">        outputs.  The other non-optional parameters are</span></span><br><span class="line"><span class="string">        self-explanatory.  If ``test_data`` is provided then the</span></span><br><span class="line"><span class="string">        network will be evaluated against the test data after each</span></span><br><span class="line"><span class="string">        epoch, and partial progress printed out.  This is useful for</span></span><br><span class="line"><span class="string">        tracking progress, but slows things down substantially."""</span></span><br><span class="line">        <span class="keyword">if</span> test_data: n_test = len(test_data)</span><br><span class="line">        n = len(training_data)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(epochs):</span><br><span class="line">            random.shuffle(training_data)</span><br><span class="line">            mini_batches = [</span><br><span class="line">                training_data[k:k+mini_batch_size]</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">0</span>, n, mini_batch_size)]</span><br><span class="line">            <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">                self.update_mini_batch(mini_batch, eta)</span><br><span class="line">            <span class="keyword">if</span> test_data:</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;"</span>.format(</span><br><span class="line">                    j, self.evaluate(test_data), n_test)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125; complete"</span>.format(j)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta)</span>:</span></span><br><span class="line">        <span class="string">"""Update the network's weights and biases by applying</span></span><br><span class="line"><span class="string">        gradient descent using backpropagation to a single mini batch.</span></span><br><span class="line"><span class="string">        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``</span></span><br><span class="line"><span class="string">        is the learning rate."""</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">            delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br><span class="line">            nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> zip(nabla_b, delta_nabla_b)]</span><br><span class="line">            nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> zip(nabla_w, delta_nabla_w)]</span><br><span class="line">        self.weights = [w-(eta/len(mini_batch))*nw</span><br><span class="line">                        <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nabla_w)]</span><br><span class="line">        self.biases = [b-(eta/len(mini_batch))*nb</span><br><span class="line">                       <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nabla_b)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return a tuple ``(nabla_b, nabla_w)`` representing the</span></span><br><span class="line"><span class="string">        gradient for the cost function C_x.  ``nabla_b`` and</span></span><br><span class="line"><span class="string">        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar</span></span><br><span class="line"><span class="string">        to ``self.biases`` and ``self.weights``."""</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="comment"># feedforward</span></span><br><span class="line">        activation = x</span><br><span class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            z = np.dot(w, activation)+b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = sigmoid(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line">        <span class="comment"># backward pass</span></span><br><span class="line">        delta = self.cost_derivative(activations[<span class="number">-1</span>], y) * \</span><br><span class="line">            sigmoid_prime(zs[<span class="number">-1</span>])</span><br><span class="line">        nabla_b[<span class="number">-1</span>] = delta</span><br><span class="line">        nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</span><br><span class="line">        <span class="comment"># Note that the variable l in the loop below is used a little</span></span><br><span class="line">        <span class="comment"># differently to the notation in Chapter 2 of the book.  Here,</span></span><br><span class="line">        <span class="comment"># l = 1 means the last layer of neurons, l = 2 is the</span></span><br><span class="line">        <span class="comment"># second-last layer, and so on.  It's a renumbering of the</span></span><br><span class="line">        <span class="comment"># scheme in the book, used here to take advantage of the fact</span></span><br><span class="line">        <span class="comment"># that Python can use negative indices in lists.</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</span><br><span class="line">            z = zs[-l]</span><br><span class="line">            sp = sigmoid_prime(z)</span><br><span class="line">            delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * sp</span><br><span class="line">            nabla_b[-l] = delta</span><br><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())</span><br><span class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(self, test_data)</span>:</span></span><br><span class="line">        <span class="string">"""Return the number of test inputs for which the neural</span></span><br><span class="line"><span class="string">        network outputs the correct result. Note that the neural</span></span><br><span class="line"><span class="string">        network's output is assumed to be the index of whichever</span></span><br><span class="line"><span class="string">        neuron in the final layer has the highest activation."""</span></span><br><span class="line">        test_results = [(np.argmax(self.feedforward(x)), y)</span><br><span class="line">                        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_data]</span><br><span class="line">        <span class="keyword">return</span> sum(int(x == y) <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_results)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span><span class="params">(self, output_activations, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return the vector of partial derivatives \partial C_x /</span></span><br><span class="line"><span class="string">        \partial a for the output activations."""</span></span><br><span class="line">        <span class="keyword">return</span> (output_activations-y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#### Miscellaneous functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""The sigmoid function."""</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""Derivative of the sigmoid function."""</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span><br></pre></td></tr></table></figure>
<p>让我们来看一下这个最简单的 BP 神经网络在手写数字识别上效果如何。初始化一个含有 30 个神经元的隐藏层。训练 30 个 epoch，mini-batch 大小为 10，学习率 $\eta=3.0$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mnist_loader</span><br><span class="line">training_data, validation_data, test_data = mnist_loader.load_data_wrapper()</span><br><span class="line"><span class="keyword">import</span> network</span><br><span class="line">net = network.Network([<span class="number">784</span>, <span class="number">30</span>, <span class="number">10</span>])</span><br><span class="line"><span class="keyword">import</span> network</span><br><span class="line">net = network.Network([<span class="number">784</span>, <span class="number">30</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p>结果为</p>
<pre><code>Epoch 0: 9129 / 10000
Epoch 1: 9295 / 10000
Epoch 2: 9348 / 10000
...
Epoch 27: 9528 / 10000
Epoch 28: 9542 / 10000
Epoch 29: 9534 / 10000
</code></pre><p>看来不错！识别率达到了 95.42%！如果将隐藏层的神经元增加到 100 个，最终识别率达到了 96.59%！</p>
<h2 id="神经网络识别手写数字目录"><a href="#神经网络识别手写数字目录" class="headerlink" title="神经网络识别手写数字目录"></a>神经网络识别手写数字目录</h2><ol>
<li><a href="http://www.lyyyuna.com/2016/05/29/handwritten-neural-net/">基于 BP 神经网络的识别手写体数字 - 神经网络基础</a></li>
<li><a href="http://www.lyyyuna.com/2016/06/25/handwritten-neural-net02/">基于 BP 神经网络的手写体数字识别 - 设计与实现</a></li>
<li><a href="http://www.lyyyuna.com/2016/06/26/handwritten-neural-net03/">基于 BP 神经网络的手写体数字识别 - 反向传播算法</a></li>
<li><a href="http://www.lyyyuna.com/2016/06/30/handwritten-neural-net04/">基于 BP 神经网络的手写体数字识别 - 优化</a></li>
</ol>
  
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/数学/">数学</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/Python/">Python</a><a href="/tags/mnist/">mnist</a><a href="/tags/neural-network/">neural network</a>
  </div>

</div>



	<div class="article-share" id="share">
	
	  <div data-url="http://www.lyyyuna.com/2016/06/25/handwritten-neural-net02/" data-title="基于 BP 神经网络的手写体数字识别 - 设计与实现 | lyyyuna 的小花园" data-tsina="null" class="share clearfix">
	  </div>
	
	</div>


</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2016/06/26/handwritten-neural-net03/" title="基于 BP 神经网络的手写体数字识别 - 反向传播算法">
  <strong>上一篇：</strong><br/>
  <span>
  基于 BP 神经网络的手写体数字识别 - 反向传播算法</span>
</a>
</div>


<div class="next">
<a href="/2016/05/29/handwritten-neural-net/"  title="基于 BP 神经网络的识别手写体数字 - 神经网络基础">
 <strong>下一篇：</strong><br/> 
 <span>基于 BP 神经网络的识别手写体数字 - 神经网络基础
</span>
</a>
</div>

</nav>

	

<section id="comments" class="comment">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</div>  
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">文章目录</strong>
 
 <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#手写体数字识别的神经网络结构"><span class="toc-number">1.</span> <span class="toc-text">手写体数字识别的神经网络结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#基于梯度下降法学习"><span class="toc-number">2.</span> <span class="toc-text">基于梯度下降法学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#代码实现"><span class="toc-number">3.</span> <span class="toc-text">代码实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络识别手写数字目录"><span class="toc-number">4.</span> <span class="toc-text">神经网络识别手写数字目录</span></a></li></ol>
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  
<div class="github-card">
<p class="asidetitle">Github 名片</p>
<div class="github-card" data-github="lyyyuna" data-width="220" data-height="119" data-theme="medium">
<script type="text/javascript" src="//cdn.jsdelivr.net/github-cards/latest/widget.js" ></script>
</div>
  </div>



  
<div class="categorieslist">
	<p class="asidetitle">分类</p>
		<ul>
		
		  
			<li><a href="/categories/数学/" title="数学">数学<sup>4</sup></a></li>
		  
		
		  
			<li><a href="/categories/数据库/" title="数据库">数据库<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/杂/" title="杂">杂<sup>12</sup></a></li>
		  
		
		  
			<li><a href="/categories/系统/" title="系统">系统<sup>6</sup></a></li>
		  
		
		  
			<li><a href="/categories/网络/" title="网络">网络<sup>8</sup></a></li>
		  
		
		  
			<li><a href="/categories/自动化测试/" title="自动化测试">自动化测试<sup>7</sup></a></li>
		  
		
		  
			<li><a href="/categories/语言/" title="语言">语言<sup>13</sup></a></li>
		  
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">标签</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/Python/" title="Python">Python<sup>8</sup></a></li>
			
		
			
				<li><a href="/tags/robot-framework/" title="robot framework">robot framework<sup>7</sup></a></li>
			
		
			
				<li><a href="/tags/Python-script/" title="Python script">Python script<sup>6</sup></a></li>
			
		
			
				<li><a href="/tags/scons/" title="scons">scons<sup>6</sup></a></li>
			
		
			
				<li><a href="/tags/Python2-7源码/" title="Python2.7源码">Python2.7源码<sup>5</sup></a></li>
			
		
			
				<li><a href="/tags/mnist/" title="mnist">mnist<sup>4</sup></a></li>
			
		
			
				<li><a href="/tags/neural-network/" title="neural network">neural network<sup>4</sup></a></li>
			
		
			
				<li><a href="/tags/asyncio/" title="asyncio">asyncio<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/http-proxy/" title="http proxy">http proxy<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/爬虫/" title="爬虫">爬虫<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/bilibili/" title="bilibili">bilibili<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/cpp/" title="cpp">cpp<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/DHT/" title="DHT">DHT<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Exif/" title="Exif">Exif<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Windows-Debugger/" title="Windows Debugger">Windows Debugger<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Python3-x源码/" title="Python3.x源码">Python3.x源码<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/PostgreSQL/" title="PostgreSQL">PostgreSQL<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/shellcode/" title="shellcode">shellcode<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Celery/" title="Celery">Celery<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/openwrt/" title="openwrt">openwrt<sup>1</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
          <li>
            
            	<a href="http://www.lihulab.net" target="_blank" title="自动化测试">自动化测试</a>
            
          </li>
        
          <li>
            
            	<a href="http://oj.lihulab.net" target="_blank" title="蠡湖实验室 OJ">蠡湖实验室 OJ</a>
            
          </li>
        
    </ul>
</div>

  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS 订阅</a>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	
	<section class="info">
		<p> Hello, I am lyy <br/>
			This is my blog, believe it or not.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		
		<a href="https://github.com/lyyyuna" target="_blank" class="icon-github" title="github"></a>
		
		
		
		
		
		
		<a href="https://www.douban.com/people/44114278" target="_blank" class="icon-douban" title="豆瓣"></a>
		
		
		<a href="http://www.zhihu.com/people/li-yi-yang-38" target="_blank" class="icon-zhihu" title="知乎"></a>
		
		
		
		<a href="mailto:lyyyuna@outlook.com" target="_blank" class="icon-email" title="Email Me"></a>
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2020 
		
		<a href="/about" target="_blank" title="lyyyuna">lyyyuna</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<div class="hoverqrcode clearfix"></div>',
  '<a class="overlay" id="qrcode"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);

  $('.hoverqrcode').hide();

  var myWidth = 0;
  function updatehoverqrcode(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
    var qrsize = myWidth > 1024 ? 200:100;
    var options = {render: 'image', size: qrsize, fill: '#2ca6cb', text: url, radius: 0.5, quiet: 1};
    var p = $('.article-share-qrcode').position();
    $('.hoverqrcode').empty().css('width', qrsize).css('height', qrsize)
                          .css('left', p.left-qrsize/2+20).css('top', p.top-qrsize-10)
                          .qrcode(options);
  };
  $(window).resize(function(){
    $('.hoverqrcode').hide();
  });
  $('.article-share-qrcode').click(function(){
    updatehoverqrcode();
    $('.hoverqrcode').toggle();
  });
  $('.article-share-qrcode').hover(function(){}, function(){
      $('.hoverqrcode').hide();
  });
});   
</script>




<script type="text/javascript">

var disqus_shortname = 'lyyyuna';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>






<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->

<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-42389939-1', 'www.lyyyuna.com');  
ga('send', 'pageview');
</script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?4ba228e3ecc48b86bafa07900215f733";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="返回顶部"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  </body>
</html>
