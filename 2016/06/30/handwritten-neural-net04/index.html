
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  
    <title>基于 BP 神经网络的手写体数字识别 - 优化 | lyyyuna 的小花园</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="lyyyuna">
    

    
    <meta name="description" content="MathJax.Hub.Config({   TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } } });    目前为止，我们论述中，似乎手写数字图像本身并没有太多篇幅。这就是神经网络的特点，那 784 个像素点只是神经网络的输入，不需要任何图像处理。 9">
<meta name="keywords" content="Python,mnist,neural network">
<meta property="og:type" content="article">
<meta property="og:title" content="基于 BP 神经网络的手写体数字识别 - 优化">
<meta property="og:url" content="http://www.lyyyuna.com/2016/06/30/handwritten-neural-net04/index.html">
<meta property="og:site_name" content="lyyyuna 的小花园">
<meta property="og:description" content="MathJax.Hub.Config({   TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } } });    目前为止，我们论述中，似乎手写数字图像本身并没有太多篇幅。这就是神经网络的特点，那 784 个像素点只是神经网络的输入，不需要任何图像处理。 95% 的识别率看起来很高了，但还有不少提升空间。本篇文章将介绍多种优化方法。 交叉熵代价函数理想情">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/tikz28.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201605/sigmoid_function.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/tikz29.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/overfitting1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/overfitting2.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/overfitting3.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/overfitting4.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/overfitting_full.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/regularized1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/regularized2.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/regularized_full.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/tenpoints1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/tenpoints2.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/tenpoints3.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/gauss1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/gauss2.png">
<meta property="og:updated_time" content="2017-11-29T00:19:00.164Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="基于 BP 神经网络的手写体数字识别 - 优化">
<meta name="twitter:description" content="MathJax.Hub.Config({   TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } } });    目前为止，我们论述中，似乎手写数字图像本身并没有太多篇幅。这就是神经网络的特点，那 784 个像素点只是神经网络的输入，不需要任何图像处理。 95% 的识别率看起来很高了，但还有不少提升空间。本篇文章将介绍多种优化方法。 交叉熵代价函数理想情">
<meta name="twitter:image" content="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/tikz28.png">

    
    <link rel="alternative" href="/atom.xml" title="lyyyuna 的小花园" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css">
</head>

  <body>
    <header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="lyyyuna 的小花园" title="lyyyuna 的小花园"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="lyyyuna 的小花园">lyyyuna 的小花园</a></h1>
				<h2 class="blog-motto">动静中之动</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
						<li><a href="/categories">Categories</a></li>
					
					<li>
 					
					<form class="search" action="/search/index.html" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" autocomplete="off" name="q" maxlength="20" placeholder="搜索" />
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2016/06/30/handwritten-neural-net04/" title="基于 BP 神经网络的手写体数字识别 - 优化" itemprop="url">基于 BP 神经网络的手写体数字识别 - 优化</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="lyyyuna" target="_blank" itemprop="author">lyyyuna</a>
		
  <p class="article-time">
    <time datetime="2016-06-30T12:25:58.000Z" itemprop="datePublished"> 发表于 2016-06-30</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">文章目录</strong>
		
			<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#交叉熵代价函数"><span class="toc-number">1.</span> <span class="toc-text">交叉熵代价函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#介绍交叉熵代价函数"><span class="toc-number">1.1.</span> <span class="toc-text">介绍交叉熵代价函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#柔性最大值传输-softmax"><span class="toc-number">1.2.</span> <span class="toc-text">柔性最大值传输 softmax</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#过拟合和正则化"><span class="toc-number">2.</span> <span class="toc-text">过拟合和正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#正则化-Regularization"><span class="toc-number">2.1.</span> <span class="toc-text">正则化 Regularization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#为什么正则化能抑制过拟合"><span class="toc-number">2.2.</span> <span class="toc-text">为什么正则化能抑制过拟合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#其他抑制过拟合的方法"><span class="toc-number">2.3.</span> <span class="toc-text">其他抑制过拟合的方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#改进权重初始化"><span class="toc-number">3.</span> <span class="toc-text">改进权重初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#代码"><span class="toc-number">4.</span> <span class="toc-text">代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络识别手写数字目录"><span class="toc-number">5.</span> <span class="toc-text">神经网络识别手写数字目录</span></a></li></ol>
		
		</div>
		
		<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>


<p>目前为止，我们论述中，似乎手写数字图像本身并没有太多篇幅。这就是神经网络的特点，那 784 个像素点只是神经网络的输入，不需要任何图像处理。</p>
<p>95% 的识别率看起来很高了，但还有不少提升空间。本篇文章将介绍多种优化方法。</p>
<h2 id="交叉熵代价函数"><a href="#交叉熵代价函数" class="headerlink" title="交叉熵代价函数"></a>交叉熵代价函数</h2><p>理想情况下我们的神经网络能够快速地从错误中学习。但实际过程中却可能学习缓慢。让我们看下面这个例子：</p>
<p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/tikz28.png" alt="例子"></p>
<p>我们期望该神经元在输入 1 时输出 0。若神经元权重初始值为 0.6，偏移初始值为 0.9，则初始输出为 0.82，离预期输出还有一段距离。我们选择学习率 $\eta=0.15$，点击 <strong>Run</strong> 观察输出变化和二次代价函数的变化动画：</p>
<p><script type="text/javascript" src="//cdn.bootcss.com/paper.js/0.9.25/paper-full.min.js"></script></p>
<p><script type="text/paperscript" src="/customjs/saturation1.js" canvas="saturation1"><br></script></p>
<center><br><canvas id="saturation1" width="520" height="300"></canvas><br></center>


<p>可以看到，神经元一直在“学习进步”，且“进步”神速，最终的输出也接近于 0。现在将权重初始值和偏移初始值都设为 2.0，再点击 <strong>Run</strong> 观察动画：</p>
<p><script type="text/paperscript" src="/customjs/saturation2.js" canvas="saturation2"><br></script></p>
<center><br><canvas id="saturation2" width="520" height="300"></canvas><br></center>

<p>参数未变，结果造成学习速度减慢。仔细观察，开始的 150 个 epoch 权重和偏移几乎保持不变。过了这个点，神经元又变成了“进步”神速的好孩子。</p>
<p>我们经常把自学习与人类的学习作比较，这里神经元的学习过程显得反常。当人类发现自己错误的离谱时会学习较快，而大部分未优化的神经元却在错误中踌躇不前。</p>
<p>让我们来探究一下问题的缘由。神经元学习慢，等同于权重和偏移变化慢，等同于代价函数的偏导数 $\partial C/\partial w$ 和 $\partial C / \partial b$ 较小。我们的二次代价函数为</p>
<p>\begin{eqnarray}<br>  C = \frac{(y-a)^2}{2},<br>\label{54}<br>\end{eqnarray}</p>
<p>其中，$a$ 是当训练输入 $x=1$ 时神经元的输出，$y=0$ 是期望输出。将 $a=\sigma(z), z = wx+b$ 代入上式，并求取偏导数可得</p>
<p>\begin{eqnarray}<br>  \frac{\partial C}{\partial w} &amp; = &amp; (a-y)\sigma’(z) x = a \sigma’(z) \label{55}\\<br>  \frac{\partial C}{\partial b} &amp; = &amp; (a-y)\sigma’(z) = a \sigma’(z),<br>\label{56}<br>\end{eqnarray}</p>
<p>结合我们的 $\sigma$ 函数图像，即 sigmoid 函数图像：</p>
<p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201605/sigmoid_function.png" alt="sigmoid 函数"></p>
<p>当神经元的输出接近于 0 时，曲线变得很平缓，所以 $\sigma’(z)$ 的值很小，结合公式 (\ref{55}) 和 (\ref{56}) 可知，$\partial C/\partial w$ 和 $\partial C / \partial b$ 的值很小。</p>
<h3 id="介绍交叉熵代价函数"><a href="#介绍交叉熵代价函数" class="headerlink" title="介绍交叉熵代价函数"></a>介绍交叉熵代价函数</h3><p>假设我们要训练如下的神经元，输入变量为 $x_1, x_2, …$，对应的权重为 $w_1, w_2, …$，偏移为 $b$：</p>
<p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/tikz29.png" alt="多输入神经元"></p>
<p>其中输出是 $a=\sigma(z), z = \sum_j w_j x_j+b$。对此，我们定义该神经元的交叉熵代价函数为</p>
<p>\begin{eqnarray}<br>  C = -\frac{1}{n} \sum_x \left[y \ln a + (1-y ) \ln (1-a) \right],<br>\label{57}<br>\end{eqnarray}</p>
<p>其中，$n$ 是所有训练数据的总和，$x$ 和 $y$ 是相应的输入和期望输出。为什么公式 (\ref{57}) 可以作为代价函数？</p>
<p>首先，由于 $a$ 的取值在 0, 1 之间，$y \ln a + (1-y) \ln (1-a)$ 为负，取反后公式 (\ref{57}) 非负。然后，当实际输出 $a$ 接近期望输出 $y$ 时，交叉熵接近于 0。这两点是代价函数的基本条件。将 $a = \sigma(z)$ 代入公式 (\ref{57}) 并计算交叉熵对权重的偏导，得</p>
<p>\begin{eqnarray}<br>  \frac{\partial C}{\partial w_j} &amp; = &amp; -\frac{1}{n} \sum_x \left(<br>    \frac{y }{\sigma(z)} -\frac{(1-y)}{1-\sigma(z)} \right)<br>  \frac{\partial \sigma}{\partial w_j} \label{58}\\<br> &amp; = &amp; -\frac{1}{n} \sum_x \left(<br>    \frac{y}{\sigma(z)}<br>    -\frac{(1-y)}{1-\sigma(z)} \right)\sigma’(z) x_j.<br>\label{59}<br>\end{eqnarray}</p>
<p>合并成一个分母，得</p>
<p>\begin{eqnarray}<br>  \frac{\partial C}{\partial w_j} &amp; = &amp; \frac{1}{n}<br>  \sum_x \frac{\sigma’(z) x_j}{\sigma(z) (1-\sigma(z))}<br>  (\sigma(z)-y).<br>\label{60}<br>\end{eqnarray}</p>
<p>由于 $\sigma’(z) = \sigma(z)(1-\sigma(z))$，上式还可以抵消，进一步简化为</p>
<p>\begin{eqnarray}<br>  \frac{\partial C}{\partial w_j} =  \frac{1}{n} \sum_x x_j(\sigma(z)-y).<br>\label{61}<br>\end{eqnarray}</p>
<p>权重的学习速率由 $\sigma(z)-y$ 控制，误差越大，学习越快。二次代价函数 (\ref{55}) 中，正是由于 $\sigma’(z)$ 的存在，自学习的速率减慢，而公式 (\ref{61}) 消掉了这一项。同理，可得交叉熵对权重的偏导数为</p>
<p>\begin{eqnarray}<br>  \frac{\partial C}{\partial b} = \frac{1}{n} \sum_x (\sigma(z)-y).<br>\label{62}<br>\end{eqnarray}</p>
<p>同样，恼人的 $\sigma’(z)$ 也被消掉了。</p>
<p>让我们再来看一下之前动画，这次使用交叉熵作为代价函数，且学习率改为 $\eta=0.005$。第一个，权重初始值是 0.6，偏移初始值是 0.9，点击 <strong>Run</strong>。</p>
<p><script type="text/paperscript" src="/customjs/saturation3.js" canvas="saturation3"><br></script></p>
<center><br><canvas id="saturation3" width="520" height="300"></canvas><br></center>

<p>意料之中，学习速度还是很快。第二个，权重和偏移初始值都为 2，点击 <strong>Run</strong>。</p>
<p><script type="text/paperscript" src="/customjs/saturation4.js" canvas="saturation4"><br></script></p>
<center><br><canvas id="saturation4" width="520" height="300"></canvas><br></center>

<p>神经元还是学习迅速。你可能注意到了 $\eta$ 的变化，这会不会影响试验结果？其实，我们关心的不是神经元学习的绝对速度，而是学习速度本身的变化。</p>
<p>上述结论完全可以推广到多层多神经元的网络，定义交叉熵为</p>
<p>\begin{eqnarray}  C = -\frac{1}{n} \sum_x<br>  \sum_j \left[y_j \ln a^L_j + (1-y_j) \ln (1-a^L_j) \right].<br>\label{63}<br>\end{eqnarray}</p>
<p>那什么时候该用交叉熵而不是二次代价函数？对于 sigmoid 神经元，交叉熵几乎永远是更优选择，也被实践证明。</p>
<h3 id="柔性最大值传输-softmax"><a href="#柔性最大值传输-softmax" class="headerlink" title="柔性最大值传输 softmax"></a>柔性最大值传输 softmax</h3><p>通过将神经网络的输出由 sigmoid 换成 softmax 层可以进一步改善学习缓慢的问题。 </p>
<p>对于输出层，其权重输入为 $z^L_j = \sum_{k} w^L_{jk} a^{L-1}_k + b^L_j$，施加 softmax 函数，输出层激励为</p>
<p>\begin{eqnarray}<br>  a^L_j = \frac{e^{z^L_j}}{\sum_k e^{z^L_k}},<br>\label{78}<br>\end{eqnarray}</p>
<p>其中，分母是所有输出神经元输出之和。又是一个看起来意义不明的函数。如果我们将所有激励相加，会发现其值正好等于 1，</p>
<p>\begin{eqnarray}<br>  \sum_j a^L_j &amp; = &amp; \frac{\sum_j e^{z^L_j}}{\sum_k e^{z^L_k}} = 1.<br>\label{79}<br>\end{eqnarray}</p>
<p>当某一个激励增加时，其他的激励必须相应地减少以保证和不变。换句话说，如果将 softmax 作为输出层，神经网络的所有输出符合概率分布。这又是一个方便的特性，尤其对于手写数字识别来说，每个输出代表每个数字的概率，之前 sigmoid 的方案有可能会有如下的输出</p>
<pre><code>[0.9, 0.3, 0.4, 0.1, 0.0, 0.4, 0.0, 0.0, 0.0, 0.1]
</code></pre><p>每个概率之间并没有联系，sigmoid 输出神经元只是各顾各的训练。而且人们拿到这个结果肯定会非常疑惑，为啥概率相加不等于 1？</p>
<h2 id="过拟合和正则化"><a href="#过拟合和正则化" class="headerlink" title="过拟合和正则化"></a>过拟合和正则化</h2><p>诺贝尔物理学奖获得者费米曾经和他的同事讨论一个数学模型。该模型能够很好地解释实验结果，但费米仍有疑虑。他问该模型用了多少个自由变量，同事回答四个。费米回答：“我记得我朋友冯诺依曼曾经说过，四个变量我能描述一头大象，五个变量就能让他转鼻子了”。</p>
<p>拥有大量自由变量的模型很容易就描述大部分实验现象。但是不能说符合实验现象的模型就是好模型。有足够自由变量的模型中，几乎可以描述任何给定大小的数据集，但没有抓住现象背后的本质。这种情况下，模型只能适用于现有数据，面对新的情况却束手无策。模型的真正考验，是它有能力对未出现的现象做出预言。</p>
<p>费米和诺依曼对四变量的模型就产生了质疑。而我们手写数字识别系统有 30 个隐藏神经元，有将近 24000 个变量！若是 100 个隐藏神经元，那就有近 80000 个变量！这么多变量，不禁要问，结果可信么？会出现费米和诺依曼担心的问题么？</p>
<p>让我们来模拟一下这种情况的发生。我们使用 30 个隐藏神经元，但我们不使用 50000 个 MNIST 训练图像，相反，只是用 1000 个训练图像。这样，问题会更显著。训练使用交叉熵函数，学习率 $\eta=0.5$，mini-batch 大小为 10，训练 400 个epochs。让我们用 <a href="https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network2.py" target="_blank" rel="noopener">network2</a> 来观察变化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mnist_loader </span><br><span class="line">training_data, validation_data, test_data = mnist_loader.load_data_wrapper()</span><br><span class="line"><span class="keyword">import</span> network2 </span><br><span class="line">net = network2.Network([<span class="number">784</span>, <span class="number">30</span>, <span class="number">10</span>], cost=network2.CrossEntropyCost) </span><br><span class="line">net.large_weight_initializer()</span><br><span class="line">net.SGD(training_data[:<span class="number">1000</span>], <span class="number">400</span>, <span class="number">10</span>, <span class="number">0.5</span>, evaluation_data=test_data, monitor_evaluation_accuracy=<span class="keyword">True</span>, monitor_training_cost=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>首先是代价函数随学习进度的变化图像：</p>
<p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/overfitting1.png" alt="代价函数变化"></p>
<p>看起来不错，代价不断减小，似乎说明我们的神经网络一直在进步。但是测试集上识别率却不是那么回事：</p>
<p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/overfitting2.png" alt="识别率变化"></p>
<p>280 个 epoch 之后，识别率处于波动稳定状态，且远低于之前达到的 95% 识别率。训练数据的交叉熵和测试集的实际结果截然不同，出现了费米担心的问题。可以说，280 个 epoch 之后的学习完全无用，标准说法是<strong>过拟合 overfitting</strong>。</p>
<p>让我们在做一点更直观的比较：训练集和测试集的交叉熵横向对比，及识别率横向对比。</p>
<p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/overfitting3.png" alt="测试集交叉熵"></p>
<p>交叉熵仅仅下降了 15 个 epoch，之后就一路飙高，持续恶化。这是我们模型过拟合的又一个标志。这里有个小疑问，epoch 15 和 epoch 280 哪个属于开始过拟合？从实践的角度看，我们真正的关心的是测试集（更接近真实情况）上的识别率，交叉熵只是算法的附带物，所以我们认为，epoch 280 之后，过拟合开始占据神经网络的学习过程。</p>
<p>下面是训练集的识别率变化：</p>
<p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/overfitting4.png" alt="训练集识别率"></p>
<p>我们的模型能够 100% 地描述 1000 个训练图像，实际却不能很好地分类测试数字。</p>
<p>最明显的检测过拟合的方法是观察测试集上识别率的变化。如果发现测试集识别率不再改善，就应该停止训练。这也是<a href="http://www.lyyyuna.com/2016/06/25/handwritten-neural-net02/">之前文章-代码实现</a>为什么要再引入验证集的原因，毕竟测试集是最终判定结果用的，应该与训练过程彻底分离。</p>
<pre><code>training_data, validation_data, test_data = mnist_loader.load_data_wrapper()
</code></pre><p>我们一直在讨论 1000 个训练图片的过拟合问题，那 50000 个图片结果还是一样吗？这里给出结果：</p>
<p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/overfitting_full.png" alt="扩大训练集"></p>
<p>可以看到，过拟合不再那么明显了，训练集的识别率只比测试集高 1.5% 左右。这也间接说明，大量训练数据下神经网络难以达到过拟合。不过训练集并不是那么容易获得的。</p>
<h3 id="正则化-Regularization"><a href="#正则化-Regularization" class="headerlink" title="正则化 Regularization"></a>正则化 Regularization</h3><p>首先要明确一点，我们并不想减少网络中变量的数目，我们需要这种特性来描述现实世界复杂的变化。</p>
<p>正则化方法能够缓解过拟合问题，最常用的是权重衰减法或者叫 $L_2$ 正则化。$L_2$ 正则化只是在原先的代价函数中加入一个正则项：</p>
<p>\begin{eqnarray}<br>C = -\frac{1}{n} \sum_{xj} \left[ y_j \ln a^L_j+(1-y_j) \ln<br>(1-a^L_j)\right] + \frac{\lambda}{2n} \sum_w w^2.<br>\label{85}<br>\end{eqnarray}</p>
<p>等式右边第一项是交叉熵，第二项是网络中所有权重的平方和，并乘以系数 $\lambda /2n$，其中 $\lambda &gt; 0$，称作正则化参数。</p>
<p>正则化不只适用于交叉熵代价函数，二次代价函数也可以使用：</p>
<p>\begin{eqnarray}<br>C = \frac{1}{2n} \sum_x |y-a^L|^2 +<br>  \frac{\lambda}{2n} \sum_w w^2.<br>\label{86}<br>\end{eqnarray}</p>
<p>总结下来就是</p>
<p>\begin{eqnarray}<br>C = C_0 + \frac{\lambda}{2n}<br>\sum_w w^2,<br>\label{87}<br>\end{eqnarray}</p>
<p>其中 $C_0$ 是未正则化的代价函数。观察该式，可以发现正则化逼迫自学习过程选择更小的权重，权重越大，代价也越高。由于代价函数的变换，随机梯度下降法中偏导数的计算也要随之改变：</p>
<p>\begin{eqnarray}<br>  \frac{\partial C}{\partial w} &amp; = &amp; \frac{\partial C_0}{\partial w} +<br>  \frac{\lambda}{n} w \label{88}\\<br>  \frac{\partial C}{\partial b} &amp; = &amp; \frac{\partial C_0}{\partial b}.<br>\label{89}<br>\end{eqnarray}</p>
<p>$\partial C_0 / \partial w$ 和 $\partial C_0 / \partial b$ 仍让可以用<a href="http://www.lyyyuna.com/2016/06/26/handwritten-neural-net03/">上一篇</a>的反向传播算法求得。对偏移的偏导数并没有改变，所以据梯度下降法学习规则仍为：</p>
<p>\begin{eqnarray}<br>b &amp; \rightarrow &amp; b -\eta \frac{\partial C_0}{\partial b}.<br>\label{90}<br>\end{eqnarray}</p>
<p>而权重的自学习规则则变成：</p>
<p>\begin{eqnarray}<br>  w &amp; \rightarrow &amp; w-\eta \frac{\partial C_0}{\partial<br>    w}-\frac{\eta \lambda}{n} w \label{91}\\<br>  &amp; = &amp; \left(1-\frac{\eta \lambda}{n}\right) w -\eta \frac{\partial<br>    C_0}{\partial w}.<br>\label{92}<br>\end{eqnarray}</p>
<p>可以看到，权重 $w$ 乘以了一个小于 1 的系数 $1-\frac{\eta \lambda}{n}$，称为权重衰减，有减小权重的趋势。而后一项由于偏导有正有负，所以权重值并不是单调递减，两项相加，彼此制约。</p>
<p>以上是梯度下降法，随机梯度下降法也只要做相应的调整：</p>
<p>\begin{eqnarray}<br>  w \rightarrow \left(1-\frac{\eta \lambda}{n}\right) w -\frac{\eta}{m}<br>  \sum_x \frac{\partial C_x}{\partial w},<br>\label{93}<br>\end{eqnarray}</p>
<p>\begin{eqnarray}<br>  b \rightarrow b - \frac{\eta}{m} \sum_x \frac{\partial C_x}{\partial b},<br>\label{94}<br>\end{eqnarray}</p>
<p>其中，求和是对一个 mini-batch 内所有数据的求和。</p>
<p>让我们实验一下。这次在 <a href="https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network2.py" target="_blank" rel="noopener">network2</a> 中加入正则化参数 $\lambda=0.1$。对比之前 1000 个训练数据集的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mnist_loader </span><br><span class="line">training_data, validation_data, test_data = mnist_loader.load_data_wrapper() </span><br><span class="line"><span class="keyword">import</span> network2 </span><br><span class="line">net = network2.Network([<span class="number">784</span>, <span class="number">30</span>, <span class="number">10</span>], cost=network2.CrossEntropyCost)</span><br><span class="line">net.large_weight_initializer()</span><br><span class="line">net.SGD(training_data[:<span class="number">1000</span>], <span class="number">400</span>, <span class="number">10</span>, <span class="number">0.5</span>, </span><br><span class="line">        evaluation_data=test_data, lmbda = <span class="number">0.1</span>, </span><br><span class="line">        monitor_evaluation_cost=<span class="keyword">True</span>, monitor_evaluation_accuracy=<span class="keyword">True</span>, </span><br><span class="line">        monitor_training_cost=<span class="keyword">True</span>, monitor_training_accuracy=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>训练集的交叉熵代价看来没什么问题：</p>
<p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/regularized1.png" alt="训练集的代价"></p>
<p>但这次识别率却是一直在上升：</p>
<p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/regularized2.png" alt="识别率上升"></p>
<p>我们在试一下 50000 个训练数据的情况：</p>
<p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/regularized_full.png" alt="识别率"></p>
<p>训练集和测试集的识别率只差 1% 左右，而正则化之前这一值是 1.5%。</p>
<p>用以下参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net = network2.Network([<span class="number">784</span>, <span class="number">100</span>, <span class="number">10</span>], cost=network2.CrossEntropyCost)</span><br><span class="line">net.large_weight_initializer()</span><br><span class="line">net.SGD(training_data, <span class="number">60</span>, <span class="number">10</span>, <span class="number">0.1</span>, lmbda=<span class="number">5.0</span>,</span><br><span class="line">      evaluation_data=validation_data,</span><br><span class="line">      monitor_evaluation_accuracy=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>识别率提高到 98%。你可以认为，由于过拟合的存在，神经网络模型易陷入局部最优解，正则之后，跳出局部最优，滚向全局最优，最终带来识别率的提升。</p>
<h3 id="为什么正则化能抑制过拟合"><a href="#为什么正则化能抑制过拟合" class="headerlink" title="为什么正则化能抑制过拟合"></a>为什么正则化能抑制过拟合</h3><p>从正则化的结果来看，似乎权重值越小越能抑制过拟合。</p>
<p>让我们看一个经典的例子，假设要对下图所示的点建立一个模型：</p>
<p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/tenpoints1.png" alt="很多点"></p>
<p>数一下，有 10 个点，那可以用一个 9 次函数精确地描述它，$y = a_0 x^9 + a_1 x^8 + \ldots + a_9$：</p>
<p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/tenpoints2.png" alt="九次函数"></p>
<p>如果允许一些误差，也可以使用一个简单的线性模型：</p>
<p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/tenpoints3.png" alt="线性模型"></p>
<p>那么，哪个才是更好的模型？哪个才能描述还未出现的新点？实践表明，允许一定误差的模型更符合实际情况。现实世界伴随着大量不确定性，传感器采集的噪声和仪器本身的精度都会给训练集加入一定的<strong>噪声</strong>，这样，后一个模型便在预测新点时占据了优势。</p>
<p>回到我们的神经网络，当输入因为某些噪声剧烈变化时，较小的权值 $w$ 能够防止网络整体特性改变过大，网络也就不会去“学习”那些没用的噪声信息了。相反，对于手写数字图像那些重复的特征，神经网络在一遍遍的 mini-batch 中，“铭记在心”。</p>
<p>人们也称这个思想为<strong>奥卡姆剃刀原理</strong>：当两个假说具有完全相同的解释力和预测力时，我们以那个较为简单的假说作为讨论依据。</p>
<h3 id="其他抑制过拟合的方法"><a href="#其他抑制过拟合的方法" class="headerlink" title="其他抑制过拟合的方法"></a>其他抑制过拟合的方法</h3><p>当然还有很多抑制过拟合的方法，比如：</p>
<p><strong>$L_1$ 正则化</strong>，即换一个正则函数。</p>
<p><strong>dropout</strong>：学习过程中随机删去一些神经元。</p>
<p><strong>人工扩展训练集</strong>：这也是我比较喜欢的一个方法，可以通过平移、缩放、旋转、elastic distortions 等扩展数据集。扩展数据简单粗暴有效，微软研究院的研究员用 elastic distortions 扩展数据后，就将 MNIST 识别率提高到了 99.3%。</p>
<h2 id="改进权重初始化"><a href="#改进权重初始化" class="headerlink" title="改进权重初始化"></a>改进权重初始化</h2><p>我们在初始化权重和偏移时，选择高斯随机，均值为 0，标准差为 1。权重输入为 $z = \sum_j w_j x_j+b$，随着输入神经元数目的增加，标准差也随之增加，例如 1000 个神经元，其正太分布曲线为</p>
<p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/gauss1.png" alt="正太分布"></p>
<p>曲线非常平坦，意味着 $z \gg 1, z \ll -1$ 的可能性都大大增加，输出 $\sigma(z)$ 极有可能饱和，出现过拟合的现象。解决的方法也非常简单，初始化时标准差选为 $1/\sqrt{n_{\rm in}}$。</p>
<p><img src="https://raw.githubusercontent.com/lyyyuna/blog_img/master/blog/201606/gauss2.png" alt="改进的正太分布"></p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>以下是 network2.py 的源码（当然我不是写的啦，<a href="http://michaelnielsen.org/" target="_blank" rel="noopener">Michael Nielsen</a> 的杰作），所用技术和算法已在上文逐一阐述。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""network2.py</span></span><br><span class="line"><span class="string">~~~~~~~~~~~~~~</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">An improved version of network.py, implementing the stochastic</span></span><br><span class="line"><span class="string">gradient descent learning algorithm for a feedforward neural network.</span></span><br><span class="line"><span class="string">Improvements include the addition of the cross-entropy cost function,</span></span><br><span class="line"><span class="string">regularization, and better initialization of network weights.  Note</span></span><br><span class="line"><span class="string">that I have focused on making the code simple, easily readable, and</span></span><br><span class="line"><span class="string">easily modifiable.  It is not optimized, and omits many desirable</span></span><br><span class="line"><span class="string">features.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Libraries</span></span><br><span class="line"><span class="comment"># Standard library</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment"># Third-party libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Define the quadratic and cross-entropy cost functions</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuadraticCost</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fn</span><span class="params">(a, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return the cost associated with an output ``a`` and desired output</span></span><br><span class="line"><span class="string">        ``y``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.5</span>*np.linalg.norm(a-y)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">delta</span><span class="params">(z, a, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return the error delta from the output layer."""</span></span><br><span class="line">        <span class="keyword">return</span> (a-y) * sigmoid_prime(z)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossEntropyCost</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fn</span><span class="params">(a, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return the cost associated with an output ``a`` and desired output</span></span><br><span class="line"><span class="string">        ``y``.  Note that np.nan_to_num is used to ensure numerical</span></span><br><span class="line"><span class="string">        stability.  In particular, if both ``a`` and ``y`` have a 1.0</span></span><br><span class="line"><span class="string">        in the same slot, then the expression (1-y)*np.log(1-a)</span></span><br><span class="line"><span class="string">        returns nan.  The np.nan_to_num ensures that that is converted</span></span><br><span class="line"><span class="string">        to the correct value (0.0).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> np.sum(np.nan_to_num(-y*np.log(a)-(<span class="number">1</span>-y)*np.log(<span class="number">1</span>-a)))</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">delta</span><span class="params">(z, a, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return the error delta from the output layer.  Note that the</span></span><br><span class="line"><span class="string">        parameter ``z`` is not used by the method.  It is included in</span></span><br><span class="line"><span class="string">        the method's parameters in order to make the interface</span></span><br><span class="line"><span class="string">        consistent with the delta method for other cost classes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> (a-y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Main Network class</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sizes, cost=CrossEntropyCost)</span>:</span></span><br><span class="line">        <span class="string">"""The list ``sizes`` contains the number of neurons in the respective</span></span><br><span class="line"><span class="string">        layers of the network.  For example, if the list was [2, 3, 1]</span></span><br><span class="line"><span class="string">        then it would be a three-layer network, with the first layer</span></span><br><span class="line"><span class="string">        containing 2 neurons, the second layer 3 neurons, and the</span></span><br><span class="line"><span class="string">        third layer 1 neuron.  The biases and weights for the network</span></span><br><span class="line"><span class="string">        are initialized randomly, using</span></span><br><span class="line"><span class="string">        ``self.default_weight_initializer`` (see docstring for that</span></span><br><span class="line"><span class="string">        method).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.num_layers = len(sizes)</span><br><span class="line">        self.sizes = sizes</span><br><span class="line">        self.default_weight_initializer()</span><br><span class="line">        self.cost=cost</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">default_weight_initializer</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Initialize each weight using a Gaussian distribution with mean 0</span></span><br><span class="line"><span class="string">        and standard deviation 1 over the square root of the number of</span></span><br><span class="line"><span class="string">        weights connecting to the same neuron.  Initialize the biases</span></span><br><span class="line"><span class="string">        using a Gaussian distribution with mean 0 and standard</span></span><br><span class="line"><span class="string">        deviation 1.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Note that the first layer is assumed to be an input layer, and</span></span><br><span class="line"><span class="string">        by convention we won't set any biases for those neurons, since</span></span><br><span class="line"><span class="string">        biases are only ever used in computing the outputs from later</span></span><br><span class="line"><span class="string">        layers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> self.sizes[<span class="number">1</span>:]]</span><br><span class="line">        self.weights = [np.random.randn(y, x)/np.sqrt(x)</span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(self.sizes[:<span class="number">-1</span>], self.sizes[<span class="number">1</span>:])]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">large_weight_initializer</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Initialize the weights using a Gaussian distribution with mean 0</span></span><br><span class="line"><span class="string">        and standard deviation 1.  Initialize the biases using a</span></span><br><span class="line"><span class="string">        Gaussian distribution with mean 0 and standard deviation 1.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Note that the first layer is assumed to be an input layer, and</span></span><br><span class="line"><span class="string">        by convention we won't set any biases for those neurons, since</span></span><br><span class="line"><span class="string">        biases are only ever used in computing the outputs from later</span></span><br><span class="line"><span class="string">        layers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        This weight and bias initializer uses the same approach as in</span></span><br><span class="line"><span class="string">        Chapter 1, and is included for purposes of comparison.  It</span></span><br><span class="line"><span class="string">        will usually be better to use the default weight initializer</span></span><br><span class="line"><span class="string">        instead.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> self.sizes[<span class="number">1</span>:]]</span><br><span class="line">        self.weights = [np.random.randn(y, x)</span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(self.sizes[:<span class="number">-1</span>], self.sizes[<span class="number">1</span>:])]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(self, a)</span>:</span></span><br><span class="line">        <span class="string">"""Return the output of the network if ``a`` is input."""</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            a = sigmoid(np.dot(w, a)+b)</span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, training_data, epochs, mini_batch_size, eta,</span></span></span><br><span class="line"><span class="function"><span class="params">            lmbda = <span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            evaluation_data=None,</span></span></span><br><span class="line"><span class="function"><span class="params">            monitor_evaluation_cost=False,</span></span></span><br><span class="line"><span class="function"><span class="params">            monitor_evaluation_accuracy=False,</span></span></span><br><span class="line"><span class="function"><span class="params">            monitor_training_cost=False,</span></span></span><br><span class="line"><span class="function"><span class="params">            monitor_training_accuracy=False)</span>:</span></span><br><span class="line">        <span class="string">"""Train the neural network using mini-batch stochastic gradient</span></span><br><span class="line"><span class="string">        descent.  The ``training_data`` is a list of tuples ``(x, y)``</span></span><br><span class="line"><span class="string">        representing the training inputs and the desired outputs.  The</span></span><br><span class="line"><span class="string">        other non-optional parameters are self-explanatory, as is the</span></span><br><span class="line"><span class="string">        regularization parameter ``lmbda``.  The method also accepts</span></span><br><span class="line"><span class="string">        ``evaluation_data``, usually either the validation or test</span></span><br><span class="line"><span class="string">        data.  We can monitor the cost and accuracy on either the</span></span><br><span class="line"><span class="string">        evaluation data or the training data, by setting the</span></span><br><span class="line"><span class="string">        appropriate flags.  The method returns a tuple containing four</span></span><br><span class="line"><span class="string">        lists: the (per-epoch) costs on the evaluation data, the</span></span><br><span class="line"><span class="string">        accuracies on the evaluation data, the costs on the training</span></span><br><span class="line"><span class="string">        data, and the accuracies on the training data.  All values are</span></span><br><span class="line"><span class="string">        evaluated at the end of each training epoch.  So, for example,</span></span><br><span class="line"><span class="string">        if we train for 30 epochs, then the first element of the tuple</span></span><br><span class="line"><span class="string">        will be a 30-element list containing the cost on the</span></span><br><span class="line"><span class="string">        evaluation data at the end of each epoch. Note that the lists</span></span><br><span class="line"><span class="string">        are empty if the corresponding flag is not set.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> evaluation_data: n_data = len(evaluation_data)</span><br><span class="line">        n = len(training_data)</span><br><span class="line">        evaluation_cost, evaluation_accuracy = [], []</span><br><span class="line">        training_cost, training_accuracy = [], []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(epochs):</span><br><span class="line">            random.shuffle(training_data)</span><br><span class="line">            mini_batches = [</span><br><span class="line">                training_data[k:k+mini_batch_size]</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">0</span>, n, mini_batch_size)]</span><br><span class="line">            <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">                self.update_mini_batch(</span><br><span class="line">                    mini_batch, eta, lmbda, len(training_data))</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"Epoch %s training complete"</span> % j</span><br><span class="line">            <span class="keyword">if</span> monitor_training_cost:</span><br><span class="line">                cost = self.total_cost(training_data, lmbda)</span><br><span class="line">                training_cost.append(cost)</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Cost on training data: &#123;&#125;"</span>.format(cost)</span><br><span class="line">            <span class="keyword">if</span> monitor_training_accuracy:</span><br><span class="line">                accuracy = self.accuracy(training_data, convert=<span class="keyword">True</span>)</span><br><span class="line">                training_accuracy.append(accuracy)</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Accuracy on training data: &#123;&#125; / &#123;&#125;"</span>.format(</span><br><span class="line">                    accuracy, n)</span><br><span class="line">            <span class="keyword">if</span> monitor_evaluation_cost:</span><br><span class="line">                cost = self.total_cost(evaluation_data, lmbda, convert=<span class="keyword">True</span>)</span><br><span class="line">                evaluation_cost.append(cost)</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Cost on evaluation data: &#123;&#125;"</span>.format(cost)</span><br><span class="line">            <span class="keyword">if</span> monitor_evaluation_accuracy:</span><br><span class="line">                accuracy = self.accuracy(evaluation_data)</span><br><span class="line">                evaluation_accuracy.append(accuracy)</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Accuracy on evaluation data: &#123;&#125; / &#123;&#125;"</span>.format(</span><br><span class="line">                    self.accuracy(evaluation_data), n_data)</span><br><span class="line">            <span class="keyword">print</span></span><br><span class="line">        <span class="keyword">return</span> evaluation_cost, evaluation_accuracy, \</span><br><span class="line">            training_cost, training_accuracy</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta, lmbda, n)</span>:</span></span><br><span class="line">        <span class="string">"""Update the network's weights and biases by applying gradient</span></span><br><span class="line"><span class="string">        descent using backpropagation to a single mini batch.  The</span></span><br><span class="line"><span class="string">        ``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the</span></span><br><span class="line"><span class="string">        learning rate, ``lmbda`` is the regularization parameter, and</span></span><br><span class="line"><span class="string">        ``n`` is the total size of the training data set.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">            delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br><span class="line">            nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> zip(nabla_b, delta_nabla_b)]</span><br><span class="line">            nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> zip(nabla_w, delta_nabla_w)]</span><br><span class="line">        self.weights = [(<span class="number">1</span>-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw</span><br><span class="line">                        <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nabla_w)]</span><br><span class="line">        self.biases = [b-(eta/len(mini_batch))*nb</span><br><span class="line">                       <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nabla_b)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return a tuple ``(nabla_b, nabla_w)`` representing the</span></span><br><span class="line"><span class="string">        gradient for the cost function C_x.  ``nabla_b`` and</span></span><br><span class="line"><span class="string">        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar</span></span><br><span class="line"><span class="string">        to ``self.biases`` and ``self.weights``."""</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="comment"># feedforward</span></span><br><span class="line">        activation = x</span><br><span class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            z = np.dot(w, activation)+b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = sigmoid(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line">        <span class="comment"># backward pass</span></span><br><span class="line">        delta = (self.cost).delta(zs[<span class="number">-1</span>], activations[<span class="number">-1</span>], y)</span><br><span class="line">        nabla_b[<span class="number">-1</span>] = delta</span><br><span class="line">        nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</span><br><span class="line">        <span class="comment"># Note that the variable l in the loop below is used a little</span></span><br><span class="line">        <span class="comment"># differently to the notation in Chapter 2 of the book.  Here,</span></span><br><span class="line">        <span class="comment"># l = 1 means the last layer of neurons, l = 2 is the</span></span><br><span class="line">        <span class="comment"># second-last layer, and so on.  It's a renumbering of the</span></span><br><span class="line">        <span class="comment"># scheme in the book, used here to take advantage of the fact</span></span><br><span class="line">        <span class="comment"># that Python can use negative indices in lists.</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</span><br><span class="line">            z = zs[-l]</span><br><span class="line">            sp = sigmoid_prime(z)</span><br><span class="line">            delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * sp</span><br><span class="line">            nabla_b[-l] = delta</span><br><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())</span><br><span class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(self, data, convert=False)</span>:</span></span><br><span class="line">        <span class="string">"""Return the number of inputs in ``data`` for which the neural</span></span><br><span class="line"><span class="string">        network outputs the correct result. The neural network's</span></span><br><span class="line"><span class="string">        output is assumed to be the index of whichever neuron in the</span></span><br><span class="line"><span class="string">        final layer has the highest activation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The flag ``convert`` should be set to False if the data set is</span></span><br><span class="line"><span class="string">        validation or test data (the usual case), and to True if the</span></span><br><span class="line"><span class="string">        data set is the training data. The need for this flag arises</span></span><br><span class="line"><span class="string">        due to differences in the way the results ``y`` are</span></span><br><span class="line"><span class="string">        represented in the different data sets.  In particular, it</span></span><br><span class="line"><span class="string">        flags whether we need to convert between the different</span></span><br><span class="line"><span class="string">        representations.  It may seem strange to use different</span></span><br><span class="line"><span class="string">        representations for the different data sets.  Why not use the</span></span><br><span class="line"><span class="string">        same representation for all three data sets?  It's done for</span></span><br><span class="line"><span class="string">        efficiency reasons -- the program usually evaluates the cost</span></span><br><span class="line"><span class="string">        on the training data and the accuracy on other data sets.</span></span><br><span class="line"><span class="string">        These are different types of computations, and using different</span></span><br><span class="line"><span class="string">        representations speeds things up.  More details on the</span></span><br><span class="line"><span class="string">        representations can be found in</span></span><br><span class="line"><span class="string">        mnist_loader.load_data_wrapper.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> convert:</span><br><span class="line">            results = [(np.argmax(self.feedforward(x)), np.argmax(y))</span><br><span class="line">                       <span class="keyword">for</span> (x, y) <span class="keyword">in</span> data]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            results = [(np.argmax(self.feedforward(x)), y)</span><br><span class="line">                        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> data]</span><br><span class="line">        <span class="keyword">return</span> sum(int(x == y) <span class="keyword">for</span> (x, y) <span class="keyword">in</span> results)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">total_cost</span><span class="params">(self, data, lmbda, convert=False)</span>:</span></span><br><span class="line">        <span class="string">"""Return the total cost for the data set ``data``.  The flag</span></span><br><span class="line"><span class="string">        ``convert`` should be set to False if the data set is the</span></span><br><span class="line"><span class="string">        training data (the usual case), and to True if the data set is</span></span><br><span class="line"><span class="string">        the validation or test data.  See comments on the similar (but</span></span><br><span class="line"><span class="string">        reversed) convention for the ``accuracy`` method, above.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        cost = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> data:</span><br><span class="line">            a = self.feedforward(x)</span><br><span class="line">            <span class="keyword">if</span> convert: y = vectorized_result(y)</span><br><span class="line">            cost += self.cost.fn(a, y)/len(data)</span><br><span class="line">        cost += <span class="number">0.5</span>*(lmbda/len(data))*sum(</span><br><span class="line">            np.linalg.norm(w)**<span class="number">2</span> <span class="keyword">for</span> w <span class="keyword">in</span> self.weights)</span><br><span class="line">        <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self, filename)</span>:</span></span><br><span class="line">        <span class="string">"""Save the neural network to the file ``filename``."""</span></span><br><span class="line">        data = &#123;<span class="string">"sizes"</span>: self.sizes,</span><br><span class="line">                <span class="string">"weights"</span>: [w.tolist() <span class="keyword">for</span> w <span class="keyword">in</span> self.weights],</span><br><span class="line">                <span class="string">"biases"</span>: [b.tolist() <span class="keyword">for</span> b <span class="keyword">in</span> self.biases],</span><br><span class="line">                <span class="string">"cost"</span>: str(self.cost.__name__)&#125;</span><br><span class="line">        f = open(filename, <span class="string">"w"</span>)</span><br><span class="line">        json.dump(data, f)</span><br><span class="line">        f.close()</span><br><span class="line"></span><br><span class="line"><span class="comment">#### Loading a Network</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="string">"""Load a neural network from the file ``filename``.  Returns an</span></span><br><span class="line"><span class="string">    instance of Network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    f = open(filename, <span class="string">"r"</span>)</span><br><span class="line">    data = json.load(f)</span><br><span class="line">    f.close()</span><br><span class="line">    cost = getattr(sys.modules[__name__], data[<span class="string">"cost"</span>])</span><br><span class="line">    net = Network(data[<span class="string">"sizes"</span>], cost=cost)</span><br><span class="line">    net.weights = [np.array(w) <span class="keyword">for</span> w <span class="keyword">in</span> data[<span class="string">"weights"</span>]]</span><br><span class="line">    net.biases = [np.array(b) <span class="keyword">for</span> b <span class="keyword">in</span> data[<span class="string">"biases"</span>]]</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line"><span class="comment">#### Miscellaneous functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorized_result</span><span class="params">(j)</span>:</span></span><br><span class="line">    <span class="string">"""Return a 10-dimensional unit vector with a 1.0 in the j'th position</span></span><br><span class="line"><span class="string">    and zeroes elsewhere.  This is used to convert a digit (0...9)</span></span><br><span class="line"><span class="string">    into a corresponding desired output from the neural network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    e = np.zeros((<span class="number">10</span>, <span class="number">1</span>))</span><br><span class="line">    e[j] = <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> e</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""The sigmoid function."""</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""Derivative of the sigmoid function."""</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span><br></pre></td></tr></table></figure>
<h2 id="神经网络识别手写数字目录"><a href="#神经网络识别手写数字目录" class="headerlink" title="神经网络识别手写数字目录"></a>神经网络识别手写数字目录</h2><ol>
<li><a href="http://www.lyyyuna.com/2016/05/29/handwritten-neural-net/">基于 BP 神经网络的识别手写体数字 - 神经网络基础</a></li>
<li><a href="http://www.lyyyuna.com/2016/06/25/handwritten-neural-net02/">基于 BP 神经网络的手写体数字识别 - 设计与实现</a></li>
<li><a href="http://www.lyyyuna.com/2016/06/26/handwritten-neural-net03/">基于 BP 神经网络的手写体数字识别 - 反向传播算法</a></li>
<li><a href="http://www.lyyyuna.com/2016/06/30/handwritten-neural-net04/">基于 BP 神经网络的手写体数字识别 - 优化</a></li>
</ol>
  
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/数学/">数学</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/Python/">Python</a><a href="/tags/mnist/">mnist</a><a href="/tags/neural-network/">neural network</a>
  </div>

</div>



	<div class="article-share" id="share">
	
	  <div data-url="http://www.lyyyuna.com/2016/06/30/handwritten-neural-net04/" data-title="基于 BP 神经网络的手写体数字识别 - 优化 | lyyyuna 的小花园" data-tsina="null" class="share clearfix">
	  </div>
	
	</div>


</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2016/07/29/robotframework-template-and-data-driven/" title="Robot Framework 模板与数据驱动测试">
  <strong>上一篇：</strong><br/>
  <span>
  Robot Framework 模板与数据驱动测试</span>
</a>
</div>


<div class="next">
<a href="/2016/06/26/handwritten-neural-net03/"  title="基于 BP 神经网络的手写体数字识别 - 反向传播算法">
 <strong>下一篇：</strong><br/> 
 <span>基于 BP 神经网络的手写体数字识别 - 反向传播算法
</span>
</a>
</div>

</nav>

	

<section id="comments" class="comment">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</div>  
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">文章目录</strong>
 
 <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#交叉熵代价函数"><span class="toc-number">1.</span> <span class="toc-text">交叉熵代价函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#介绍交叉熵代价函数"><span class="toc-number">1.1.</span> <span class="toc-text">介绍交叉熵代价函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#柔性最大值传输-softmax"><span class="toc-number">1.2.</span> <span class="toc-text">柔性最大值传输 softmax</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#过拟合和正则化"><span class="toc-number">2.</span> <span class="toc-text">过拟合和正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#正则化-Regularization"><span class="toc-number">2.1.</span> <span class="toc-text">正则化 Regularization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#为什么正则化能抑制过拟合"><span class="toc-number">2.2.</span> <span class="toc-text">为什么正则化能抑制过拟合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#其他抑制过拟合的方法"><span class="toc-number">2.3.</span> <span class="toc-text">其他抑制过拟合的方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#改进权重初始化"><span class="toc-number">3.</span> <span class="toc-text">改进权重初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#代码"><span class="toc-number">4.</span> <span class="toc-text">代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络识别手写数字目录"><span class="toc-number">5.</span> <span class="toc-text">神经网络识别手写数字目录</span></a></li></ol>
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  
<div class="github-card">
<p class="asidetitle">Github 名片</p>
<div class="github-card" data-github="lyyyuna" data-width="220" data-height="119" data-theme="medium">
<script type="text/javascript" src="//cdn.jsdelivr.net/github-cards/latest/widget.js" ></script>
</div>
  </div>



  
<div class="categorieslist">
	<p class="asidetitle">分类</p>
		<ul>
		
		  
			<li><a href="/categories/数学/" title="数学">数学<sup>4</sup></a></li>
		  
		
		  
			<li><a href="/categories/杂/" title="杂">杂<sup>12</sup></a></li>
		  
		
		  
			<li><a href="/categories/系统/" title="系统">系统<sup>6</sup></a></li>
		  
		
		  
			<li><a href="/categories/网络/" title="网络">网络<sup>8</sup></a></li>
		  
		
		  
			<li><a href="/categories/自动化测试/" title="自动化测试">自动化测试<sup>7</sup></a></li>
		  
		
		  
			<li><a href="/categories/语言/" title="语言">语言<sup>12</sup></a></li>
		  
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">标签</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/Python/" title="Python">Python<sup>8</sup></a></li>
			
		
			
				<li><a href="/tags/robot-framework/" title="robot framework">robot framework<sup>7</sup></a></li>
			
		
			
				<li><a href="/tags/scons/" title="scons">scons<sup>6</sup></a></li>
			
		
			
				<li><a href="/tags/Python-script/" title="Python script">Python script<sup>6</sup></a></li>
			
		
			
				<li><a href="/tags/Python2-7源码/" title="Python2.7源码">Python2.7源码<sup>5</sup></a></li>
			
		
			
				<li><a href="/tags/neural-network/" title="neural network">neural network<sup>4</sup></a></li>
			
		
			
				<li><a href="/tags/mnist/" title="mnist">mnist<sup>4</sup></a></li>
			
		
			
				<li><a href="/tags/asyncio/" title="asyncio">asyncio<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/Exif/" title="Exif">Exif<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/cpp/" title="cpp">cpp<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/http-proxy/" title="http proxy">http proxy<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/爬虫/" title="爬虫">爬虫<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/DHT/" title="DHT">DHT<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/bilibili/" title="bilibili">bilibili<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Windows-Debugger/" title="Windows Debugger">Windows Debugger<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/openwrt/" title="openwrt">openwrt<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/shellcode/" title="shellcode">shellcode<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Python3-x源码/" title="Python3.x源码">Python3.x源码<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Celery/" title="Celery">Celery<sup>1</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
          <li>
            
            	<a href="http://www.lihulab.net" target="_blank" title="自动化测试">自动化测试</a>
            
          </li>
        
          <li>
            
            	<a href="http://oj.lihulab.net" target="_blank" title="蠡湖实验室 OJ">蠡湖实验室 OJ</a>
            
          </li>
        
    </ul>
</div>

  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS 订阅</a>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	
	<section class="info">
		<p> Hello, I am lyy <br/>
			This is my blog, believe it or not.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		
		<a href="https://github.com/lyyyuna" target="_blank" class="icon-github" title="github"></a>
		
		
		
		
		
		
		<a href="https://www.douban.com/people/44114278" target="_blank" class="icon-douban" title="豆瓣"></a>
		
		
		<a href="http://www.zhihu.com/people/li-yi-yang-38" target="_blank" class="icon-zhihu" title="知乎"></a>
		
		
		
		<a href="mailto:lyyyuna@outlook.com" target="_blank" class="icon-email" title="Email Me"></a>
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2018 
		
		<a href="/about" target="_blank" title="lyyyuna">lyyyuna</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<div class="hoverqrcode clearfix"></div>',
  '<a class="overlay" id="qrcode"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);

  $('.hoverqrcode').hide();

  var myWidth = 0;
  function updatehoverqrcode(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
    var qrsize = myWidth > 1024 ? 200:100;
    var options = {render: 'image', size: qrsize, fill: '#2ca6cb', text: url, radius: 0.5, quiet: 1};
    var p = $('.article-share-qrcode').position();
    $('.hoverqrcode').empty().css('width', qrsize).css('height', qrsize)
                          .css('left', p.left-qrsize/2+20).css('top', p.top-qrsize-10)
                          .qrcode(options);
  };
  $(window).resize(function(){
    $('.hoverqrcode').hide();
  });
  $('.article-share-qrcode').click(function(){
    updatehoverqrcode();
    $('.hoverqrcode').toggle();
  });
  $('.article-share-qrcode').hover(function(){}, function(){
      $('.hoverqrcode').hide();
  });
});   
</script>




<script type="text/javascript">

var disqus_shortname = 'lyyyuna';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>






<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->

<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-42389939-1', 'www.lyyyuna.com');  
ga('send', 'pageview');
</script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?4ba228e3ecc48b86bafa07900215f733";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="返回顶部"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  </body>
</html>
